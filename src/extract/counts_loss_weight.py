import extract.data_loading as data_loading
import numpy as np
import json
import tqdm
import h5py
import click


def compute_counts_loss_weight(
    files_spec_path, reference_fasta, num_tasks, profile_length,
    chrom_sizes_tsv, task_inds=None, chrom_set=None, batch_size=128
):
    """
    Given a dataset, computes a reasonable counts loss weight as the median
    number of reads in the profiles (averaged over strands) divided by 10.
    Arguments:
        `files_spec_path`: path to the JSON files spec for the model
        `reference_fasta`: needed for data loader but is not logically required
            for this task
        `num_tasks`: number of tasks in the dataset
        `profile_length`: length output profiles
        `chrom_sizes_tsv`: path to chromosome size TSV
        `task_inds`: list of indices for which the weight should be computed for
        `chrom_set`: set of chromosomes to use; if None, use all canonical
            chromosomes
        `batch_size`: batch size for running predictions/performance
    Results will be saved in the specified HDF5, under the following keys:
    """
    if not chrom_set:
        # By default, use all canonical chromosomes
        with open(chrom_sizes_tsv, "r") as f:
            chrom_set = [line.split("\t")[0] for line in f]

    if not task_inds:
        task_inds = np.arange(num_tasks)  # Use all tasks
    else:
        task_inds = np.array(task_inds)

    # Create data loading function and get all peak coordinates
    input_func = data_loading.get_input_func(
        files_spec_path, 1, profile_length, reference_fasta
        # We don't need any input sequences, so set input length to 1
    )
    coords = data_loading.get_positive_inputs(
        files_spec_path, chrom_set=chrom_set, task_indices=task_inds
    )
    num_examples = len(coords)
    
    counts = np.empty((num_examples, len(task_inds), 2))

    num_batches = int(np.ceil(num_examples / batch_size))
    for i in tqdm.trange(num_batches):
        batch_slice = slice(i * batch_size, (i + 1) * batch_size)
        coords_batch = coords[batch_slice]

        # Get the inputs to the model
        _, profiles = input_func(coords_batch)
        true_profs = profiles[:, task_inds]

        counts[batch_slice] = np.sum(true_profs, axis=2)
   
    # Median count over examples / 10, averaged over tasks/strands
    return np.mean(np.median(counts, axis=0) / 10)


@click.command()
@click.option(
    "--files-spec-path", "-f", required=True,
    help="Path to JSON specifying file paths used to train model"
)
@click.option(
    "--reference-fasta", "-r", default="/users/amtseng/genomes/hg38.fasta",
    help="Path to reference genome Fasta"
)
@click.option(
    "--chrom-sizes", "-c", default="/users/amtseng/genomes/hg38.canon.chrom.sizes",
    help="Path to chromosome sizes"
)
@click.option(
    "--profile-length", "-pl", default=1000, type=int,
    help="Length of profiles provided to and generated by model"
)
@click.option(
    "--num-tasks", "-n", required=True, help="Number of tasks associated to TF",
    type=int
)
@click.option(
    "--task-inds", "-i", default=None, type=str,
    help="Comma-delimited set of indices (0-based) of the task(s) to compute importance scores for; by default aggregates over all tasks"
)
@click.option(
    "--chrom-set", "-s", default=None, type=str,
    help="Comma-delimited set of chromosomes to compute importance scores for; by default uses all chromosomes in the specified TSV"
)
def main(
    files_spec_path, reference_fasta, chrom_sizes, profile_length, num_tasks,
    task_inds, chrom_set
):
    if chrom_set:
        chrom_set = chrom_set.split(",")
    if task_inds:
        task_inds = [int(x) for x in task_inds.split(",")]

    weight = compute_counts_loss_weight(
        files_spec_path, reference_fasta, num_tasks, profile_length,
        chrom_sizes, task_inds=task_inds, chrom_set=chrom_set, batch_size=128
    )

    print("Suggested counts loss weight: %d" % weight)


if __name__ == "__main__":
    main()
