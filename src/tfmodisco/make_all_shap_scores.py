import numpy as np
import click
import json
import model.train_profile_model as train_profile_model
import feature.make_profile_dataset as make_profile_dataset
import keras.utils
import extract.compute_shap as compute_shap
import tqdm
import os
import h5py

def import_metrics_json(models_path, run_num):
    """
    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a
    Python dictionary. Returns None if the path does not exist.
    """
    path = os.path.join(models_path, str(run_num), "metrics.json")
    if not os.path.exists(path):
        return None
    with open(path, "r") as f:
        return json.load(f)


def get_best_run(models_path):
    """
    Given the path to a set of runs, determines the run with the best summit
    profile loss at the end.
    Arguments:
        `models_path`: path to all models, where each run directory is of the
            form {models_path}/{run_num}/metrics.json
    Returns the number of the run, the (one-indexed) number of the epoch, the
    value associated with that run and epoch, and a dict of all the values used
    for comparison (mapping pair of run number and epoch number to value).
    """
    # Get the metrics, ignoring empty or nonexistent metrics.json files
    metrics = {
        run_num : import_metrics_json(models_path, run_num)
        for run_num in os.listdir(models_path)
    }
    # Remove empties
    metrics = {key : val for key, val in metrics.items() if val}
    
    # Get the best value
    best_run, best_run_epoch, best_val, all_vals = None, None, None, {}
    for run_num in metrics.keys():
        try:
            val = np.mean(metrics[run_num]["summit_prof_nll"]["values"])
            last_epoch = len(metrics[run_num]["val_epoch_loss"]["values"])
            if best_val is None or val < best_val:
                best_run, best_run_epoch, best_val = run_num, last_epoch, val
            all_vals[(run_num, best_run_epoch)] = val
        except Exception as e:
            print(
                "Warning: Was not able to compute values for run %s" % run_num
            )
            continue
    return best_run, best_run_epoch, best_val, all_vals


@click.command()
@click.option(
    "--models-path-stem", "-m", required=True,
    help="Stem path to trained models"
)
@click.option(
    "--files-spec-path", "-f", required=True,
    help="Path to JSON specifying file paths used to train model"
)
@click.option(
    "--reference-fasta", "-r", default="/users/amtseng/genomes/hg38.fasta",
    help="Path to reference genome Fasta"
)
@click.option(
    "--chrom-sizes", "-c", default="/users/amtseng/genomes/hg38.canon.chrom.sizes",
    help="Path to chromosome sizes"
)
@click.option(
    "--chrom-splits-json", "-s",
    default="/users/amtseng/tfmodisco/data/processed/ENCODE/chrom_splits.json",
    help="Path to chromosome splits JSON"
)
@click.option(
    "--input-length", "-il", default=1346, type=int,
    help="Length of input sequences to model"
)
@click.option(
    "--profile-length", "-pl", default=1000, type=int,
    help="Length of profiles provided to and generated by model"
)
@click.option(
    "--num-tasks", "-n", required=True, help="Number of tasks trained by model",
    type=int
)
@click.option(
    "--task-index", "-i", default=None, type=int,
    help="(0-based) Index of the task to compute importance scores for; by default aggregates over all tasks"
)
@click.option(
    "--outfile-stem", "-o", required=True,
    help="Where to store the hdf5 with scores, as a path prefix"
)
def main(
    models_path_stem, files_spec_path, reference_fasta, chrom_sizes,
    chrom_splits_json, input_length, profile_length, num_tasks, task_index,
    outfile_stem
):
    """
    Takes a set of all peak coordinates and the stem path to a set of models,
    picks the best model for each fold, and computes importance scores for the
    full peak set for each fold, saving the results in HDF5 files. The output
    HDF5 will have, for each peak, the coordinate of the summit-centered
    interval, the importance scores, and the one-hot encoded sequence.
    """
    # Extract files
    with open(files_spec_path, "r") as f:
        files_spec = json.load(f)
    peak_beds = files_spec["peak_beds"]
    if task_index is not None:
        peak_beds = [peak_beds[task_index]]
    profile_hdf5 = files_spec["profile_hdf5"]

    # Get set of chromosomes from given chromosome sizes
    with open(chrom_sizes, "r") as f:
        chrom_set = [line.split()[0] for line in f]


    with open(chrom_splits_json, "r") as f:
        splits_json = json.load(f)

    # Iterate through all splits, and create SHAP scores for each one
    for split in splits_json:
        print("Fold %s" % split)

        os.makedirs(os.path.dirname(outfile_stem), exist_ok=True)
        h5_file = h5py.File(outfile_stem + "_fold" + split + ".h5", "w")

        # Find the best model
        fold_path = models_path_stem + ("_fold%s" % split)
        best_run, last_epoch, best_val, all_vals = get_best_run(fold_path)
        print("\tBest run: %s, best epoch: %s, best val: %s" % (
            best_run, last_epoch, best_val
        ))

        print("\tImporting model...")
        model_path = os.path.join(
            fold_path, best_run, "model_ckpt_epoch_%d.h5" % last_epoch
        )
        model = train_profile_model.load_model(
            model_path, num_tasks, profile_length
        )

        # Make data loader
        batch_size = 128
        data_loader = make_profile_dataset.create_data_loader(
            peak_beds, profile_hdf5, "SummitCenteringCoordsBatcher",
            batch_size=batch_size, reference_fasta=reference_fasta,
            chrom_sizes=chrom_sizes, input_length=input_length,
            profile_length=profile_length, negative_ratio=0, peak_tiling_stride=0,
            revcomp=False, jitter_size=0, dataset_seed=None, chrom_set=chrom_set,
            shuffle=False, return_coords=True
        )
        enq = keras.utils.OrderedEnqueuer(data_loader, use_multiprocessing=True)
        workers, queue_size = 10, 20
        enq.start(workers, queue_size)
        para_batch_gen = enq.get()

        # Make explainers
        explainer = compute_shap.create_explainer(model, task_index=task_index)

        print("Computing importance scores...")
        num_batches = len(enq.sequence)
        num_expected = num_batches * batch_size
        # Allocate arrays to hold results
        hyp_scores = np.empty((num_expected, input_length, 4))
        all_input_seqs = np.empty((num_expected, input_length, 4))
        all_coords = np.empty((num_expected, 3), dtype=object)
        num_seen = 0
        for i in tqdm.trange(len(enq.sequence)):
            input_seqs, profiles, status, coords, peaks = next(para_batch_gen)
            cont_profs = profiles[:, num_tasks:]
            num_in_batch = len(input_seqs)
            start, end = num_seen, num_seen + num_in_batch
            hyp_scores[start:end] = explainer(input_seqs, cont_profs)
            all_input_seqs[start:end] = input_seqs
            all_coords[start:end] = coords
            num_seen += num_in_batch
        enq.stop()

        # Cut off excess
        hyp_scores = hyp_scores[:num_seen]
        all_input_seqs = all_input_seqs[:num_seen]
        all_coords = all_coords[:num_seen]

        # The coordinates need to be expanded/cut to the right input length
        midpoints = (all_coords[:, 1] + all_coords[:, 2]) // 2
        all_coords[:, 1] = midpoints - (input_length // 2)
        all_coords[:, 2] = all_coords[:, 1] + input_length

        print("Saving result to HDF5...")
        h5_file.create_dataset("coords_chrom", data=all_coords[:, 0].astype("S"))
        h5_file.create_dataset("coords_start", data=all_coords[:, 1].astype(int))
        h5_file.create_dataset("coords_end", data=all_coords[:, 2].astype(int))
        h5_file.create_dataset("hyp_scores", data=hyp_scores)
        h5_file.create_dataset("input_seqs", data=all_input_seqs)
        model_ds = h5_file.create_dataset("model", data=0)
        model_ds.attrs["model"] = model_path
        h5_file.close()

if __name__ == "__main__":
    main()
