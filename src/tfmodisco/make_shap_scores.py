import numpy as np
import click
import json
import model.train_profile_model as train_profile_model
import model.train_count_regression_model as train_count_regression_model
import keras.utils
import extract.compute_profile_shap as compute_profile_shap
import extract.compute_countreg_shap as compute_countreg_shap
import extract.data_loading as data_loading
import tqdm
import os
import h5py

@click.command()
@click.option(
    "--model-type", "-t", type=click.Choice(["profile", "countreg"]),
    default="profile", help="Type (architecture) of model"
)
@click.option(
    "--model-path", "-m", required=True, help="Path to trained model"
)
@click.option(
    "--files-spec-path", "-f", required=True,
    help="Path to JSON specifying file paths used to train model"
)
@click.option(
    "--reference-fasta", "-r", default="/users/amtseng/genomes/hg38.fasta",
    help="Path to reference genome Fasta"
)
@click.option(
    "--chrom-sizes", "-c", default="/users/amtseng/genomes/hg38.canon.chrom.sizes",
    help="Path to chromosome sizes"
)
@click.option(
    "--input-length", "-il", default=1346, type=int,
    help="Length of input sequences to model"
)
@click.option(
    "--profile-length", "-pl", default=1000, type=int,
    help="Length of profiles provided to and generated by model"
)
@click.option(
    "--num-tasks", "-n", required=True, help="Number of tasks associated to TF",
    type=int
)
@click.option(
    "--model-num-tasks", "-mn", required=None, type=int,
    help="Number of tasks in model architecture, if different from number of TF tasks; if so, need to specify the set of task indices to limit to"
)
@click.option(
    "--task-index", "-i", default=None, type=int,
    help="(0-based) Index of the task(s) to compute importance scores for; by default aggregates over all tasks"
)
@click.option(
    "--outfile", "-o", required=True, help="Where to store the hdf5 with scores"
)
def main(
    model_type, model_path, files_spec_path, reference_fasta, chrom_sizes,
    input_length, profile_length, num_tasks, model_num_tasks, task_index,
    outfile
):
    """
    Takes a set of all peak coordinates and a trained model, and computes
    importance scores, saving the results in an HDF5 file. The output HDF5 will
    have, for each peak, the coordinate of the summit-centered interval, the
    importance scores, and the one-hot encoded sequence.
    """
    # Extract files
    with open(files_spec_path, "r") as f:
        files_spec = json.load(f)
    peak_beds = files_spec["peak_beds"]
    if task_index is not None:
        peak_beds = [peak_beds[task_index]]
    profile_hdf5 = files_spec["profile_hdf5"]

    if model_num_tasks and model_num_tasks != num_tasks:
        assert task_index is not None and model_num_tasks == 1, \
            "Only supporting all- and single-task models right now"
    else:
        model_num_tasks = num_tasks

    # Get set of chromosomes from given chromosome sizes
    with open(chrom_sizes, "r") as f:
        chrom_set = [line.split()[0] for line in f]

    # Import model
    if model_type == "profile":
        model = train_profile_model.load_model(
            model_path, model_num_tasks, profile_length
        )
    else:
        model = train_count_regression_model.load_model(
            model_path, model_num_tasks
        )

    # Get set of positive coordinates
    all_pos_coords = data_loading.get_positive_inputs(
        files_spec_path, chrom_set=chrom_set,
        task_indices=(None if task_index is None else [task_index])
    )
    num_coords = len(all_pos_coords)

    # Make data loader
    input_func = data_loading.get_input_func(
        files_spec_path, input_length, profile_length, reference_fasta
    )

    # Make explainer
    if model_type == "profile":
        explainer = compute_profile_shap.create_explainer(
            model, task_index=task_index
        )
    else:
        explainer = compute_countreg_shap.create_explainer(
            model, task_index=task_index
        )

    # Create the datasets in the HDF5
    os.makedirs(os.path.dirname(outfile), exist_ok=True)
    f = h5py.File(outfile, "w")
    coords_chrom_dset = f.create_dataset(
        "coords_chrom", (num_coords,),
        dtype=h5py.string_dtype(encoding="ascii"), compression="gzip"
    )
    coords_start_dset = f.create_dataset(
        "coords_start", (num_coords,), dtype=int, compression="gzip"
    )
    coords_end_dset = f.create_dataset(
        "coords_end", (num_coords,), dtype=int, compression="gzip"
    )
    hyp_scores_dset = f.create_dataset(
        "hyp_scores", (num_coords, input_length, 4), compression="gzip"
    )
    input_seqs_dset = f.create_dataset(
        "input_seqs", (num_coords, input_length, 4), compression="gzip"
    )
    model = f.create_dataset("model", data=0)
    model.attrs["model"] = model_path

    print("Computing importance scores...")
    batch_size = 128
    num_batches = int(np.ceil(num_coords / batch_size))
    for i in tqdm.trange(num_batches):
        start, end = i * batch_size, (i + 1) * batch_size
        coords = all_pos_coords[start:end]
        input_seqs, profiles = input_func(coords)
        cont_profs = profiles[:, num_tasks:]

        # If the model architecture has fewer tasks, limit the input here
        if model_num_tasks and model_num_tasks != num_tasks:
            cont_profs = cont_profs[:, task_index:task_index + 1]

        # Expand/cut coordinates to right input length
        midpoints = (coords[:, 1] + coords[:, 2]) // 2
        coords[:, 1] = midpoints - (input_length // 2)
        coords[:, 2] = coords[:, 1] + input_length

        if model_type == "profile":
            hyp_scores_dset[start:end] = explainer(input_seqs, cont_profs)
        else: 
            hyp_scores_dset[start:end] = explainer(input_seqs)
        input_seqs_dset[start:end] = input_seqs
        coords_chrom_dset[start:end] = coords[:, 0].astype("S")
        coords_start_dset[start:end] = coords[:, 1].astype(int)
        coords_end_dset[start:end] = coords[:, 2].astype(int)


if __name__ == "__main__":
    main()
