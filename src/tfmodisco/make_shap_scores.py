import os
# This following environment variable is required, otherwise if we try to import
# the same model twice (i.e. to compute scores for multiple output heads), we'll
# get an h5py locking error
os.environ["HDF5_USE_FILE_LOCKING"] = "FALSE"
import numpy as np
import click
import json
import model.train_profile_model as train_profile_model
import model.train_countreg_model as train_countreg_model
import keras.utils
import extract.compute_shap as compute_shap
import extract.data_loading as data_loading
import tqdm
import h5py

@click.command()
@click.option(
    "--model-type", "-t", type=click.Choice(["profile", "countreg"]),
    default="profile", help="Type (architecture) of model"
)
@click.option(
    "--model-path", "-m", required=True, help="Path to trained model"
)
@click.option(
    "--files-spec-path", "-f", required=True,
    help="Path to JSON specifying file paths used to train model"
)
@click.option(
    "--reference-fasta", "-r", default="/users/amtseng/genomes/hg38.fasta",
    help="Path to reference genome Fasta"
)
@click.option(
    "--chrom-sizes", "-c", default="/users/amtseng/genomes/hg38.canon.chrom.sizes",
    help="Path to chromosome sizes"
)
@click.option(
    "--input-length", "-il", default=2114, type=int,
    help="Length of input sequences to model"
)
@click.option(
    "--profile-length", "-pl", default=1000, type=int,
    help="Length of profiles provided to and generated by model"
)
@click.option(
    "--data-num-tasks", "-dn", required=True, help="Number of tasks associated to dataset",
    type=int
)
@click.option(
    "--model-num-tasks", "-mn", required=None, type=int,
    help="Number of tasks in model architecture, if different from number of dataset tasks; if so, need to specify the set of task indices to limit to"
)
@click.option(
    "--task-index", "-i", default=None, type=int,
    help="Index (0-based) of the task for which to compute importance scores; by default aggregates over all tasks"
)
@click.option(
    "--outfile", "-o", required=True, help="Where to store the hdf5 with scores"
)
def main(
    model_type, model_path, files_spec_path, reference_fasta, chrom_sizes,
    input_length, profile_length, data_num_tasks, model_num_tasks, task_index,
    outfile
):
    """
    Takes a set of all peak coordinates and a trained model, and computes
    importance scores, saving the results in an HDF5 file. The output HDF5 will
    have, for each peak, the coordinate of the summit-centered interval, the
    importance scores, and the one-hot encoded sequence. For profile models,
    saves the hypothetical importance scores from both heads.
    """
    # Extract files
    with open(files_spec_path, "r") as f:
        files_spec = json.load(f)
    peak_beds = files_spec["peak_beds"]
    if task_index is not None:
        peak_beds = [peak_beds[task_index]]
    profile_hdf5 = files_spec["profile_hdf5"]

    if model_num_tasks and model_num_tasks != data_num_tasks:
        assert task_index is not None  # Must specify which peaks
        assert model_num_tasks == 1
        # Can only handle single-task models for now
    else:
        model_num_tasks = data_num_tasks

    # Get set of chromosomes from given chromosome sizes
    with open(chrom_sizes, "r") as f:
        chrom_set = [line.split()[0] for line in f]

    # Import model
    if model_type == "profile":
        model = train_profile_model.load_model(
            model_path, model_num_tasks, profile_length
        )
    else:
        model = train_countreg_model.load_model(
            model_path, model_num_tasks
        )

    # Get set of positive coordinates
    all_pos_coords = data_loading.get_positive_inputs(
        files_spec_path, chrom_set=chrom_set,
        task_indices=([task_index] if task_index is not None else None)
    )
    num_coords = len(all_pos_coords)

    # Make data loader
    input_func = data_loading.get_input_func(
        files_spec_path, input_length, profile_length, reference_fasta
    )

    # Make explainer
    if model_type == "profile":
        profile_explainer = compute_shap.create_profile_model_profile_explainer(
            model,
            task_index=(task_index if model_num_tasks == data_num_tasks else None)
        )
        count_explainer = compute_shap.create_profile_model_count_explainer(
            model,
            task_index=(task_index if model_num_tasks == data_num_tasks else None)
        )
    else:
        explainer = compute_shap.create_countreg_model_explainer(
            model,
            task_index=(task_index if model_num_tasks == data_num_tasks else None)
        )

    # Create the datasets in the HDF5
    os.makedirs(os.path.dirname(outfile), exist_ok=True)
    f = h5py.File(outfile, "w")
    coords_chrom_dset = f.create_dataset(
        "coords_chrom", (num_coords,),
        dtype=h5py.string_dtype(encoding="ascii"), compression="gzip"
    )
    coords_start_dset = f.create_dataset(
        "coords_start", (num_coords,), dtype=int, compression="gzip"
    )
    coords_end_dset = f.create_dataset(
        "coords_end", (num_coords,), dtype=int, compression="gzip"
    )
    if model_type == "profile":
        profile_hyp_scores_dset = f.create_dataset(
            "profile_hyp_scores", (num_coords, input_length, 4),
            compression="gzip"
        )
        count_hyp_scores_dset = f.create_dataset(
            "count_hyp_scores", (num_coords, input_length, 4),
            compression="gzip"
        )
    else:
        hyp_scores_dset = f.create_dataset(
            "hyp_scores", (num_coords, input_length, 4), compression="gzip"
        )
    input_seqs_dset = f.create_dataset(
        "input_seqs", (num_coords, input_length, 4), compression="gzip"
    )
    model = f.create_dataset("model", data=0)
    model.attrs["model"] = model_path

    print("Computing importance scores...")
    batch_size = 128
    num_batches = int(np.ceil(num_coords / batch_size))
    for i in tqdm.trange(num_batches):
        start, end = i * batch_size, (i + 1) * batch_size
        coords = all_pos_coords[start:end]
        input_seqs, profiles = input_func(coords)
        cont_profs = profiles[:, data_num_tasks:]

        # If the model architecture has fewer tasks, limit the input here
        if model_num_tasks and model_num_tasks != data_num_tasks:
            cont_profs = cont_profs[:, task_index : task_index + 1]

        # Expand/cut coordinates to right input length
        midpoints = (coords[:, 1] + coords[:, 2]) // 2
        coords[:, 1] = midpoints - (input_length // 2)
        coords[:, 2] = coords[:, 1] + input_length

        if model_type == "profile":
            profile_hyp_scores_dset[start:end] = profile_explainer(
                input_seqs, cont_profs
            )
            count_hyp_scores_dset[start:end] = count_explainer(
                input_seqs, cont_profs
            )
        else: 
            hyp_scores_dset[start:end] = explainer(input_seqs)

        input_seqs_dset[start:end] = input_seqs
        coords_chrom_dset[start:end] = coords[:, 0].astype("S")
        coords_start_dset[start:end] = coords[:, 1].astype(int)
        coords_end_dset[start:end] = coords[:, 2].astype(int)


if __name__ == "__main__":
    main()
