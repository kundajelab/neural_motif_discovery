import os
import h5py
import pyBigWig
import numpy as np
import tqdm
import click

def ask_to_continue():
    resp = input("Continue? [y|N]: ")
    if resp.lower() in ("y", "yes"):
        return
    else:
        raise Exception("Aborted")


def fetch_bigwig_paths(base_path, tf_name):
    """
    Reads in the set of BigWig paths corresponding to TF-ChIPseq profiles for
    pseudoreplicates. These BigWigs should be generated by
    `generate_ENCODE_pseudorep_bigwigs.sh`. This function performs
    error-checking to ensure that the proper files are there under `base_path`.
    Arguments:
        `base_path`: path containing the BigWig profiles of pseudoreplicates
        `tf_name`: name of TF
    Returns a list of the following structure:
    [
        (
            (exp1_rep1_neg_bigwig, exp1_rep1_pos_bigwig),
            (exp1_rep2_neg_bigwig, exp1_rep2_pos_bigwig)
        ),
        (
            (exp2_rep1_neg_bigwig, exp2_rep1_pos_bigwig),
            (exp2_rep2_neg_bigwig, exp2_rep2_pos_bigwig)
        ),
        ...
    The experiments are ordered such that the experiment IDs are in sorted
    order. This will assert that every experiment have at least 2 replicates.
    If more than 2 are found, it will arbitrarily pick 2. The order of the
    replicates is also arbitrary.
    """
    bigwig_list = [
        item for item in os.listdir(base_path) if item.endswith(".bw")
    ]

    # Read in names, organizing into experiment IDs/replicate IDs
    bigwig_dict = {}
    for name in bigwig_list:
        tokens = name[:-3].split("_")
        assert len(tokens) == 6, \
            "Found BigWig of improperly formatted name: %s" % name
        assert tokens[0] == tf_name
        expid_cline = (tokens[1], tokens[2])
        repid = tokens[4]
        strand = tokens[5]
        assert strand in ("neg", "pos"), "Found BigWig of strand other than pos/neg"
    
        if expid_cline not in bigwig_dict:
            bigwig_dict[expid_cline] = {}
        if repid not in bigwig_dict[expid_cline]:
            bigwig_dict[expid_cline][repid] = {}
        assert strand not in bigwig_dict[expid_cline][repid], \
            "Found duplicate BigWig for %s, %s, %s" % \
            (expid_cline, repid, strand)
        bigwig_dict[expid_cline][repid][strand] = os.path.join(base_path, name)

    # Generate list of paired paths
    bigwig_paths = []
    for expid_cline in sorted(bigwig_dict.keys(), key=lambda p: p[0]):
        replicate_dict = bigwig_dict[expid_cline]
        assert len(replicate_dict) >= 2, \
            "%s %s does not have at least 2 replicates" % expid_cline

        expid_list = []
        for repid in list(replicate_dict.keys())[:2]:
            assert len(replicate_dict[repid]) == 2, \
                "%s %s %s does not have pos/neg strands" % (expid_cline[0], expid_cline[1], repid)
            expid_list.append(
                (replicate_dict[repid]["neg"], replicate_dict[repid]["pos"])
            )
        bigwig_paths.append(tuple(expid_list))

    print("Found the following experiments/replicates:")
    for i in range(len(bigwig_paths)):
        exp_paths = bigwig_paths[i]
        print("%s\t%s\n%s\t%s\n" % (
            os.path.basename(exp_paths[0][0]), os.path.basename(exp_paths[0][1]),
            os.path.basename(exp_paths[1][0]), os.path.basename(exp_paths[1][1])
        ))
    print("Total experiments: %d" % len(bigwig_paths))
    ask_to_continue()

    return bigwig_paths


def create_hdf5(
    bigwig_paths, chrom_sizes_path, out_path, chunk_size, batch_size=100
):
    """
    Creates an HDF5 file containing all BigWig tracks.
    Arguments:
        `bigwig_paths`: a list of paired pairs of paths, as returned by
            `fetch_bigwig_paths`
        `chrom_sizes_path`: path to canonical chromosome sizes
        `out_path`: where to write the HDF5
        `chunk_size`: chunk size to use in HDF5 along the chromosome size
            dimension; this is recommended to be the expected size of the
            queries made
        `batch_size`: number of chunks to write at a time
    This creates an HDF5 file, containing a dataset for each chromosome. Each
    dataset will be a large array of shape L x T x 2 x 2, where L is the length
    of the chromosome, and T is the number of tasks. Each task holds a 2 x 2
    matrix of the read counts at that position, for the two pseudoreplicates and
    two strands (negative then positive), in that order. The HDF5 will also
    contain a dataset which has the paths to the corresponding source BigWigs,
    stored as a T x 2 x 2 array of paths.
    """
    bigwig_readers = []
    for exp in bigwig_paths:
        rep1, rep2 = exp
        exp_readers = [
            [pyBigWig.open(rep1[0]), pyBigWig.open(rep1[1])],
            [pyBigWig.open(rep2[0]), pyBigWig.open(rep2[1])]
        ]
        bigwig_readers.append(exp_readers)
   
    # Read in chromosome sizes
    with open(chrom_sizes_path, "r") as f:
        chrom_sizes = {}
        for line in f:
            tokens = line.strip().split("\t")
            chrom_sizes[tokens[0]] = int(tokens[1])
   
    # Convert batch size to be in terms of rows, not number of chunks
    batch_size = batch_size * chunk_size

    with h5py.File(out_path, "w") as f:
        # Store source paths
        f.create_dataset("bigwig_paths", data=np.array(bigwig_paths, dtype="S"))
        for chrom in sorted(chrom_sizes.keys()):
            chrom_size = chrom_sizes[chrom]
            num_batches = int(np.ceil(chrom_size / batch_size))
            chrom_dset = f.create_dataset(
                chrom, (chrom_size, len(bigwig_paths), 2, 2), dtype="f",
                compression="gzip", chunks=(chunk_size, len(bigwig_paths), 2, 2)
            )
            for i in tqdm.trange(num_batches, desc=chrom):
                start = i * batch_size
                end = min(chrom_size, (i + 1) * batch_size)

                values = np.stack([
                    np.stack([
                        np.stack([
                            np.nan_to_num(exp_readers[0][0].values(chrom, start, end)),
                            np.nan_to_num(exp_readers[0][1].values(chrom, start, end))
                        ], axis=1),
                        np.stack([
                            np.nan_to_num(exp_readers[1][0].values(chrom, start, end)),
                            np.nan_to_num(exp_readers[1][1].values(chrom, start, end))
                        ], axis=1)
                    ], axis=1) for exp_readers in bigwig_readers
                ], axis=1)

                chrom_dset[start : end] = values

@click.command()
@click.option(
    "--tf-name", "-t", required=True,
    help="Name of TF, needs to match prefix of TF-ChIPseq BigWig files"
)
@click.option(
    "--base-path", "-b", default=None,
    help="Path to directory containing BigWigs; defaults to /users/amtseng/tfmodisco/data/interim/ENCODE/{tf_name}/tf_chipseq_pseudorep_bigwigs/"
)
@click.option(
    "--chrom-sizes-path", "-c",
    default="/users/amtseng/genomes/hg38.canon.chrom.sizes",
    help="Path to canonical chromosome sizes"
)
@click.option(
    "--out-path", "-o", default=None,
    help="Destination for new HDF5; defaults to /users/amtseng/tfmodisco/data/processed/ENCODE/labels/{tf_name}/{tf_name}_pseudorep_profiles.h5"
)
@click.option(
    "--chunk-size", "-s", default=1500,
    help="Chunk size along chromosome length dimension for HDF5"
)
def main(tf_name, base_path, chrom_sizes_path, out_path, chunk_size):
    """
    Converts TF-ChIPseq and control ChIPseq profile BigWigs into an HDF5 file.
    HDF5 has separate datasets for each chromosome. Each chromosome's dataset
    is stored as an L x 2T x 2 array, where L is the size of the chromosome,
    T is the number of matched experiments (i.e. tasks), and 2 is for each
    strand. The dataset under the key `bigwig_paths` stores the paths for the
    source BigWigs.
    """
    if not base_path: 
        base_path = "/users/amtseng/tfmodisco/data/interim/ENCODE/%s/tf_chipseq_pseudorep_bigwigs/" % tf_name
    if not out_path:
        out_path = \
            "/users/amtseng/tfmodisco/data/processed/ENCODE/labels/%s/%s_pseudorep_profiles.h5" % (tf_name, tf_name)
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    bigwig_paths = fetch_bigwig_paths(base_path, tf_name)
    print("Found %d experiments/tasks" % len(bigwig_paths))
    create_hdf5(bigwig_paths, chrom_sizes_path, out_path, chunk_size)


if __name__ == "__main__":
    main()
