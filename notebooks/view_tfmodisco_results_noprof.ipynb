{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct links to results\n",
    "[TF-MoDISco results](#tfm-results)\n",
    "\n",
    "[Summary of motifs](#motif-summary)\n",
    "\n",
    "[TOMTOM matches to motifs](#tomtom)\n",
    "\n",
    "[Sample of seqlets for each motif](#seqlets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/users/amtseng/tfmodisco/src/\"))\n",
    "from tfmodisco.run_tfmodisco import import_shap_scores\n",
    "from motif_bench.match_motifs import match_motifs_to_database\n",
    "import modisco.visualization.viz_sequence as viz_sequence\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.cluster\n",
    "import scipy.cluster.hierarchy\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import modisco\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"figure.titlesize\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters/fetch arguments\n",
    "tf_name = os.environ[\"TFM_TF_NAME\"]\n",
    "shap_scores_path = os.environ[\"TFM_SHAP_PATH\"]\n",
    "tfm_results_path = os.environ[\"TFM_TFM_PATH\"]\n",
    "if \"TFM_TASK_INDEX\" in os.environ:\n",
    "    task_index = int(os.environ[\"TFM_TASK_INDEX\"])\n",
    "else:\n",
    "    task_index = None\n",
    "hyp_score_key = os.environ[\"TFM_HYP_SCORE_KEY\"]\n",
    "\n",
    "print(\"TF name: %s\" % tf_name)\n",
    "print(\"DeepSHAP scores path: %s\" % shap_scores_path)\n",
    "print(\"TF-MoDISco results path: %s\" % tfm_results_path)\n",
    "print(\"Task index: %s\" % task_index)\n",
    "print(\"Importance score key: %s\" % hyp_score_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and constants\n",
    "input_length = 1000\n",
    "shap_score_center_size = 400\n",
    "\n",
    "base_path = \"/users/amtseng/tfmodisco/\"\n",
    "data_path = os.path.join(base_path, \"data/processed/ENCODE/\")\n",
    "labels_path = os.path.join(data_path, \"labels/%s\" % tf_name)\n",
    "\n",
    "# Paths to original called peaks\n",
    "all_peak_beds = sorted([item for item in os.listdir(labels_path) if item.endswith(\".bed.gz\")])\n",
    "if task_index is None:\n",
    "    peak_bed_paths = [os.path.join(labels_path, item) for item in all_peak_beds]\n",
    "else:\n",
    "    peak_bed_paths = [os.path.join(labels_path, all_peak_beds[task_index])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coords(\n",
    "    seqlets_arr, one_hot_seqs, hyp_scores, pred_coords, input_length,\n",
    "    input_center_cut_size\n",
    "):\n",
    "    \"\"\"\n",
    "    From the seqlets object of a TF-MoDISco pattern's seqlets and alignments,\n",
    "    extracts the set of coordinates for the seqlets.\n",
    "    Arguments:\n",
    "        `seqlets_arr`: a TF-MoDISco pattern's seqlets object array (N-array)\n",
    "        `one_hot_seqs`: an N x R x 4 array of input sequences, where R is\n",
    "            the cut centered size\n",
    "        `hyp_scores`: an N x R x 4 array of hypothetical importance scores\n",
    "        `pred_coords`: an N x 3 object array of coordinates for the input sequences\n",
    "        `input_length`: length of original input sequences, I\n",
    "        `input_center_cut_size`: centered cut size of SHAP scores used\n",
    "    Returns an N x Q x 4 array of one-hot seqlet sequences, an N x Q x 4 array of\n",
    "    hypothetical seqlet importance scores, and an N x 3 object array of seqlet\n",
    "    coordinates, where Q is the seqlet length.\n",
    "    Note that it is important that the seqlet indices match exactly with the indices\n",
    "    out of the N. This should be the exact sequences in the original SHAP scores.\n",
    "    \"\"\"\n",
    "    seqlet_seqs, seqlet_hyps, seqlet_coords = [], [], []\n",
    "    \n",
    "    def seqlet_coord_to_input_coord(seqlet_coord):\n",
    "        return seqlet_coord + ((input_length - input_center_cut_size) // 2)\n",
    "        \n",
    "    # For each seqlet, fetch the true/predicted profiles\n",
    "    for seqlet in seqlets_arr:\n",
    "        coord_index = seqlet.coor.example_idx\n",
    "        seqlet_start = seqlet.coor.start\n",
    "        seqlet_end = seqlet.coor.end\n",
    "        seqlet_rc = seqlet.coor.is_revcomp\n",
    "        \n",
    "        # The one-hot-sequences and hypothetical scores are assumed to already by cut/centered,\n",
    "        # so the indices match the seqlet indices\n",
    "        if seqlet_rc:\n",
    "            seqlet_seqs.append(np.flip(one_hot_seqs[coord_index, seqlet_start:seqlet_end], axis=(0, 1)))\n",
    "            seqlet_hyps.append(np.flip(hyp_scores[coord_index, seqlet_start:seqlet_end], axis=(0, 1)))\n",
    "        else:\n",
    "            seqlet_seqs.append(one_hot_seqs[coord_index, seqlet_start:seqlet_end])\n",
    "            seqlet_hyps.append(hyp_scores[coord_index, seqlet_start:seqlet_end])\n",
    "            \n",
    "        # Get the coordinates of the seqlet based on the input coordinates\n",
    "        inp_start = seqlet_coord_to_input_coord(seqlet_start)\n",
    "        inp_end = seqlet_coord_to_input_coord(seqlet_end)\n",
    "        chrom, start, _ = pred_coords[coord_index]\n",
    "        seqlet_coords.append([chrom, start + inp_start, start + inp_end])\n",
    "    \n",
    "    return np.stack(seqlet_seqs), np.stack(seqlet_hyps), np.array(seqlet_coords, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tfmodisco_results(tfm_results_path, hyp_scores, one_hot_seqs, input_center_cut_size):\n",
    "    \"\"\"\n",
    "    Imports the TF-MoDISco results object.\n",
    "    Arguments:\n",
    "        `tfm_results_path`: path to HDF5 containing TF-MoDISco results\n",
    "        `hyp_scores`: hypothetical importance scores used for this run\n",
    "        `one_hot_seqs`: input sequences used for this run\n",
    "        `input_center_cut_size`: centered cut size of SHAP scores used\n",
    "    \"\"\" \n",
    "    # Everything should already be cut to `input_center_cut_size`\n",
    "    act_scores = hyp_scores * one_hot_seqs\n",
    "    \n",
    "    track_set = modisco.tfmodisco_workflow.workflow.prep_track_set(\n",
    "        task_names=[\"task0\"],\n",
    "        contrib_scores={\"task0\": act_scores},\n",
    "        hypothetical_contribs={\"task0\": hyp_scores},\n",
    "        one_hot=one_hot_seqs\n",
    "    )\n",
    "    \n",
    "    with h5py.File(tfm_results_path,\"r\") as f:\n",
    "        return modisco.tfmodisco_workflow.workflow.TfModiscoResults.from_hdf5(f, track_set=track_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_peak_table(peak_bed_paths):\n",
    "    tables = []\n",
    "    for peak_bed_path in peak_bed_paths:\n",
    "        table = pd.read_csv(\n",
    "            peak_bed_path, sep=\"\\t\", header=None,  # Infer compression\n",
    "            names=[\n",
    "                \"chrom\", \"peak_start\", \"peak_end\", \"name\", \"score\",\n",
    "                \"strand\", \"signal\", \"pval\", \"qval\", \"summit_offset\"\n",
    "            ]\n",
    "        )\n",
    "        # Add summit location column\n",
    "        table[\"summit\"] = table[\"peak_start\"] + table[\"summit_offset\"]\n",
    "        tables.append(table)\n",
    "    return pd.concat(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summit_distances(coords, peak_table):\n",
    "    \"\"\"\n",
    "    Given a set of coordinates, computes the distance of the center of each\n",
    "    coordinate to the nearest summit.\n",
    "    Arguments:\n",
    "        `coords`: an N x 3 object array of coordinates\n",
    "        `peak_table`: a 6-column table of peak data, as imported by\n",
    "            `import_peak_table`\n",
    "    Returns and N-array of integers, which is the distance of each coordinate\n",
    "    midpoint to the nearest coordinate.\n",
    "    \"\"\"\n",
    "    chroms = coords[:, 0]\n",
    "    midpoints = (coords[:, 1] + coords[:, 2]) // 2\n",
    "    dists = []\n",
    "    for i in range(len(coords)):\n",
    "        chrom = chroms[i]\n",
    "        midpoint = midpoints[i]\n",
    "        rows = peak_table[peak_table[\"chrom\"] == chrom]\n",
    "        dist_arr = (midpoint - rows[\"summit\"]).values\n",
    "        min_dist = dist_arr[np.argmin(np.abs(dist_arr))]\n",
    "        dists.append(min_dist)\n",
    "    return np.array(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summit_dists(summit_dists):\n",
    "    \"\"\"\n",
    "    Plots the distribution of seqlet distances to summits.\n",
    "    Arguments:\n",
    "        `summit_dists`: the array of distances as returned by\n",
    "            `get_summit_distances`\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    num_bins = max(len(summit_dists) // 30, 20)\n",
    "    plt.hist(summit_dists, bins=num_bins, color=\"purple\")\n",
    "    plt.title(\"Histogram of distance of seqlets to peak summits\")\n",
    "    plt.xlabel(\"Signed distance from seqlet center to nearest peak summit (bp)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_freqs = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "def info_content(track, pseudocount=0.001):\n",
    "    \"\"\"\n",
    "    Given an L x 4 track, computes information content for each base and\n",
    "    returns it as an L-array.\n",
    "    \"\"\"\n",
    "    num_bases = track.shape[1]\n",
    "    # Normalize track to probabilities along base axis\n",
    "    track_norm = (track + pseudocount) / (np.sum(track, axis=1, keepdims=True) + (num_bases * pseudocount))\n",
    "    ic = track_norm * np.log2(track_norm / np.expand_dims(background_freqs, axis=0))\n",
    "    return np.sum(ic, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pfm_to_pwm(pfm):\n",
    "    ic = info_content(pfm)\n",
    "    return pfm * np.expand_dims(ic, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dna_to_one_hot(dna_seq):\n",
    "    \"\"\"\n",
    "    Converts a single DNA sequence as a string to a one-hot encoding.\n",
    "    \"\"\"\n",
    "    dna_seq = dna_seq.upper()\n",
    "    one_hot_map = np.identity(5)[:, :-1]\n",
    "    base_vals = np.frombuffer(bytearray(dna_seq, \"utf8\"), dtype=np.int8)\n",
    "    base_vals[~np.isin(base_vals, np.array([65, 67, 71, 84]))] = 85\n",
    "    _, base_inds = np.unique(base_vals, return_inverse=True)\n",
    "    return one_hot_map[base_inds].reshape((len(dna_seq), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import SHAP scores, profile predictions, and TF-MoDISco results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SHAP coordinates and one-hot sequences\n",
    "hyp_scores, _, one_hot_seqs, shap_coords = import_shap_scores(shap_scores_path, hyp_score_key, center_cut_size=shap_score_center_size)\n",
    "# This cuts the sequences/scores off just as how TF-MoDISco saw them, but the coordinates are uncut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the set of peaks\n",
    "peak_table = import_peak_table(peak_bed_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TF-MoDISco results object\n",
    "tfm_obj = import_tfmodisco_results(tfm_results_path, hyp_scores, one_hot_seqs, shap_score_center_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some SHAP score tracks\n",
    "Plot the central region of some randomly selected actual importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index in np.random.choice(hyp_scores.shape[0], size=5, replace=False):\n",
    "    viz_sequence.plot_weights((hyp_scores[index] * one_hot_seqs[index])[100:300], subticks_frequency=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tfm-results\"></a>\n",
    "### Plot TF-MoDISco results\n",
    "Plot all motifs by metacluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "motifs = []  # Save the motifs as trimmed CWMs\n",
    "motif_pfms = []  # Save the trimmed PFMs, as well\n",
    "num_seqlets = []  # Number of seqlets for each motif\n",
    "motif_seqlets = {}  # Save seqlets of each motif\n",
    "metaclusters = tfm_obj.metacluster_idx_to_submetacluster_results\n",
    "num_metaclusters = len(metaclusters.keys())\n",
    "for metacluster_i, metacluster_key in enumerate(metaclusters.keys()):\n",
    "    metacluster = metaclusters[metacluster_key]\n",
    "    print(\"Metacluster %d/%d\" % (metacluster_i + 1, num_metaclusters))\n",
    "    print(\"==========================================\")\n",
    "    patterns = metacluster.seqlets_to_patterns_result.patterns\n",
    "    if not patterns:\n",
    "        break\n",
    "    motifs.append([])\n",
    "    motif_pfms.append([])\n",
    "    num_seqlets.append([])\n",
    "    num_patterns = len(patterns)\n",
    "    for pattern_i, pattern in enumerate(patterns):\n",
    "        seqlets = pattern.seqlets\n",
    "        print(\"Pattern %d/%d\" % (pattern_i + 1, num_patterns))\n",
    "        print(\"--------------------------------------\")\n",
    "        \n",
    "        pfm = pattern[\"sequence\"].fwd\n",
    "        hcwm = pattern[\"task0_hypothetical_contribs\"].fwd\n",
    "        cwm = pattern[\"task0_contrib_scores\"].fwd\n",
    "        print(\"%d seqlets\" % len(seqlets))\n",
    "        print(\"Sequence\")\n",
    "        viz_sequence.plot_weights(pfm, subticks_frequency=10)\n",
    "        print(\"Hypothetical contributions\")\n",
    "        viz_sequence.plot_weights(hcwm, subticks_frequency=10)\n",
    "        print(\"Actual contributions (CWM)\")\n",
    "        viz_sequence.plot_weights(cwm, subticks_frequency=10)\n",
    "        \n",
    "        # Trim motif based on IC\n",
    "        pfm_ic = info_content(pfm)\n",
    "        ic_trim_thresh = np.max(pfm_ic) * 0.2  # Cut off anything less than 20% of max IC\n",
    "        pass_inds = np.where(pfm_ic >= ic_trim_thresh)[0]\n",
    "        trimmed_cwm = cwm[np.min(pass_inds): np.max(pass_inds) + 1]\n",
    "        trimmed_pfm = pfm[np.min(pass_inds): np.max(pass_inds) + 1]\n",
    "        motifs[-1].append(trimmed_cwm)\n",
    "        motif_pfms[-1].append(trimmed_pfm)\n",
    "        \n",
    "        num_seqlets[-1].append(len(seqlets))\n",
    "        \n",
    "        seqlet_seqs, seqlet_hyps, seqlet_coords = extract_coords(\n",
    "            seqlets, one_hot_seqs, hyp_scores, shap_coords, input_length,\n",
    "            shap_score_center_size\n",
    "        )\n",
    "        \n",
    "        key_pair = (metacluster_key, pattern_i)\n",
    "        motif_seqlets[key_pair] = (seqlet_seqs, seqlet_hyps)\n",
    "\n",
    "        assert np.allclose(np.sum(seqlet_seqs, axis=0) / len(seqlet_seqs), pattern[\"sequence\"].fwd)\n",
    "        # ^Sanity check: PFM derived from seqlets match the PFM stored in the pattern\n",
    "        \n",
    "        print(\"Seqlet distance from summits\")\n",
    "        summit_dists = get_summit_distances(seqlet_coords, peak_table)\n",
    "        plot_summit_dists(summit_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"motif-summary\"></a>\n",
    "**Summary of motifs**\n",
    "\n",
    "Motifs are trimmed based on information content, and presented in descending order by number of supporting seqlets. The motifs are separated by metacluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(motifs)):\n",
    "    print(\"Metacluster %d/%d\" % (i + 1, num_metaclusters))\n",
    "    print(\"==========================================\")\n",
    "    for j in range(len(motifs[i])):\n",
    "        print(\"Motif %d/%d (number of seqlets: %d)\" % (j + 1, len(motifs[i]), num_seqlets[i][j]))\n",
    "        viz_sequence.plot_weights(motifs[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tomtom\"></a>\n",
    "**Top TOMTOM matches for each motif**\n",
    "\n",
    "Here, the motifs are plotted as PWMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(motifs)):\n",
    "    print(\"Metacluster %d/%d\" % (i + 1, num_metaclusters))\n",
    "    print(\"==========================================\")\n",
    "    tomtom_matches = match_motifs_to_database(motif_pfms[i])\n",
    "    for j in range(len(motifs[i])):\n",
    "        print(\"Motif %d/%d (number of seqlets: %d)\" % (j + 1, len(motifs[i]), num_seqlets[i][j]))\n",
    "        viz_sequence.plot_weights(pfm_to_pwm(motif_pfms[i][j]))\n",
    "        print(\"Top TOMTOM matches\")\n",
    "        for match_name, match_seq, match_qval in tomtom_matches[j]:\n",
    "            print(\"%s: q-val = %f\" % (match_name, match_qval))\n",
    "            viz_sequence.plot_weights(dna_to_one_hot(match_seq))\n",
    "        print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seqlets\"></a>\n",
    "**Sample of seqlets supporting each motif**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_seqlets_to_show = 10\n",
    "metaclusters = tfm_obj.metacluster_idx_to_submetacluster_results\n",
    "num_metaclusters = len(metaclusters.keys())\n",
    "for metacluster_i, metacluster_key in enumerate(metaclusters.keys()):\n",
    "    metacluster = metaclusters[metacluster_key]\n",
    "    print(\"Metacluster %d/%d\" % (metacluster_i + 1, num_metaclusters))\n",
    "    print(\"==========================================\")\n",
    "    patterns = metacluster.seqlets_to_patterns_result.patterns\n",
    "    if not patterns:\n",
    "        break\n",
    "    num_patterns = len(patterns)\n",
    "    for pattern_i, pattern in enumerate(patterns):\n",
    "        seqlets = pattern.seqlets\n",
    "        print(\"Pattern %d/%d\" % (pattern_i + 1, num_patterns))\n",
    "        print(\"--------------------------------------\")\n",
    "        print(\"Actual contributions (CWM)\")\n",
    "        viz_sequence.plot_weights(pattern[\"task0_contrib_scores\"].fwd, subticks_frequency=10)\n",
    "        \n",
    "        key_pair = (metacluster_key, pattern_i)\n",
    "        seqlet_seqs, seqlet_hyps = motif_seqlets[key_pair]\n",
    "        \n",
    "        print(\"Sample of seqlets\")\n",
    "        sample_size = min(num_seqlets_to_show, len(seqlet_seqs))\n",
    "        sample_inds = np.random.choice(len(seqlet_seqs), size=sample_size, replace=False)\n",
    "        for i in sample_inds:\n",
    "            viz_sequence.plot_weights(seqlet_seqs[i] * seqlet_hyps[i], subticks_frequency=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
