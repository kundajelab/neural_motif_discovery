{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../src/\"))\n",
    "import feature.util as feature_util\n",
    "import feature.make_profile_dataset as make_profile_dataset\n",
    "import model.profile_performance as profile_performance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import sklearn.cluster\n",
    "import scipy.cluster.hierarchy\n",
    "import json\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook(range(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths for data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_name = \"GABPA\"\n",
    "num_tasks = 9\n",
    "fold_num = 9\n",
    "task_index = 7\n",
    "\n",
    "# Note that this only works for single task queries right now\n",
    "\n",
    "files_spec_path = \"/users/amtseng/tfmodisco/data/processed/ENCODE/config/{0}/{0}_training_paths.json\".format(tf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the file specs\n",
    "with open(files_spec_path, \"r\") as f:\n",
    "    files_spec = json.load(f)\n",
    "peaks_beds = files_spec[\"peak_beds\"]\n",
    "profile_hdf5 = files_spec[\"profile_hdf5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the files and model, and some constants\n",
    "reference_fasta = \"/users/amtseng/genomes/hg38.fasta\"\n",
    "chrom_sizes = \"/users/amtseng/genomes/hg38.canon.chrom.sizes\"\n",
    "chrom_splits_json = \"/users/amtseng/tfmodisco/data/processed/ENCODE/chrom_splits.json\"\n",
    "input_length = 2114\n",
    "profile_length = 1000\n",
    "\n",
    "revcomp = False\n",
    "chrom_set_key = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chromosome sets\n",
    "with open(chrom_splits_json, \"r\") as f:\n",
    "    chrom_splits = json.load(f)\n",
    "split = chrom_splits[str(fold_num)]\n",
    "\n",
    "if chrom_set_key in (\"train\", \"val\", \"test\"):\n",
    "    chrom_set = split[chrom_set_key]\n",
    "else:\n",
    "    chrom_set = split[\"train\"] + split[\"val\"] + split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Prepare inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps coordinates to 1-hot encoded sequence\n",
    "coords_to_seq = feature_util.CoordsToSeq(reference_fasta, center_size_to_use=input_length)\n",
    "\n",
    "# Maps coordinates to profiles\n",
    "coords_to_vals = make_profile_dataset.CoordsToVals(profile_hdf5, profile_length)\n",
    "\n",
    "# Maps many coordinates to inputs sequences and profiles for the network\n",
    "def coords_to_network_inputs(coords):\n",
    "    input_seq = coords_to_seq(coords)\n",
    "    profs = coords_to_vals(coords)\n",
    "\n",
    "    if revcomp:\n",
    "        input_seq_rc = np.flip(input_seq, axis=(1, 2))\n",
    "        profs_rc = np.flip(profs, axis=(1, 3))\n",
    "        return np.concatenate([input_seq, input_seq_rc]), np.swapaxes(np.concatenate([profs, profs_rc]), 1, 2)\n",
    "    else:\n",
    "        return input_seq, np.swapaxes(profs, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import set of positive peaks\n",
    "pos_coords_table = pd.read_csv(peaks_beds[task_index], sep=\"\\t\", header=None, compression=\"gzip\")\n",
    "pos_coords_table = pos_coords_table[pos_coords_table[0].isin(chrom_set)]\n",
    "\n",
    "# Summit-center\n",
    "arr = pos_coords_table.values\n",
    "arr[:, 1] = arr[:, 1] + arr[:, 9] - (input_length // 2)\n",
    "arr[:, 2] = arr[:, 1] + input_length\n",
    "pos_coords = arr[:, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the set of all control and target profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_coords = len(pos_coords)\n",
    "num_samples = num_coords * (2 if revcomp else 1)\n",
    "target_profs = np.empty((num_samples, profile_length, 2))\n",
    "cont_profs = np.empty((num_samples, profile_length, 2))\n",
    "\n",
    "batch_size = 128\n",
    "num_batches = int(np.ceil(num_coords / batch_size))\n",
    "for i in tqdm.notebook.trange(num_batches):\n",
    "    batch_slice = slice(i * batch_size, (i + 1) * batch_size)\n",
    "    _, profiles = coords_to_network_inputs(pos_coords[batch_slice])\n",
    "    \n",
    "    target_profs[batch_slice] = profiles[:, task_index]\n",
    "    cont_profs[batch_slice] = profiles[:, num_tasks + task_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View profile correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profiles(target_profs, cont_profs):\n",
    "    \"\"\"\n",
    "    Plots the given profiles with a heatmap.\n",
    "    Arguments:\n",
    "        `target_profs`: an N x O x 2 NumPy array of target profiles, either as raw\n",
    "            counts or probabilities (they will be normalized)\n",
    "        `cont_profs`: an N x O x 2 NumPy array of control profiles, either as\n",
    "            raw counts or probabilities (they will be normalized)\n",
    "    \"\"\"\n",
    "    assert len(target_profs.shape) == 3\n",
    "    assert target_profs.shape == cont_profs.shape\n",
    "    num_profs, width, _ = target_profs.shape\n",
    "\n",
    "    # First, normalize the profiles along the output profile dimension\n",
    "    def normalize(arr, axis=0):\n",
    "        arr_sum = np.sum(arr, axis=axis, keepdims=True)\n",
    "        arr_sum[arr_sum == 0] = 1  # If 0, keep 0 as the quotient instead of dividing by 0\n",
    "        return arr / arr_sum\n",
    "    target_profs_norm = normalize(target_profs, axis=1)\n",
    "    cont_profs_norm = normalize(cont_profs, axis=1)\n",
    "\n",
    "    # Compute the mean profiles across all examples\n",
    "    target_profs_mean = np.mean(target_profs_norm, axis=0)\n",
    "    cont_profs_mean = np.mean(cont_profs_norm, axis=0)\n",
    "\n",
    "    # Perform k-means clustering on the target profiles, with the strands pooled\n",
    "    kmeans_clusters = max(5, num_profs // 50)  # Set number of clusters based on number of profiles, with minimum\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=kmeans_clusters)\n",
    "    cluster_assignments = kmeans.fit_predict(\n",
    "        np.reshape(target_profs_norm, (target_profs_norm.shape[0], -1))\n",
    "    )\n",
    "\n",
    "    # Perform hierarchical clustering on the cluster centers to determine optimal ordering\n",
    "    kmeans_centers = kmeans.cluster_centers_\n",
    "    cluster_order = scipy.cluster.hierarchy.leaves_list(\n",
    "        scipy.cluster.hierarchy.optimal_leaf_ordering(\n",
    "            scipy.cluster.hierarchy.linkage(kmeans_centers, method=\"centroid\"), kmeans_centers\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Order the profiles so that the cluster assignments follow the optimal ordering\n",
    "    cluster_inds = []\n",
    "    for cluster_id in cluster_order:\n",
    "        cluster_inds.append(np.where(cluster_assignments == cluster_id)[0])\n",
    "    cluster_inds = np.concatenate(cluster_inds)\n",
    "\n",
    "    # Compute a matrix of profiles, normalized to the maximum height, ordered by clusters\n",
    "    def make_profile_matrix(flat_profs, order_inds):\n",
    "        matrix = flat_profs[order_inds]\n",
    "        maxes = np.max(matrix, axis=1, keepdims=True)\n",
    "        maxes[maxes == 0] = 1  # If 0, keep 0 as the quotient instead of dividing by 0\n",
    "        return matrix / maxes\n",
    "    target_matrix = make_profile_matrix(target_profs_norm, cluster_inds)\n",
    "    cont_matrix = make_profile_matrix(cont_profs_norm, cluster_inds)\n",
    "\n",
    "    # Create a figure with the right dimensions\n",
    "    mean_height = 4\n",
    "    heatmap_height = min(num_profs * 0.004, 8)\n",
    "    fig_height = mean_height + (2 * heatmap_height)\n",
    "    fig, ax = plt.subplots(\n",
    "        3, 2, figsize=(16, fig_height), sharex=True,\n",
    "        gridspec_kw={\n",
    "            \"width_ratios\": [1, 1],\n",
    "            \"height_ratios\": [mean_height / fig_height, heatmap_height / fig_height, heatmap_height / fig_height]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot the average profiles\n",
    "    ax[0, 0].plot(target_profs_mean[:, 0], color=\"darkslateblue\")\n",
    "    ax[0, 0].plot(-target_profs_mean[:, 1], color=\"darkorange\")\n",
    "    ax[0, 1].plot(cont_profs_mean[:, 0], color=\"darkslateblue\")\n",
    "    ax[0, 1].plot(-cont_profs_mean[:, 1], color=\"darkorange\")\n",
    "\n",
    "    # Set axes on average profiles\n",
    "    max_mean_val = max(np.max(target_profs_mean), np.max(cont_profs_mean))\n",
    "    mean_ylim = max_mean_val * 1.05  # Make 5% higher\n",
    "    ax[0, 0].set_title(\"Target profiles\")\n",
    "    ax[0, 0].set_ylabel(\"Average probability\")\n",
    "    ax[0, 1].set_title(\"Control profiles\")\n",
    "    for j in (0, 1):\n",
    "        ax[0, j].set_ylim(-mean_ylim, mean_ylim)\n",
    "        ax[0, j].label_outer()\n",
    "\n",
    "    # Plot the heatmaps\n",
    "    ax[1, 0].imshow(target_matrix[:, :, 0], interpolation=\"nearest\", aspect=\"auto\", cmap=\"Blues\")\n",
    "    ax[1, 1].imshow(cont_matrix[:, :, 0], interpolation=\"nearest\", aspect=\"auto\", cmap=\"Blues\")\n",
    "    ax[2, 0].imshow(target_matrix[:, :, 1], interpolation=\"nearest\", aspect=\"auto\", cmap=\"Oranges\")\n",
    "    ax[2, 1].imshow(cont_matrix[:, :, 1], interpolation=\"nearest\", aspect=\"auto\", cmap=\"Oranges\")\n",
    "\n",
    "    # Set axes on heatmaps\n",
    "    for i in (1, 2):\n",
    "        for j in (0, 1):\n",
    "            ax[i, j].set_yticks([])\n",
    "            ax[i, j].set_yticklabels([])\n",
    "            ax[i, j].label_outer()\n",
    "    width = target_matrix.shape[1]\n",
    "    delta = 100\n",
    "    num_deltas = (width // 2) // delta\n",
    "    labels = list(range(max(-width // 2, -num_deltas * delta), min(width // 2, num_deltas * delta) + 1, delta))\n",
    "    tick_locs = [label + max(width // 2, num_deltas * delta) for label in labels]\n",
    "    for j in (0, 1):\n",
    "        ax[2, j].set_xticks(tick_locs)\n",
    "        ax[2, j].set_xticklabels(labels)\n",
    "        ax[2, j].set_xlabel(\"Distance from summit center (bp)\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_profiles(target_profs, cont_profs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_pears, prof_spear, _ = profile_performance.profile_corr_mse(\n",
    "    np.expand_dims(target_profs, axis=1), np.expand_dims(cont_profs, axis=1),\n",
    "    1, 0, False, False\n",
    ")\n",
    "prof_pears, prof_spear = prof_pears[:, 0], prof_spear[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, figsize=(20, 8))\n",
    "ax[0].hist(prof_pears, bins=100)\n",
    "ax[1].hist(prof_spear, bins=100)\n",
    "ax[0].set_xlabel(\"Profile Pearson correlations\")\n",
    "ax[1].set_xlabel(\"Proflie Spearman correlations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View count correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = np.sum(target_profs, axis=1)\n",
    "cont_counts = np.sum(cont_profs, axis=1)\n",
    "log_target_counts = np.ravel(np.log(target_counts + 1))\n",
    "log_cont_counts = np.ravel(np.log(cont_counts + 1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(log_target_counts, log_cont_counts, alpha=0.1)\n",
    "\n",
    "xlims = ax.get_xlim()\n",
    "ylims = ax.get_ylim()\n",
    "min_limit, max_limit = np.min([xlims, ylims]), np.max([xlims, ylims])\n",
    "ax.plot([min_limit, max_limit], [min_limit, max_limit], color=\"black\", linestyle=\"--\")\n",
    "ax.set_xlim(xlims)\n",
    "ax.set_ylim(ylims)\n",
    "\n",
    "ax.set_xlabel(\"Log target counts\")\n",
    "ax.set_ylabel(\"Log control counts\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Pearson: %f\" % scipy.stats.pearsonr(log_target_counts, log_cont_counts)[0])\n",
    "print(\"Spearman: %f\" % scipy.stats.spearmanr(log_target_counts, log_cont_counts)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
