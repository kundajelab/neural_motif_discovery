{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"figure.titlesize\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_name = \"NR3C1\"\n",
    "num_tasks = 7\n",
    "best_fold = 6\n",
    "test_set_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_path = \"/users/amtseng/tfmodisco/results/peak_predictions/{0}/{0}_peak_prediction_performance_fold{1}.h5\".format(tf_name, best_fold)\n",
    "\n",
    "files_spec_json = \"/users/amtseng/tfmodisco/data/processed/ENCODE/config/NR3C1/NR3C1_training_paths.json\"\n",
    "with open(files_spec_json, \"r\") as f:\n",
    "    files_spec = json.load(f)\n",
    "\n",
    "chrom_splits_json = \"/users/amtseng/tfmodisco/data/processed/ENCODE/chrom_splits.json\"\n",
    "with open(chrom_splits_json, \"r\") as f:\n",
    "    chrom_splits = json.load(f)\n",
    "test_set_chroms = chrom_splits[str(best_fold)][\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 1346\n",
    "profile_length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import peak coordinates for each task\n",
    "For each task, import the set of peaks belonging to that task. This obtains a set of indices for coordinates in `prediction_path` that correspond to each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in tables containing the peak coordinates, padded to `input_length`\n",
    "task_coords = []\n",
    "assert len(files_spec[\"peak_beds\"]) == num_tasks\n",
    "for peak_bed_path in files_spec[\"peak_beds\"]:\n",
    "    table = pd.read_csv(\n",
    "        peak_bed_path, sep=\"\\t\", header=None,  # Infer compression\n",
    "        names=[\n",
    "            \"chrom\", \"peak_start\", \"peak_end\", \"name\", \"score\",\n",
    "            \"strand\", \"signal\", \"pval\", \"qval\", \"summit_offset\"\n",
    "        ]\n",
    "    )\n",
    "    # Add summit location column:\n",
    "    table[\"summit\"] = table[\"peak_start\"] + table[\"summit_offset\"]\n",
    "    \n",
    "    # Add start and end columns, at proper length\n",
    "    table[\"start\"] = table[\"summit\"] - (input_length // 2)\n",
    "    table[\"end\"] = table[\"start\"] + input_length\n",
    "    \n",
    "    task_coords.append(table[[\"chrom\", \"start\", \"end\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in coordinates within the saved predictions\n",
    "with h5py.File(prediction_path, \"r\") as f:\n",
    "    pred_coords = pd.DataFrame(\n",
    "        data={\n",
    "            \"chrom\": f[\"coords\"][\"coords_chrom\"][:].astype(str),\n",
    "            \"start\": f[\"coords\"][\"coords_start\"][:],\n",
    "            \"end\": f[\"coords\"][\"coords_end\"][:]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "# If specified, limit only to test-set chromosomes\n",
    "pred_coords = pred_coords[pred_coords[\"chrom\"].isin(test_set_chroms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the set of indices of prediction coordinates corresponding to each task\n",
    "task_coord_inds = []\n",
    "for task_index in range(num_tasks):\n",
    "    task_inds = pred_coords.reset_index().drop_duplicates([\"chrom\", \"start\", \"end\"]).merge(\n",
    "        task_coords[task_index].reset_index(), on=[\"chrom\", \"start\", \"end\"]\n",
    "    ).sort_values(\"index_y\")[\"index_x\"].values\n",
    "    \n",
    "    task_coord_inds.append(np.sort(task_inds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the set of performance metrics\n",
    "For each task, import the set of performance metrics corresponding to the peaks in that task, for that task only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tf_metrics(tf_name, prediction_path, task_coord_inds):\n",
    "    \"\"\"\n",
    "    Imports the set of all metrics for the given TF, for each task.\n",
    "    `task_coord_inds` is the set of indices corresponding to each task\n",
    "    within the saved prediction coordinates.\n",
    "    Returns a dictionary of the following format:\n",
    "        `nll`: [\n",
    "            <task 0 NLL vector>\n",
    "            <task 1 NLL vector>\n",
    "            ...\n",
    "        ],\n",
    "        `count_mse`: [\n",
    "            <task 0 MSE value>\n",
    "            <task 1 MSE value>\n",
    "            ...\n",
    "        ]\n",
    "        ...\n",
    "    \"\"\"\n",
    "    perfs = {}\n",
    "    \n",
    "    pred_reader = h5py.File(prediction_path, \"r\")\n",
    "    perf_reader = pred_reader[\"performance\"]\n",
    "    \n",
    "    for key in perf_reader.keys():\n",
    "        if len(perf_reader[key].shape) >= 2:  # Profile metric\n",
    "            metrics = [\n",
    "                perf_reader[key][task_coord_inds[task_index]][:, task_index]\n",
    "                for task_index in range(num_tasks)\n",
    "            ]\n",
    "        else:  # Count metric\n",
    "            metrics = [\n",
    "                perf_reader[key][task_index]\n",
    "                for task_index in range(num_tasks)\n",
    "            ]\n",
    "        perfs[key] = metrics\n",
    "\n",
    "    # Compute the normalized NLL for the actual performance\n",
    "    norm_nlls = []\n",
    "    for task_index in range(num_tasks):\n",
    "        nlls = perfs[\"nll\"][task_index]\n",
    "        num_reads = np.mean(np.sum(\n",
    "            pred_reader[\"predictions\"][\"true_profs\"][task_coord_inds[task_index]], axis=2\n",
    "        ), axis=2)[:, task_index]\n",
    "        norm_nlls.append(nlls / num_reads)\n",
    "    perfs[\"norm_nll\"] = norm_nlls\n",
    "\n",
    "    pred_reader.close()\n",
    "    \n",
    "    return perfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perf_bounds(tf_perfs, tf_name):\n",
    "    def create_violins(ax, perfs_list):\n",
    "        num_perfs = len(perfs_list)\n",
    "        \n",
    "        q1, med, q3 = np.stack([\n",
    "            np.nanpercentile(data, [25, 50, 70], axis=0) for data in perfs_list\n",
    "        ], axis=1)\n",
    "        iqr = q3 - q1\n",
    "        lower_outlier = q1 - (1.5 * iqr)\n",
    "        upper_outlier = q3 + (1.5 * iqr)\n",
    "\n",
    "        \n",
    "        sorted_clipped_data = [  # Remove outliers\n",
    "            np.sort(vec[(vec >= lower_outlier[i]) & (vec <= upper_outlier[i])])\n",
    "            for i, vec in enumerate(perfs_list)\n",
    "        ]\n",
    "        \n",
    "        plot_parts = ax.violinplot(\n",
    "            sorted_clipped_data, showmeans=False, showmedians=False, showextrema=False\n",
    "        )\n",
    "        violin_parts = plot_parts[\"bodies\"]\n",
    "        for i in range(num_perfs):\n",
    "            violin_parts[i].set_facecolor(\"mediumorchid\")\n",
    "            violin_parts[i].set_edgecolor(\"mediumorchid\")\n",
    "            violin_parts[i].set_alpha(0.7)\n",
    "        \n",
    "        inds = np.arange(1, num_perfs + 1)\n",
    "        ax.vlines(inds, q1, q3, color=\"black\", linewidth=5, zorder=1)\n",
    "        ax.scatter(inds, med, marker=\"o\", color=\"white\", s=30, zorder=2)\n",
    "    \n",
    "    # Profile metrics\n",
    "    for metric_key, metric_name in [\n",
    "        (\"nll\", \"NLL\"), (\"norm_nll\", \"Normalized NLL\"), (\"jsd\", \"JSD\"), (\"profile_mse\", \"Profile MSE\"),\n",
    "        (\"profile_pearson\", \"Profile Pearson\"), (\"profile_spearman\", \"Profile Spearman\")\n",
    "    ]:\n",
    "        fig, ax = plt.subplots(figsize=(20, 5))\n",
    "        create_violins(ax, tf_perfs[metric_key])\n",
    "        ax.set_title(\"Task-specific %s of %s\" % (metric_name, tf_name))\n",
    "        ax.set_xticks(np.arange(1, num_tasks + 1))\n",
    "        ax.set_xticklabels([\"Task %d\" % task_index for task_index in range(num_tasks)])\n",
    "        plt.show()\n",
    "        \n",
    "    # Count metrics\n",
    "    for metric_key, metric_name in [\n",
    "        (\"count_mse\", \"Count MSE\"), (\"count_pearson\", \"Count Pearson\"), (\"count_spearman\", \"Count Spearman\")\n",
    "    ]:\n",
    "        fig, ax = plt.subplots(figsize=(20, 5))\n",
    "        label_locs = np.arange(num_tasks)  # Location of labels\n",
    "        ax.bar(\n",
    "            label_locs, tf_perfs[metric_key],\n",
    "            color=(num_tasks * [\"mediumorchid\"]), alpha=0.7\n",
    "        )\n",
    "        ax.set_title(\"Task-specific %s of %s\" % (metric_name, tf_name))\n",
    "        ax.set_xticks(label_locs)\n",
    "        ax.set_xticklabels([\"Task %d\" % task_index for task_index in range(num_tasks)])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = import_tf_metrics(tf_name, prediction_path, task_coord_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_perf_bounds(perfs, tf_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
