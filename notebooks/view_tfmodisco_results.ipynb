{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct links to results\n",
    "[TF-MoDISco results](#tfm-results)\n",
    "\n",
    "[Summary of motifs](#motif-summary)\n",
    "\n",
    "[Sample of seqlets for each motif](#seqlets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/users/amtseng/tfmodisco/src/tfmodisco\"))\n",
    "from run_tfmodisco import import_shap_scores\n",
    "import modisco.visualization.viz_sequence as viz_sequence\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.cluster\n",
    "import scipy.cluster.hierarchy\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import modisco\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"figure.titlesize\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters/fetch arguments\n",
    "tf_name = os.environ[\"TFM_TF_NAME\"]\n",
    "fold = int(os.environ[\"TFM_FOLD\"])\n",
    "if \"TFM_TASK_INDEX\" in os.environ:\n",
    "    task_index = int(os.environ[\"TFM_TASK_INDEX\"])\n",
    "else:\n",
    "    task_index = None\n",
    "shap_scores_path = os.environ[\"TFM_SHAP_PATH\"]\n",
    "tfm_results_path = os.environ[\"TFM_TFM_PATH\"]\n",
    "    \n",
    "print(\"TF name: %s\" % tf_name)\n",
    "print(\"Fold: %s\" % fold)\n",
    "print(\"Task index: %s\" % task_index)\n",
    "print(\"DeepSHAP scores path: %s\" % shap_scores_path)\n",
    "print(\"TF-MoDISco results path: %s\" % tfm_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and constants\n",
    "input_length, profile_length = 1346, 1000\n",
    "shap_score_center_size = 400\n",
    "profile_display_center_size = 400\n",
    "\n",
    "base_path = \"/users/amtseng/tfmodisco/\"\n",
    "data_path = os.path.join(base_path, \"data/processed/ENCODE/\")\n",
    "labels_path = os.path.join(data_path, \"labels/%s\" % tf_name)\n",
    "results_path = os.path.join(base_path, \"results\")\n",
    "\n",
    "# Paths to saved predictions; used only for plotting profiles\n",
    "preds_path = os.path.join(results_path, \"peak_predictions/{0}/{0}_peak_prediction_performance_fold{1}.h5\".format(tf_name, fold))\n",
    "    \n",
    "# Paths to original called peaks\n",
    "all_peak_beds = sorted([item for item in os.listdir(labels_path) if item.endswith(\".bed.gz\")])\n",
    "if task_index is None:\n",
    "    peak_bed_paths = [os.path.join(labels_path, item) for item in all_peak_beds]\n",
    "else:\n",
    "    peak_bed_paths = [os.path.join(labels_path, all_peak_beds[task_index])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_profiles(preds_path):\n",
    "    \"\"\"\n",
    "    Imports the set of profile predictions.\n",
    "    Arguments:\n",
    "        `preds_path`: path to predictions/performance metrics of the model\n",
    "    Returns an M x T x O x 2 array of true profile counts, an M x T x O x 2\n",
    "    array of predicted profile probabilities, and an M x 3 object array of\n",
    "    corresponding coordinates.\n",
    "    \"\"\"\n",
    "    with h5py.File(preds_path, \"r\") as f:\n",
    "        num_seqs, num_tasks, input_length, _ = f[\"predictions\"][\"true_profs\"].shape\n",
    "        batch_size = min(1000, num_seqs)\n",
    "        num_batches = int(np.ceil(num_seqs / batch_size))\n",
    "        \n",
    "        true_profs = np.empty((num_seqs, num_tasks, input_length, 2))\n",
    "        pred_profs = np.empty((num_seqs, num_tasks, input_length, 2))\n",
    "        coords = np.empty((num_seqs, 3), dtype=object)\n",
    "        \n",
    "        for i in tqdm.notebook.trange(num_batches, desc=\"Importing predictions\"):\n",
    "            batch_slice = slice(i * batch_size, (i + 1) * batch_size)\n",
    "            true_profs[batch_slice] = f[\"predictions\"][\"true_profs\"][batch_slice]\n",
    "            pred_profs[batch_slice] = np.exp(f[\"predictions\"][\"log_pred_profs\"][batch_slice])\n",
    "            coords[batch_slice, 0] = f[\"coords\"][\"coords_chrom\"][batch_slice].astype(str)\n",
    "            coords[batch_slice, 1] = f[\"coords\"][\"coords_start\"][batch_slice]\n",
    "            coords[batch_slice, 2] = f[\"coords\"][\"coords_end\"][batch_slice]\n",
    "    \n",
    "    return true_profs, pred_profs, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_profiles_and_coords(\n",
    "    seqlets_arr, one_hot_seqs, hyp_scores, true_profs, pred_profs, pred_coords,\n",
    "    input_length, profile_length, input_center_cut_size, profile_center_cut_size,\n",
    "    task_index=None\n",
    "):\n",
    "    \"\"\"\n",
    "    From the seqlets object of a TF-MoDISco pattern's seqlets and alignments,\n",
    "    extracts the predicted and observed profiles of the model, as well as the\n",
    "    set of coordinates for the seqlets.\n",
    "    Arguments:\n",
    "        `seqlets_arr`: a TF-MoDISco pattern's seqlets object array (N-array)\n",
    "        `one_hot_seqs`: an N x R x 4 array of input sequences, where R is\n",
    "            the cut centered size\n",
    "        `hyp_scores`: an N x R x 4 array of hypothetical importance scores\n",
    "        `true_profs`: an N x T x O x 2 array of true profile counts\n",
    "        `pred_profs`: an N x T x O x 2 array of predicted profile probabilities\n",
    "        `pred_coords`: an N x 3 object array of coordinates for the input sequence\n",
    "            underlying the predictions\n",
    "        `input_length`: length of original input sequences, I\n",
    "        `profile_length`: length of profile predictions, O\n",
    "        `input_center_cut_size`: centered cut size of SHAP scores used\n",
    "        `profile_center_cut_size`: size to cut profiles to when returning them, P\n",
    "        `task_index`: index of task to focus on for profiles; if None, returns\n",
    "            profiles for all tasks\n",
    "    Returns an N x (T or 1) x P x 2 array of true profile counts, an\n",
    "    N x (T or 1) x P x 2 array of predicted profile probabilities, an N x Q x 4\n",
    "    array of one-hot seqlet sequences, an N x Q x 4 array of hypothetical seqlet\n",
    "    importance scores, and an N x 3 object array of seqlet coordinates, where P\n",
    "    is the profile cut size and Q is the seqlet length. Returned profiles are\n",
    "    centered at the same center as the seqlets.\n",
    "    Note that it is important that the seqlet indices match exactly with the indices\n",
    "    out of the N. This should be the exact sequences in the original SHAP scores.\n",
    "    \"\"\"\n",
    "    true_seqlet_profs, pred_seqlet_profs, seqlet_seqs, seqlet_hyps, seqlet_coords = [], [], [], [], []\n",
    "    \n",
    "    def seqlet_coord_to_profile_coord(seqlet_coord):\n",
    "        return seqlet_coord + ((input_length - input_center_cut_size) // 2) - ((input_length - profile_length) // 2)\n",
    "    \n",
    "    def seqlet_coord_to_input_coord(seqlet_coord):\n",
    "        return seqlet_coord + ((input_length - input_center_cut_size) // 2)\n",
    "        \n",
    "    # For each seqlet, fetch the true/predicted profiles\n",
    "    for seqlet in seqlets_arr:\n",
    "        coord_index = seqlet.coor.example_idx\n",
    "        seqlet_start = seqlet.coor.start\n",
    "        seqlet_end = seqlet.coor.end\n",
    "        seqlet_rc = seqlet.coor.is_revcomp\n",
    "        \n",
    "        # Get indices of profile to cut out\n",
    "        seqlet_center = (seqlet_start + seqlet_end) // 2\n",
    "        prof_center = seqlet_coord_to_profile_coord(seqlet_center)\n",
    "        prof_start = prof_center - (profile_center_cut_size // 2)\n",
    "        prof_end = prof_start + profile_center_cut_size\n",
    "        \n",
    "        task_start, task_end = (task_index, task_index + 1) if task_index is not None else (None, None)\n",
    "        true_prof = true_profs[coord_index, task_start:task_end, prof_start:prof_end]  # (T or 1) x P x 2\n",
    "        pred_prof = pred_profs[coord_index, task_start:task_end, prof_start:prof_end]  # (T or 1) x P x 2\n",
    "        true_seqlet_profs.append(true_prof)\n",
    "        pred_seqlet_profs.append(pred_prof)\n",
    "        \n",
    "        # The one-hot-sequences and hypothetical scores are assumed to already by cut/centered,\n",
    "        # so the indices match the seqlet indices\n",
    "        if seqlet_rc:\n",
    "            seqlet_seqs.append(np.flip(one_hot_seqs[coord_index, seqlet_start:seqlet_end], axis=(0, 1)))\n",
    "            seqlet_hyps.append(np.flip(hyp_scores[coord_index, seqlet_start:seqlet_end], axis=(0, 1)))\n",
    "        else:\n",
    "            seqlet_seqs.append(one_hot_seqs[coord_index, seqlet_start:seqlet_end])\n",
    "            seqlet_hyps.append(hyp_scores[coord_index, seqlet_start:seqlet_end])\n",
    "            \n",
    "        # Get the coordinates of the seqlet based on the input coordinates\n",
    "        inp_start = seqlet_coord_to_input_coord(seqlet_start)\n",
    "        inp_end = seqlet_coord_to_input_coord(seqlet_end)\n",
    "        chrom, start, _ = pred_coords[coord_index]\n",
    "        seqlet_coords.append([chrom, start + inp_start, start + inp_end])\n",
    "    \n",
    "    return np.stack(true_seqlet_profs), np.stack(pred_seqlet_profs), np.stack(seqlet_seqs), np.stack(seqlet_hyps), np.array(seqlet_coords, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tfmodisco_results(tfm_results_path, hyp_scores, one_hot_seqs, input_center_cut_size):\n",
    "    \"\"\"\n",
    "    Imports the TF-MoDISco results object.\n",
    "    Arguments:\n",
    "        `tfm_results_path`: path to HDF5 containing TF-MoDISco results\n",
    "        `hyp_scores`: hypothetical importance scores used for this run\n",
    "        `one_hot_seqs`: input sequences used for this run\n",
    "        `input_center_cut_size`: centered cut size of SHAP scores used\n",
    "    \"\"\" \n",
    "    # Everything should already be cut to `input_center_cut_size`\n",
    "    act_scores = hyp_scores * one_hot_seqs\n",
    "    \n",
    "    track_set = modisco.tfmodisco_workflow.workflow.prep_track_set(\n",
    "        task_names=[\"task0\"],\n",
    "        contrib_scores={\"task0\": act_scores},\n",
    "        hypothetical_contribs={\"task0\": hyp_scores},\n",
    "        one_hot=one_hot_seqs\n",
    "    )\n",
    "    \n",
    "    with h5py.File(tfm_results_path,\"r\") as f:\n",
    "        return modisco.tfmodisco_workflow.workflow.TfModiscoResults.from_hdf5(f, track_set=track_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profiles(seqlet_true_profs, seqlet_pred_profs, kmeans_clusters=5):\n",
    "    \"\"\"\n",
    "    Plots the given profiles with a heatmap.\n",
    "    Arguments:\n",
    "        `seqlet_true_profs`: an N x O x 2 NumPy array of true profiles, either as raw\n",
    "            counts or probabilities (they will be normalized)\n",
    "        `seqlet_pred_profs`: an N x O x 2 NumPy array of predicted profiles, either as\n",
    "            raw counts or probabilities (they will be normalized)\n",
    "        `kmeans_cluster`: when displaying profile heatmaps, there will be this\n",
    "            many clusters\n",
    "    \"\"\"\n",
    "    assert len(seqlet_true_profs.shape) == 3\n",
    "    assert seqlet_true_profs.shape == seqlet_pred_profs.shape\n",
    "    num_profs, width, _ = seqlet_true_profs.shape\n",
    "\n",
    "    # First, normalize the profiles along the output profile dimension\n",
    "    def normalize(arr, axis=0):\n",
    "        arr_sum = np.sum(arr, axis=axis, keepdims=True)\n",
    "        arr_sum[arr_sum == 0] = 1  # If 0, keep 0 as the quotient instead of dividing by 0\n",
    "        return arr / arr_sum\n",
    "    true_profs_norm = normalize(seqlet_true_profs, axis=1)\n",
    "    pred_profs_norm = normalize(seqlet_pred_profs, axis=1)\n",
    "\n",
    "    # Compute the mean profiles across all examples\n",
    "    true_profs_mean = np.mean(true_profs_norm, axis=0)\n",
    "    pred_profs_mean = np.mean(pred_profs_norm, axis=0)\n",
    "\n",
    "    # Perform k-means clustering on the predicted profiles, with the strands pooled\n",
    "    kmeans_clusters = max(5, num_profs // 50)  # Set number of clusters based on number of profiles, with minimum\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters=kmeans_clusters)\n",
    "    cluster_assignments = kmeans.fit_predict(\n",
    "        np.reshape(pred_profs_norm, (pred_profs_norm.shape[0], -1))\n",
    "    )\n",
    "\n",
    "    # Perform hierarchical clustering on the cluster centers to determine optimal ordering\n",
    "    kmeans_centers = kmeans.cluster_centers_\n",
    "    cluster_order = scipy.cluster.hierarchy.leaves_list(\n",
    "        scipy.cluster.hierarchy.optimal_leaf_ordering(\n",
    "            scipy.cluster.hierarchy.linkage(kmeans_centers, method=\"centroid\"), kmeans_centers\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Order the profiles so that the cluster assignments follow the optimal ordering\n",
    "    cluster_inds = []\n",
    "    for cluster_id in cluster_order:\n",
    "        cluster_inds.append(np.where(cluster_assignments == cluster_id)[0])\n",
    "    cluster_inds = np.concatenate(cluster_inds)\n",
    "\n",
    "    # Compute a matrix of profiles, normalized to the maximum height, ordered by clusters\n",
    "    def make_profile_matrix(flat_profs, order_inds):\n",
    "        matrix = flat_profs[order_inds]\n",
    "        maxes = np.max(matrix, axis=1, keepdims=True)\n",
    "        maxes[maxes == 0] = 1  # If 0, keep 0 as the quotient instead of dividing by 0\n",
    "        return matrix / maxes\n",
    "    true_matrix = make_profile_matrix(true_profs_norm, cluster_inds)\n",
    "    pred_matrix = make_profile_matrix(pred_profs_norm, cluster_inds)\n",
    "\n",
    "    # Create a figure with the right dimensions\n",
    "    mean_height = 4\n",
    "    heatmap_height = min(num_profs * 0.004, 8)\n",
    "    fig_height = mean_height + (2 * heatmap_height)\n",
    "    fig, ax = plt.subplots(\n",
    "        3, 2, figsize=(16, fig_height), sharex=True,\n",
    "        gridspec_kw={\n",
    "            \"width_ratios\": [1, 1],\n",
    "            \"height_ratios\": [mean_height / fig_height, heatmap_height / fig_height, heatmap_height / fig_height]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot the average predictions\n",
    "    ax[0, 0].plot(true_profs_mean[:, 0], color=\"darkslateblue\")\n",
    "    ax[0, 0].plot(-true_profs_mean[:, 1], color=\"darkorange\")\n",
    "    ax[0, 1].plot(pred_profs_mean[:, 0], color=\"darkslateblue\")\n",
    "    ax[0, 1].plot(-pred_profs_mean[:, 1], color=\"darkorange\")\n",
    "\n",
    "    # Set axes on average predictions\n",
    "    max_mean_val = max(np.max(true_profs_mean), np.max(pred_profs_mean))\n",
    "    mean_ylim = max_mean_val * 1.05  # Make 5% higher\n",
    "    ax[0, 0].set_title(\"True profiles\")\n",
    "    ax[0, 0].set_ylabel(\"Average probability\")\n",
    "    ax[0, 1].set_title(\"Predicted profiles\")\n",
    "    for j in (0, 1):\n",
    "        ax[0, j].set_ylim(-mean_ylim, mean_ylim)\n",
    "        ax[0, j].label_outer()\n",
    "\n",
    "    # Plot the heatmaps\n",
    "    ax[1, 0].imshow(true_matrix[:, :, 0], interpolation=\"nearest\", aspect=\"auto\", cmap=\"Blues\")\n",
    "    ax[1, 1].imshow(pred_matrix[:, :, 0], interpolation=\"nearest\", aspect=\"auto\", cmap=\"Blues\")\n",
    "    ax[2, 0].imshow(true_matrix[:, :, 1], interpolation=\"nearest\", aspect=\"auto\", cmap=\"Oranges\")\n",
    "    ax[2, 1].imshow(pred_matrix[:, :, 1], interpolation=\"nearest\", aspect=\"auto\", cmap=\"Oranges\")\n",
    "\n",
    "    # Set axes on heatmaps\n",
    "    for i in (1, 2):\n",
    "        for j in (0, 1):\n",
    "            ax[i, j].set_yticks([])\n",
    "            ax[i, j].set_yticklabels([])\n",
    "            ax[i, j].label_outer()\n",
    "    width = true_matrix.shape[1]\n",
    "    delta = 100\n",
    "    num_deltas = (width // 2) // delta\n",
    "    labels = list(range(max(-width // 2, -num_deltas * delta), min(width // 2, num_deltas * delta) + 1, delta))\n",
    "    tick_locs = [label + max(width // 2, num_deltas * delta) for label in labels]\n",
    "    for j in (0, 1):\n",
    "        ax[2, j].set_xticks(tick_locs)\n",
    "        ax[2, j].set_xticklabels(labels)\n",
    "        ax[2, j].set_xlabel(\"Distance from peak summit (bp)\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_peak_table(peak_bed_paths):\n",
    "    tables = []\n",
    "    for peak_bed_path in peak_bed_paths:\n",
    "        table = pd.read_csv(\n",
    "            peak_bed_path, sep=\"\\t\", header=None,  # Infer compression\n",
    "            names=[\n",
    "                \"chrom\", \"peak_start\", \"peak_end\", \"name\", \"score\",\n",
    "                \"strand\", \"signal\", \"pval\", \"qval\", \"summit_offset\"\n",
    "            ]\n",
    "        )\n",
    "        # Add summit location column\n",
    "        table[\"summit\"] = table[\"peak_start\"] + table[\"summit_offset\"]\n",
    "        tables.append(table)\n",
    "    return pd.concat(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summit_distances(coords, peak_table):\n",
    "    \"\"\"\n",
    "    Given a set of coordinates, computes the distance of the center of each\n",
    "    coordinate to the nearest summit.\n",
    "    Arguments:\n",
    "        `coords`: an N x 3 object array of coordinates\n",
    "        `peak_table`: a 6-column table of peak data, as imported by\n",
    "            `import_peak_table`\n",
    "    Returns and N-array of integers, which is the distance of each coordinate\n",
    "    midpoint to the nearest coordinate.\n",
    "    \"\"\"\n",
    "    chroms = coords[:, 0]\n",
    "    midpoints = (coords[:, 1] + coords[:, 2]) // 2\n",
    "    dists = []\n",
    "    for i in range(len(coords)):\n",
    "        chrom = chroms[i]\n",
    "        midpoint = midpoints[i]\n",
    "        rows = peak_table[peak_table[\"chrom\"] == chrom]\n",
    "        dist_arr = (midpoint - rows[\"summit\"]).values\n",
    "        min_dist = dist_arr[np.argmin(np.abs(dist_arr))]\n",
    "        dists.append(min_dist)\n",
    "    return np.array(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summit_dists(summit_dists):\n",
    "    \"\"\"\n",
    "    Plots the distribution of seqlet distances to summits.\n",
    "    Arguments:\n",
    "        `summit_dists`: the array of distances as returned by\n",
    "            `get_summit_distances`\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    num_bins = max(len(summit_dists) // 30, 20)\n",
    "    plt.hist(summit_dists, bins=num_bins, color=\"purple\")\n",
    "    plt.title(\"Histogram of distance of seqlets to peak summits\")\n",
    "    plt.xlabel(\"Signed distance from seqlet center to nearest peak summit (bp)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_freqs = np.array([0.27, 0.23, 0.23, 0.27])\n",
    "def info_content(track, pseudocount=0.001):\n",
    "    \"\"\"\n",
    "    Given an L x 4 track, computes information content for each base and\n",
    "    returns it as an L-array.\n",
    "    \"\"\"\n",
    "    num_bases = track.shape[1]\n",
    "    # Normalize track to probabilities along base axis\n",
    "    track_norm = (track + pseudocount) / (np.sum(track, axis=1, keepdims=True) + (num_bases * pseudocount))\n",
    "    ic = track_norm * np.log2(track_norm / np.expand_dims(background_freqs, axis=0))\n",
    "    return np.sum(ic, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import SHAP scores, profile predictions, and TF-MoDISco results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SHAP coordinates and one-hot sequences\n",
    "hyp_scores, _, one_hot_seqs, shap_coords = import_shap_scores(shap_scores_path, center_cut_size=shap_score_center_size)\n",
    "# This cuts the sequences/scores off just as how TF-MoDISco saw them, but the coordinates are uncut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the set of all profiles and their coordinates\n",
    "true_profs, pred_profs, all_pred_coords = import_profiles(preds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the set of peaks\n",
    "peak_table = import_peak_table(peak_bed_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the predicted profiles/coordinates to the task-specific SHAP coordinates/scores\n",
    "shap_coords_table = pd.DataFrame(shap_coords, columns=[\"chrom\", \"start\", \"end\"])\n",
    "pred_coords_table = pd.DataFrame(all_pred_coords, columns=[\"chrom\", \"start\", \"end\"])\n",
    "\n",
    "subset_inds = pred_coords_table.reset_index().drop_duplicates([\"chrom\", \"start\", \"end\"]).merge(\n",
    "    shap_coords_table.reset_index(), on=[\"chrom\", \"start\", \"end\"]\n",
    ").sort_values(\"index_y\")[\"index_x\"].values\n",
    "\n",
    "true_profs = true_profs[subset_inds]\n",
    "pred_profs = pred_profs[subset_inds]\n",
    "pred_coords = all_pred_coords[subset_inds]\n",
    "\n",
    "# Make sure the coordinates all match\n",
    "assert np.all(pred_coords == shap_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TF-MoDISco results object\n",
    "tfm_obj = import_tfmodisco_results(tfm_results_path, hyp_scores, one_hot_seqs, shap_score_center_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some SHAP score tracks\n",
    "Plot the central region of some randomly selected actual importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index in np.random.choice(hyp_scores.shape[0], size=5, replace=False):\n",
    "    viz_sequence.plot_weights((hyp_scores[index] * one_hot_seqs[index])[100:300], subticks_frequency=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tfm-results\"></a>\n",
    "### Plot TF-MoDISco results\n",
    "Plot all motifs by metacluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "motifs = []  # Save the motifs as trimmed CWMs\n",
    "num_seqlets = []  # Number of seqlets for each motif\n",
    "motif_seqlets = {}  # Save seqlets of each motif\n",
    "metaclusters = tfm_obj.metacluster_idx_to_submetacluster_results\n",
    "num_metaclusters = len(metaclusters.keys())\n",
    "for metacluster_i, metacluster_key in enumerate(metaclusters.keys()):\n",
    "    metacluster = metaclusters[metacluster_key]\n",
    "    print(\"Metacluster %d/%d\" % (metacluster_i + 1, num_metaclusters))\n",
    "    print(\"==========================================\")\n",
    "    patterns = metacluster.seqlets_to_patterns_result.patterns\n",
    "    if not patterns:\n",
    "        break\n",
    "    motifs.append([])\n",
    "    num_seqlets.append([])\n",
    "    num_patterns = len(patterns)\n",
    "    for pattern_i, pattern in enumerate(patterns):\n",
    "        seqlets = pattern.seqlets\n",
    "        print(\"Pattern %d/%d\" % (pattern_i + 1, num_patterns))\n",
    "        print(\"--------------------------------------\")\n",
    "        \n",
    "        pfm = pattern[\"sequence\"].fwd\n",
    "        hcwm = pattern[\"task0_hypothetical_contribs\"].fwd\n",
    "        cwm = pattern[\"task0_contrib_scores\"].fwd\n",
    "        print(\"%d seqlets\" % len(seqlets))\n",
    "        print(\"Sequence\")\n",
    "        viz_sequence.plot_weights(pfm, subticks_frequency=10)\n",
    "        print(\"Hypothetical contributions\")\n",
    "        viz_sequence.plot_weights(hcwm, subticks_frequency=10)\n",
    "        print(\"Actual contributions (CWM)\")\n",
    "        viz_sequence.plot_weights(cwm, subticks_frequency=10)\n",
    "        \n",
    "        # Trim motif based on IC\n",
    "        pfm_ic = info_content(pfm)\n",
    "        ic_trim_thresh = np.max(pfm_ic) * 0.2  # Cut off anything less than 20% of max IC\n",
    "        pass_inds = np.where(pfm_ic >= ic_trim_thresh)[0]\n",
    "        trimmed_cwm = cwm[np.min(pass_inds): np.max(pass_inds) + 1]\n",
    "        motifs[-1].append(trimmed_cwm)\n",
    "        num_seqlets[-1].append(len(seqlets))\n",
    "        \n",
    "        seqlet_true_profs, seqlet_pred_profs, seqlet_seqs, seqlet_hyps, seqlet_coords = extract_profiles_and_coords(\n",
    "            seqlets, one_hot_seqs, hyp_scores, true_profs, pred_profs, pred_coords,\n",
    "            input_length, profile_length, shap_score_center_size,\n",
    "            profile_display_center_size, task_index=task_index\n",
    "        )\n",
    "        \n",
    "        key_pair = (metacluster_key, pattern_i)\n",
    "        motif_seqlets[key_pair] = (seqlet_seqs, seqlet_hyps)\n",
    "\n",
    "        assert np.allclose(np.sum(seqlet_seqs, axis=0) / len(seqlet_seqs), pattern[\"sequence\"].fwd)\n",
    "        # ^Sanity check: PFM derived from seqlets match the PFM stored in the pattern\n",
    "        print(\"Predicted/observed profiles\")\n",
    "        plot_profiles(\n",
    "            # Flatten to NT x O x 2\n",
    "            np.reshape(seqlet_true_profs, (-1, seqlet_true_profs.shape[2], seqlet_true_profs.shape[3])),\n",
    "            np.reshape(seqlet_pred_profs, (-1, seqlet_pred_profs.shape[2], seqlet_pred_profs.shape[3]))\n",
    "        )\n",
    "        \n",
    "        print(\"Seqlet distance from summits\")\n",
    "        summit_dists = get_summit_distances(seqlet_coords, peak_table)\n",
    "        plot_summit_dists(summit_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"motif-summary\"></a>\n",
    "**Summary of motifs**\n",
    "\n",
    "Motifs are trimmed based on information content, and presented in descending order by number of supporting seqlets. The motifs are separated by metacluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(motifs)):\n",
    "    print(\"Metacluster %d/%d\" % (i + 1, num_metaclusters))\n",
    "    print(\"==========================================\")\n",
    "    for j in np.flip(np.argsort(num_seqlets[i])):\n",
    "        print(\"Number of seqlets: %d\" % num_seqlets[i][j])\n",
    "        viz_sequence.plot_weights(motifs[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seqlets\"></a>\n",
    "**Sample of seqlets supporting each motif**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_seqlets_to_show = 10\n",
    "metaclusters = tfm_obj.metacluster_idx_to_submetacluster_results\n",
    "num_metaclusters = len(metaclusters.keys())\n",
    "for metacluster_i, metacluster_key in enumerate(metaclusters.keys()):\n",
    "    metacluster = metaclusters[metacluster_key]\n",
    "    print(\"Metacluster %d/%d\" % (metacluster_i + 1, num_metaclusters))\n",
    "    print(\"==========================================\")\n",
    "    patterns = metacluster.seqlets_to_patterns_result.patterns\n",
    "    if not patterns:\n",
    "        break\n",
    "    num_patterns = len(patterns)\n",
    "    for pattern_i, pattern in enumerate(patterns):\n",
    "        seqlets = pattern.seqlets\n",
    "        print(\"Pattern %d/%d\" % (pattern_i + 1, num_patterns))\n",
    "        print(\"--------------------------------------\")\n",
    "        print(\"Actual contributions (CWM)\")\n",
    "        viz_sequence.plot_weights(pattern[\"task0_contrib_scores\"].fwd, subticks_frequency=10)\n",
    "        \n",
    "        key_pair = (metacluster_key, pattern_i)\n",
    "        seqlet_seqs, seqlet_hyps = motif_seqlets[key_pair]\n",
    "        \n",
    "        print(\"Sample of seqlets\")\n",
    "        sample_size = min(num_seqlets_to_show, len(seqlet_seqs))\n",
    "        sample_inds = np.random.choice(len(seqlet_seqs), size=sample_size, replace=False)\n",
    "        for i in sample_inds:\n",
    "            viz_sequence.plot_weights(seqlet_seqs[i] * seqlet_hyps[i], subticks_frequency=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
