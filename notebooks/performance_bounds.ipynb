{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"figure.titlesize\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_names = [\"E2F6\", \"FOXA2\", \"SPI1\", \"CEBPB\", \"MAX\", \"NR3C1\", \"GABPA\", \"MAFK\", \"JUND\", \"REST\"]\n",
    "best_folds = {\n",
    "    \"E2F6\": 4,\n",
    "    \"FOXA2\": 7,\n",
    "    \"SPI1\": 7,\n",
    "    \"CEBPB\": 7,\n",
    "    \"MAX\": 1,\n",
    "    \"NR3C1\": 6,\n",
    "    \"GABPA\": 7,\n",
    "    \"MAFK\": 8,\n",
    "    \"JUND\": 7,\n",
    "    \"REST\": 7\n",
    "}\n",
    "\n",
    "predictions_paths = {\n",
    "    tf_name: {\n",
    "        fold: \"/users/amtseng/tfmodisco/results/peak_predictions/{0}/{0}_peak_prediction_performance_fold{1}.h5\".format(tf_name, fold)\n",
    "        for fold in range(1, 11)\n",
    "    } for tf_name in tf_names\n",
    "}\n",
    "performance_bounds_paths = {\n",
    "    tf_name: \"/users/amtseng/tfmodisco/results/performance_bounds/{0}/{0}_performance_bounds.h5\".format(tf_name)\n",
    "    for tf_name in tf_names\n",
    "}\n",
    "\n",
    "chrom_splits_json = \"/users/amtseng/tfmodisco/data/processed/ENCODE/chrom_splits.json\"\n",
    "with open(chrom_splits_json, \"r\") as f:\n",
    "    chrom_splits = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 1346\n",
    "profile_length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the set of performance metrics\n",
    "For each peak, get the actual performance metrics, the lower bounds, and the upper bounds. The coordinates in the files should match exactly\\*\n",
    "\n",
    "\\*The saved coordinates in the predictions/actual performances are padded to the input length, but the coordinates in the performance bounds are padded to the profile length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tf_metrics(tf_name):\n",
    "    \"\"\"\n",
    "    Imports the set of all metrics for the given TF. Returns a dictionary of the\n",
    "    following format:\n",
    "        `actual_perf`:\n",
    "            `fold_num`:\n",
    "                `nll`: ...\n",
    "                `norm_nll`: ...\n",
    "                ...\n",
    "            `fold_num`:\n",
    "                ...\n",
    "        `lower_perf`:\n",
    "            ...\n",
    "        `upper_perf`:\n",
    "            ...\n",
    "    \"\"\"\n",
    "    perfs = {}\n",
    "    \n",
    "    performance_bounds_path = performance_bounds_paths[tf_name]\n",
    "    perf_bound_reader = h5py.File(performance_bounds_path, \"r\")\n",
    "    \n",
    "    perf_bound_reader_coords = perf_bound_reader[\"coords\"]\n",
    "    lower_perf = perf_bound_reader[\"performance_lower\"]\n",
    "    upper_perf = perf_bound_reader[\"performance_upper\"]\n",
    "    lower_perf_dict = {key : lower_perf[key][:] for key in lower_perf.keys()}\n",
    "    upper_perf_dict = {key : upper_perf[key][:] for key in upper_perf.keys()}\n",
    "    perfs[\"lower_perf\"] = lower_perf_dict\n",
    "    perfs[\"upper_perf\"] = upper_perf_dict\n",
    "    perfs[\"actual_perf\"] = {}\n",
    "\n",
    "    for fold in tqdm.notebook.trange(1, 11):\n",
    "        predictions_path = predictions_paths[tf_name][fold]\n",
    "        pred_reader = h5py.File(predictions_path, \"r\")\n",
    "\n",
    "        # Check that the coordinates match up exactly\n",
    "        pred_reader_coords = pred_reader[\"coords\"]\n",
    "        assert np.all(\n",
    "            pred_reader_coords[\"coords_end\"][:] - pred_reader_coords[\"coords_start\"][:] == input_length\n",
    "        )\n",
    "        assert np.all(\n",
    "            perf_bound_reader_coords[\"coords_end\"][:] - perf_bound_reader_coords[\"coords_start\"][:] ==\n",
    "            profile_length\n",
    "        )\n",
    "        assert np.all(\n",
    "            perf_bound_reader_coords[\"coords_start\"][:] - ((input_length - profile_length) // 2) ==\n",
    "            pred_reader_coords[\"coords_start\"][:]\n",
    "        )\n",
    "        assert np.all(\n",
    "            perf_bound_reader_coords[\"coords_end\"][:] + ((input_length - profile_length) // 2) ==\n",
    "            pred_reader_coords[\"coords_end\"][:]\n",
    "        )\n",
    "        assert np.all(\n",
    "            perf_bound_reader_coords[\"coords_chrom\"][:] == pred_reader_coords[\"coords_chrom\"][:]\n",
    "        )\n",
    "\n",
    "        actual_perf = pred_reader[\"performance\"]\n",
    "        actual_perf_dict = {key : actual_perf[key][:] for key in actual_perf.keys()}\n",
    "\n",
    "        # Compute the normalized NLL for the actual performance \n",
    "        actual_perf_dict[\"norm_nll\"] = actual_perf_dict[\"nll\"][:] / \\\n",
    "            np.mean(np.sum(pred_reader[\"predictions\"][\"true_profs\"][:], axis=2), axis=2)\n",
    "\n",
    "        perfs[\"actual_perf\"][fold] = actual_perf_dict\n",
    "        pred_reader.close()\n",
    "        \n",
    "    perf_bound_reader.close()\n",
    "    \n",
    "    return perfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the performance bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perf_bounds(tf_perfs, tf_name):\n",
    "    def create_violins(ax, lower_perfs, actual_perfs_list, upper_perfs):\n",
    "        num_actual_perfs = len(actual_perfs_list)\n",
    "        all_data = np.stack([lower_perfs] + actual_perfs_list + [upper_perfs], axis=0)\n",
    "        q1, med, q3 = np.nanpercentile(all_data, [25, 50, 70], axis=1)\n",
    "        iqr = q3 - q1\n",
    "        lower_outlier = q1 - (1.5 * iqr)\n",
    "        upper_outlier = q3 + (1.5 * iqr)\n",
    "        \n",
    "        sorted_clipped_data = [  # Remove outliers\n",
    "            np.sort(vec[(vec >= lower_outlier[i]) & (vec <= upper_outlier[i])])\n",
    "            for i, vec in enumerate(all_data)\n",
    "        ]\n",
    "        \n",
    "        plot_parts = ax.violinplot(\n",
    "            sorted_clipped_data, showmeans=False, showmedians=False, showextrema=False\n",
    "        )\n",
    "        violin_parts = plot_parts[\"bodies\"]\n",
    "        violin_parts[0].set_facecolor(\"coral\")\n",
    "        violin_parts[0].set_edgecolor(\"coral\")\n",
    "        violin_parts[0].set_alpha(0.7)\n",
    "        for i in range(1, num_actual_perfs + 1):\n",
    "            violin_parts[i].set_facecolor(\"mediumorchid\")\n",
    "            violin_parts[i].set_edgecolor(\"mediumorchid\")\n",
    "            violin_parts[i].set_alpha(0.7)\n",
    "        violin_parts[-1].set_facecolor(\"slateblue\")\n",
    "        violin_parts[-1].set_edgecolor(\"slateblue\")\n",
    "        violin_parts[-1].set_alpha(0.7)\n",
    "        \n",
    "        inds = np.arange(1, num_actual_perfs + 2 + 1)\n",
    "        ax.vlines(inds, q1, q3, color=\"black\", linewidth=5, zorder=1)\n",
    "        ax.scatter(inds, med, marker=\"o\", color=\"white\", s=30, zorder=2)\n",
    "    \n",
    "    # Profile metrics\n",
    "    for metric_key, metric_name in [\n",
    "        (\"nll\", \"NLL\"), (\"norm_nll\", \"Normalized NLL\"), (\"jsd\", \"JSD\"), (\"profile_mse\", \"Profile MSE\"),\n",
    "        (\"profile_pearson\", \"Profile Pearson\"), (\"profile_spearman\", \"Profile Spearman\")\n",
    "    ]:\n",
    "        lower_perfs = np.nanmean(tf_perfs[\"lower_perf\"][metric_key], axis=1)\n",
    "        upper_perfs = np.nanmean(tf_perfs[\"upper_perf\"][metric_key], axis=1)\n",
    "        actual_perfs_list = [\n",
    "            np.nanmean(tf_perfs[\"actual_perf\"][fold][metric_key], axis=1)\n",
    "            for fold in range(1, 11)\n",
    "        ]\n",
    "        fig, ax = plt.subplots(figsize=(20, 5))\n",
    "        create_violins(ax, lower_perfs, actual_perfs_list, upper_perfs)\n",
    "        ax.set_title(\"%s of %s predictions across folds\\nGenome-wide performance\" % (metric_name, tf_name))\n",
    "        ax.set_xticks(np.arange(1, 13))\n",
    "        ax.set_xticklabels(\n",
    "            [\"Randomized\"] + [\"Fold %d\" % (fold + 1) for fold in range(10)] + [\"Pseudoreps\"]\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "    # Count metrics\n",
    "    for metric_key, metric_name in [\n",
    "        (\"count_mse\", \"Count MSE\"), (\"count_pearson\", \"Count Pearson\"), (\"count_spearman\", \"Count Spearman\")\n",
    "    ]:\n",
    "        lower_bound = np.nanmean(tf_perfs[\"lower_perf\"][metric_key])\n",
    "        upper_bound = np.nanmean(tf_perfs[\"upper_perf\"][metric_key])\n",
    "        actual_perfs_list = [\n",
    "            np.nanmean(tf_perfs[\"actual_perf\"][fold][metric_key]) for fold in range(1, 11)\n",
    "        ]\n",
    "        fig, ax = plt.subplots(figsize=(20, 5))\n",
    "        \n",
    "        label_locs = np.arange(12)  # Location of labels\n",
    "        \n",
    "        ax.bar(\n",
    "            label_locs, [lower_bound] + actual_perfs_list + [upper_bound],\n",
    "            color=([\"coral\"] + (10 * [\"mediumorchid\"]) + [\"slateblue\"]), alpha=0.7\n",
    "        )\n",
    "        ax.set_title(\"%s of %s predictions across folds\\nGenome-wide performance\" % (metric_name, tf_name))\n",
    "        ax.set_xticks(label_locs)\n",
    "        ax.set_xticklabels(\n",
    "            [\"Randomized\"] + [\"Fold %d\" % (fold + 1) for fold in range(10)] + [\"Pseudoreps\"]\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tf_name in tf_names:\n",
    "    print(tf_name)\n",
    "    perfs = import_tf_metrics(tf_name)\n",
    "    plot_perf_bounds(perfs, tf_name)\n",
    "    del perfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
