{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct links to results\n",
    "[Multi-task profile model performance across all 10 folds](#multitask-fold)\n",
    "\n",
    "[Single-task profile model performance across all 10 folds](#singletask-fold)\n",
    "\n",
    "[Fine-tuned multi-task profile model task-specific performance](#finetune-multitask-task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import vdom.helpers as vdomh\n",
    "from IPython.display import display\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"figure.titlesize\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"legend.fontsize\": 15,\n",
    "    \"font.size\": 13,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters/fetch arguments\n",
    "tf_name = os.environ[\"TFM_RESULTS_TF_NAME\"]\n",
    "num_tasks = int(os.environ[\"TFM_RESULTS_NUM_TASKS\"])\n",
    "best_multitask_fold = int(os.environ[\"TFM_RESULTS_BEST_MULTITASK_FOLD\"])\n",
    "best_singletask_folds = [int(x) for x in os.environ[\"TFM_RESULTS_BEST_SINGLETASK_FOLDS\"].split(\",\")]\n",
    "\n",
    "if \"TFM_PERF_CACHE\" in os.environ:\n",
    "    perf_cache_dir = os.environ[\"TFM_PERF_CACHE\"]\n",
    "else:\n",
    "    perf_cache_dir = None\n",
    "\n",
    "print(\"TF name: %s\" % tf_name)\n",
    "print(\"Number of tasks: %d\" % num_tasks)\n",
    "print(\"Best multi-task fold: %d\" % best_multitask_fold)\n",
    "print(\"Best single-task folds: %s\" % \" \".join([str(x) for x in best_singletask_folds]))\n",
    "print(\"Saved performance cache: %s\" % perf_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_base = \"/users/amtseng/tfmodisco/results/peak_predictions/\"\n",
    "\n",
    "# Paths for all 10 folds for multi-task profile models\n",
    "multitask_preds_paths = [\n",
    "    os.path.join(\n",
    "        preds_base,\n",
    "        \"multitask_profile\",\n",
    "        \"{0}_multitask_profile_fold{1}/{0}_multitask_profile_fold{1}_pred_perf.h5\".format(tf_name, fold)\n",
    "    ) for fold in range(1, 11)\n",
    "]\n",
    "\n",
    "# Paths for all 10 folds for single-task profile models, for each task\n",
    "singletask_preds_paths = [\n",
    "    [\n",
    "        os.path.join(\n",
    "            preds_base,\n",
    "            \"singletask_profile\",\n",
    "            \"{0}_singletask_profile_fold{1}/task_{2}/{0}_singletask_profile_task{2}_fold{1}_pred_perf.h5\".format(\n",
    "                tf_name, fold, task_index\n",
    "            )\n",
    "        ) for fold in range(1, 11)\n",
    "    ] for task_index in range(num_tasks)\n",
    "]\n",
    "\n",
    "# Path for fine-tuned multi-task profile model\n",
    "multitask_finetuned_preds_path = os.path.join(\n",
    "    preds_base,\n",
    "    \"multitask_profile_finetune\",\n",
    "    \"{0}_multitask_profile_finetune_fold{1}/{0}_multitask_profile_finetune_fold{1}_pred_perf.h5\".format(\n",
    "        tf_name, best_multitask_fold\n",
    "    )\n",
    ")\n",
    "\n",
    "# Paths for fine-tuned single-task profile models, for each task\n",
    "singletask_finetuned_preds_paths = [\n",
    "    os.path.join(\n",
    "        preds_base,\n",
    "        \"singletask_profile_finetune\",\n",
    "        \"{0}_singletask_profile_finetune_fold{1}/task_{2}/{0}_singletask_profile_finetune_task{2}_fold{1}_pred_perf.h5\".format(\n",
    "            tf_name, fold, task_index\n",
    "        )\n",
    "    ) for task_index, fold in enumerate(best_singletask_folds)\n",
    "]\n",
    "\n",
    "# Path for upper-bound and lower-bound performance metrics\n",
    "perf_bounds_path = \"/users/amtseng/tfmodisco/results/performance_bounds/{0}_performance_bounds.h5\".format(tf_name)\n",
    "\n",
    "# File specifications (including peak files)\n",
    "files_spec_json = \"/users/amtseng/tfmodisco/data/processed/ENCODE/config/{0}/{0}_training_paths.json\".format(tf_name)\n",
    "with open(files_spec_json, \"r\") as f:\n",
    "    files_spec = json.load(f)\n",
    "\n",
    "# Chromosome split definition (i.e. test set chromosomes)\n",
    "chrom_splits_json = \"/users/amtseng/tfmodisco/data/processed/ENCODE/chrom_splits.json\"\n",
    "with open(chrom_splits_json, \"r\") as f:\n",
    "    chrom_splits = json.load(f)\n",
    "all_fold_test_chroms = [\n",
    "    chrom_splits[str(fold)][\"test\"] for fold in range(1, 11)\n",
    "]\n",
    "best_multitask_fold_test_chroms = chrom_splits[str(best_multitask_fold)][\"test\"]\n",
    "best_singletask_fold_test_chroms = [chrom_splits[str(fold)][\"test\"] for fold in best_singletask_folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 2114\n",
    "profile_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perf_cache_dir:\n",
    "    os.makedirs(perf_cache_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "For subsetting predictions/performance metrics to peak subsets, extracting performance metrics, and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_coord_inds(superset_coords, subset_coords):\n",
    "    \"\"\"\n",
    "    Both `superset_coords` and `subset_coords` are Pandas DataFrames of\n",
    "    coordinates. This will return the indices within `superset_coords`\n",
    "    (indices being the 0-indexed row numbers) that correspond to the\n",
    "    coordinates in `subset_coords`. Returns a sorted NumPy array of indices.\n",
    "    Note that if there are duplicates in either set of coordinates, they\n",
    "    will be dropped (i.e. the returned indices will be unique).\n",
    "    \"\"\"\n",
    "    inds = superset_coords.reset_index().drop_duplicates([\"chrom\", \"start\", \"end\"]).merge(\n",
    "        subset_coords.reset_index(), on=[\"chrom\", \"start\", \"end\"]\n",
    "    ).sort_values(\"index_y\")[\"index_x\"].values\n",
    "    return np.unique(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_coords_by_chroms(coords, chroms):\n",
    "    \"\"\"\n",
    "    Given a Pandas DataFrame of coordinates (column names \"chrom\", \"start\",\n",
    "    and \"end\"), filters for the chromosomes in `chroms`\n",
    "    \"\"\"\n",
    "    return coords[coords[\"chrom\"].isin(chroms)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_performance_metrics(pred_perf_path, coord_sets=None, task_inds=None):\n",
    "    \"\"\"\n",
    "    Extracts the set of performance metrics from a saved predictions/performance\n",
    "    HDF5 file. If specified, filters for coordinates that are in `coord_sets`.\n",
    "    `coord_sets` is a list of coordinate DataFrames, and a set of metrics will\n",
    "    be fetched for each table of coordinates provided. Otherwise, will simply\n",
    "    return all coordinates available (i.e. only one coordinate set with all\n",
    "    coordinates). If `task_inds` is specified, it must be a list of indices\n",
    "    parallel to `coord_sets`. For each coordinate set, the metrics extracted\n",
    "    will be for that task index only. If unspecified, the average over all tasks\n",
    "    is retained for each coordinate set.\n",
    "    Returns a dictionary of the following form:\n",
    "        `nll`: [\n",
    "            <NLL vector for coord set 1>\n",
    "            <NLL vector for coord set 2>\n",
    "            ...\n",
    "        ],\n",
    "        `count_mse`: [\n",
    "            <MSE scalar for coord set 1>\n",
    "            <MSE scalar for coord set 2>\n",
    "            ...\n",
    "        ]\n",
    "        ...\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    reader = h5py.File(pred_perf_path, \"r\")\n",
    "    coord_reader = reader[\"coords\"]\n",
    "    pred_reader = reader[\"predictions\"]\n",
    "    perf_reader = reader[\"performance\"]\n",
    "    \n",
    "    # First, get the set of indices within the HDF5 predictions/performance that\n",
    "    # correspond to the given coordinate sets\n",
    "    if coord_sets is None:\n",
    "        subset_inds = [np.arange(perf_reader[\"nll\"].shape[0])]  # The entire vector\n",
    "    else:\n",
    "        # Import the DataFrame of coordinates in this HDF5\n",
    "        pred_perf_coords = pd.DataFrame(\n",
    "            data={\n",
    "                \"chrom\": coord_reader[\"coords_chrom\"][:].astype(str),\n",
    "                \"start\": coord_reader[\"coords_start\"][:],\n",
    "                \"end\": coord_reader[\"coords_end\"][:]\n",
    "            }\n",
    "        )\n",
    "        subset_inds = [\n",
    "            subset_coord_inds(pred_perf_coords, coord_set)\n",
    "            for coord_set in coord_sets\n",
    "        ]\n",
    "    \n",
    "    # If we didn't specify a task index for each coordinate set, just use\n",
    "    # all tasks; either way, let's get each set of task indices into a\n",
    "    # NumPy array form\n",
    "    if task_inds is None:\n",
    "        task_inds = [np.arange(perf_reader[\"nll\"].shape[1]) for _ in range(len(subset_inds))]\n",
    "    else:\n",
    "        task_inds = [np.array([i]) for i in task_inds]\n",
    "        \n",
    "    # For each performance metric, for each coordinate set/task index, extract\n",
    "    # the metrics values\n",
    "    for key in perf_reader.keys():\n",
    "        metrics_list = []\n",
    "        for i in range(len(subset_inds)):\n",
    "            subset = subset_inds[i]\n",
    "            tasks = task_inds[i]\n",
    "            \n",
    "            if len(perf_reader[key].shape) >= 2:  # Profile metric\n",
    "                metrics_list.append(\n",
    "                    np.mean(perf_reader[key][subset][:, tasks], axis=1)\n",
    "                )\n",
    "            else:  # Count metric\n",
    "                # If the coordinate set is limited, then we'll need to recompute\n",
    "                # the count metrics (i.e. MSE and correlations), since these were\n",
    "                # saved for the entire set\n",
    "                if coord_sets is None:\n",
    "                    # No need to recompute\n",
    "                    metrics_list.append(\n",
    "                        np.mean(perf_reader[key][tasks])\n",
    "                    )\n",
    "                else:\n",
    "                    log_true_counts = np.ravel(np.log(pred_reader[\"true_counts\"][subset][:, tasks] + 1))\n",
    "                    log_pred_counts = np.ravel(pred_reader[\"log_pred_counts\"][subset][:, tasks])\n",
    "                    if key == \"count_mse\":\n",
    "                        metrics_list.append(\n",
    "                            np.mean(np.square(log_true_counts - log_pred_counts))\n",
    "                        )\n",
    "                    elif key == \"count_pearson\":\n",
    "                        metrics_list.append(\n",
    "                            scipy.stats.pearsonr(log_true_counts, log_pred_counts)[0]\n",
    "                        )\n",
    "                    elif key == \"count_spearman\":\n",
    "                        metrics_list.append(\n",
    "                            scipy.stats.spearmanr(log_true_counts, log_pred_counts)[0]\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(\"Unknown count metric key: %s\" % key)\n",
    "\n",
    "        result[key] = metrics_list\n",
    "    \n",
    "    reader.close()\n",
    "    \n",
    "    # Convert NaNs to 0\n",
    "    for key in result:\n",
    "        result[key] = [np.nan_to_num(a) for a in result[key]]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_performance_bounds(perf_bounds_path, input_length, coord_sets=None, task_inds=None):\n",
    "    \"\"\"\n",
    "    Extracts the set of lower and upper bound performance metrics from a saved\n",
    "    HDF5 file. `input_length` is the lenght of input sequence to use.\n",
    "    If specified, filters for coordinates that are in `coord_sets`.\n",
    "    `coord_sets` is a list of coordinate DataFrames, and a set of metrics will\n",
    "    be fetched for each table of coordinates provided. Otherwise, will simply\n",
    "    return all coordinates available (i.e. only one coordinate set with all\n",
    "    coordinates). If `task_inds` is specified, it must be a list of indices\n",
    "    parallel to `coord_sets`. For each coordinate set, the metrics extracted\n",
    "    will be for that task index only. If unspecified, the average over all tasks\n",
    "    is retained for each coordinate set.\n",
    "    Returns a dictionary of the following form:\n",
    "        `nll`: [\n",
    "            (\n",
    "                <lower-bound NLL vector for coord set 1>,\n",
    "                <upper-bound NLL vector for coord set 1>\n",
    "            ),\n",
    "            (\n",
    "                <lower-bound NLL vector for coord set 2>,\n",
    "                <upper-bound NLL vector for coord set 2>\n",
    "            ),\n",
    "            \n",
    "            ...\n",
    "        ],\n",
    "        `count_mse`: [\n",
    "            (\n",
    "                <lower-bound MSE scalar for coord set 1>,\n",
    "                <upper-bound MSE scalar for coord set 1>\n",
    "            ),\n",
    "            (\n",
    "                <lower-bound MSE scalar for coord set 2>,\n",
    "                <upper-bound MSE scalar for coord set 2>\n",
    "            ),\n",
    "            ...\n",
    "        ]\n",
    "        ...\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    reader = h5py.File(perf_bounds_path, \"r\")\n",
    "    coord_reader = reader[\"coords\"]\n",
    "    lower_perf_reader = reader[\"performance_lower\"]\n",
    "    upper_perf_reader = reader[\"performance_upper\"]\n",
    "    \n",
    "    # First, get the set of indices within the HDF5 predictions/performance that\n",
    "    # correspond to the given coordinate sets\n",
    "    if coord_sets is None:\n",
    "        subset_inds = [np.arange(lower_perf_reader[\"nll\"].shape[0])]  # The entire vector\n",
    "    else:\n",
    "        # Import the DataFrame of coordinates in this HDF5\n",
    "        perf_coords = pd.DataFrame(\n",
    "            data={\n",
    "                \"chrom\": coord_reader[\"coords_chrom\"][:].astype(str),\n",
    "                \"start\": coord_reader[\"coords_start\"][:],\n",
    "                \"end\": coord_reader[\"coords_end\"][:]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Unlike the predictions, the performance bounds are computed solely\n",
    "        # based on profiles, so their saved coordinates have a different\n",
    "        # length, although they are centered at the same summit. So we need\n",
    "        # to re-pad them.\n",
    "        perf_coords[\"midpoint\"] = (perf_coords[\"start\"] + perf_coords[\"end\"]) // 2\n",
    "        perf_coords[\"start\"] = perf_coords[\"midpoint\"] - (input_length // 2)\n",
    "        perf_coords[\"end\"] = perf_coords[\"start\"] + input_length\n",
    "        del perf_coords[\"midpoint\"]\n",
    "        \n",
    "        subset_inds = [\n",
    "            subset_coord_inds(perf_coords, coord_set)\n",
    "            for coord_set in coord_sets\n",
    "        ]\n",
    "    \n",
    "    # If we didn't specify a task index for each coordinate set, just use\n",
    "    # all tasks; either way, let's get each set of task indices into a\n",
    "    # NumPy array form\n",
    "    if task_inds is None:\n",
    "        task_inds = [np.arange(lower_perf_reader[\"nll\"].shape[1]) for _ in range(len(subset_inds))]\n",
    "    else:\n",
    "        task_inds = [np.array([i]) for i in task_inds]\n",
    "        \n",
    "    # For each performance metric, for each coordinate set/task index, extract\n",
    "    # the metrics values for lower and upper bound\n",
    "    for key in lower_perf_reader.keys():\n",
    "        metrics_list = []\n",
    "        for i in range(len(subset_inds)):\n",
    "            subset = subset_inds[i]\n",
    "            tasks = task_inds[i]\n",
    "            \n",
    "            if len(lower_perf_reader[key].shape) >= 2:  # Profile metric\n",
    "                metrics_list.append(\n",
    "                    (\n",
    "                        np.mean(lower_perf_reader[key][subset][:, tasks], axis=1),\n",
    "                        np.mean(upper_perf_reader[key][subset][:, tasks], axis=1)\n",
    "                    )\n",
    "                )\n",
    "            else:  # Count metric\n",
    "                # For the performance bounds, we'll use counts metrics as-is without\n",
    "                # recomputing them; this is because the counts metrics are distributed\n",
    "                # very uniformly\n",
    "                metrics_list.append(\n",
    "                    (\n",
    "                        np.mean(lower_perf_reader[key][tasks]),\n",
    "                        np.mean(upper_perf_reader[key][tasks]),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        result[key] = metrics_list\n",
    "\n",
    "    reader.close()\n",
    "    \n",
    "    # Convert NaNs to 0\n",
    "    for key in result:\n",
    "        result[key] = [(np.nan_to_num(p1), np.nan_to_num(p2)) for p1, p2 in result[key]]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_counts(pred_perf_path, task_inds=None):\n",
    "    \"\"\"\n",
    "    Extracts the set of all true and predicted log counts from a saved\n",
    "    predictions/performance HDF5 file. Returns N x T x 2 arrays for the\n",
    "    true and predicted log counts, respectively. `N` is all peaks in the\n",
    "    file. If specified, limit tasks to those in `task_inds`.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    reader = h5py.File(pred_perf_path, \"r\")\n",
    "    pred_reader = reader[\"predictions\"]\n",
    "    \n",
    "    if task_inds is None:\n",
    "        task_inds = np.arange(pred_reader[\"true_counts\"].shape[1])\n",
    "    else:\n",
    "        task_inds = np.array(task_inds)\n",
    "    \n",
    "    log_true_counts = np.log(pred_reader[\"true_counts\"][:, task_inds] + 1)\n",
    "    log_pred_counts = pred_reader[\"log_pred_counts\"][:, task_inds]\n",
    "    \n",
    "    reader.close()\n",
    "    return log_true_counts, log_pred_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(lower_arr, query_arr, upper_arr):\n",
    "    \"\"\"\n",
    "    Given three same-size arrays, returns a version of the query array\n",
    "    in which the array is min-max normalized to the lower and upper\n",
    "    bounds (i.e. if the lower bound is 0 and the upper bound is 1).\n",
    "    If the upper bound is not higher than the lower bound, then the\n",
    "    bounds will be flipped. This is done independently for each entry.\n",
    "    \"\"\"\n",
    "    normed = (query_arr - lower_arr) / (upper_arr - lower_arr)\n",
    "    # Clamp between 0 and 1\n",
    "    return np.clip(normed, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cdf(ax, data, steps=1000, density=False, inverse=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Plots a CDF to the given axes. `steps` is the number of steps in the\n",
    "    CDF. If `inverse` is True, plots an inverse CDF (AKA survivorship plot).\n",
    "    `density` is whether or not to normalize to fractions.\n",
    "    \"\"\"\n",
    "    hist, bin_edges = np.histogram(data, bins=steps)\n",
    "    if inverse:\n",
    "        cumsum = len(data) - np.cumsum(hist)\n",
    "    else:\n",
    "        cumsum = np.cumsum(hist)\n",
    "    if density:\n",
    "        cumsum = cumsum / len(data)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.\n",
    "    ax.step(bin_centers, cumsum, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performances(perf_dict, title_header, cond_labels=None):\n",
    "    \"\"\"\n",
    "    Creates plots for a performance dictionary of the following form:\n",
    "        `nll`: [\n",
    "            <NLL vector for cond 1>\n",
    "            <NLL vector for cond 2>\n",
    "            ...\n",
    "        ],\n",
    "        `count_mse`: [\n",
    "            <MSE scalar for cond 1>\n",
    "            <MSE scalar for cond 2>\n",
    "            ...\n",
    "        ]\n",
    "        ...\n",
    "    For profile metrics (i.e. where the metrics are a vector), creates CDF\n",
    "    plots. For count metrics (i.e. where the metrics are a scalar), creates bar\n",
    "    plots.\n",
    "    `cond_labels` must be arrays parallel to the set of vectors or scalars for\n",
    "    each metric.\n",
    "    Returns a dictionray mapping metric keys to figures.\n",
    "    \"\"\"\n",
    "    figs = {}\n",
    "    labels = [None] * len(perf_dict[metric_key]) if not cond_labels else cond_labels\n",
    "    # Profile metrics\n",
    "    for metric_key, metric_name in [\n",
    "        (\"nll\", \"NLL\"), (\"cross_ent\", \"Cross Entropy\"), (\"jsd\", \"JSD\"), (\"profile_mse\", \"Profile MSE\"),\n",
    "        (\"profile_pearson\", \"Profile Pearson\"), (\"profile_spearman\", \"Profile Spearman\")\n",
    "    ]:\n",
    "        fig, ax = plt.subplots(figsize=(20, 8))\n",
    "        if metric_key.endswith(\"pearson\") or metric_key.endswith(\"spearman\"):\n",
    "            inverse = True\n",
    "            title = \"%s: inverse CDF of %s\" % (title_header, metric_name)\n",
    "        else:\n",
    "            inverse = False\n",
    "            title = \"%s: CDF of %s\" % (title_header, metric_name)\n",
    "        for i, arr in enumerate(perf_dict[metric_key]):\n",
    "            # Remove any NaNs\n",
    "            arr = arr[np.isfinite(arr)]\n",
    "            # If the data is out of the range [0, 1] (e.g. with correlations), clip them\n",
    "            arr = np.clip(arr, 0, 1)\n",
    "            make_cdf(ax, arr, steps=1000, density=True, inverse=inverse, label=labels[i])\n",
    "        if cond_labels:\n",
    "            plt.legend()\n",
    "        ax.set_title(title)\n",
    "        plt.show()\n",
    "        figs[metric_key] = fig\n",
    "\n",
    "    # Count metrics\n",
    "    for metric_key, metric_name in [\n",
    "        (\"count_mse\", \"Count MSE\"), (\"count_pearson\", \"Count Pearson\"), (\"count_spearman\", \"Count Spearman\")\n",
    "    ]:\n",
    "        fig, ax = plt.subplots(figsize=(20, 5))\n",
    "        label_locs = np.arange(len(perf_dict[metric_key]))  # Location of labels\n",
    "        bars = ax.bar(\n",
    "            label_locs, perf_dict[metric_key], color=\"mediumorchid\", alpha=0.7\n",
    "        )\n",
    "        ax.set_title(\"%s: %s\" % (title_header, metric_name))\n",
    "        if cond_labels:\n",
    "            ax.set_xticks(label_locs)\n",
    "            ax.set_xticklabels(labels)\n",
    "            \n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(\n",
    "                \"%.3f\" % height, xy=(bar.get_x() + (bar.get_width() / 2), height),\n",
    "                xytext=(0, 1), textcoords=\"offset points\", ha=\"center\", va=\"bottom\"\n",
    "            )\n",
    "        plt.show()\n",
    "        figs[metric_key] = fig\n",
    "    return figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_scatterplots(counts, cond_labels=None):\n",
    "    \"\"\"\n",
    "    Creates a row of scatterplots for the counts of the following form:\n",
    "        [\n",
    "            (\n",
    "                <array of true log counts>\n",
    "                <array of predicted log counts>\n",
    "            ),\n",
    "            ...\n",
    "        ]\n",
    "    `cond_labels` must be arrays parallel to the set of pairs of counts.\n",
    "    Returns the figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(ncols=len(counts), figsize=(8 * len(counts), 8))\n",
    "    if len(counts) == 1:\n",
    "        ax = [ax]\n",
    "    for i, (true_log_counts, pred_log_counts) in enumerate(counts):\n",
    "        ax[i].scatter(np.ravel(true_log_counts), np.ravel(pred_log_counts), alpha=0.02)\n",
    "        ax[i].set_xlabel(\"True log counts\")\n",
    "        ax[i].set_ylabel(\"Predicted log counts\")\n",
    "        (min_x, max_x), (min_y, max_y) = ax[i].get_xlim(), ax[i].get_ylim()\n",
    "        min_both, max_both = min(min_x, min_y), max(max_x, max_y)\n",
    "        ax[i].set_xlim(min_both, max_both)\n",
    "        ax[i].set_ylim(min_both, max_both)\n",
    "        ax[i].plot(\n",
    "            [min_both, max_both], [min_both, max_both],\n",
    "            color=\"black\", linestyle=\"--\", alpha=0.3, zorder=0\n",
    "        )\n",
    "        if cond_labels:\n",
    "            ax[i].set_title(cond_labels[i])\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_perf_dict(perf_dict, path):\n",
    "    \"\"\"\n",
    "    Saves the performance dictionary mapping keys to lists of arrays\n",
    "    or lists of scalars.\n",
    "    \"\"\"\n",
    "    with h5py.File(path, \"w\") as f:\n",
    "        for key in perf_dict:\n",
    "            group = f.create_group(key)\n",
    "            for i, data in enumerate(perf_dict[key]):\n",
    "                if type(data) is np.ndarray:\n",
    "                    group.create_dataset(str(i), data=data, compression=\"gzip\")\n",
    "                else:\n",
    "                    group.create_dataset(str(i), data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_perf_dict(path):\n",
    "    \"\"\"\n",
    "    Saves the performance dictionary mapping keys to \n",
    "    \"\"\"\n",
    "    perf_dict = {}\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        for key in f:\n",
    "            perf_dict[key] = []\n",
    "            for i in range(len(f[key].keys())):\n",
    "                if not f[key][str(i)].shape:\n",
    "                    perf_dict[key].append(f[key][str(i)][()])\n",
    "                else:\n",
    "                    perf_dict[key].append(f[key][str(i)][:])\n",
    "    return perf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import peak coordinates for each task\n",
    "For each task, import the set of peaks belonging to that task. This allows us to get a set of indices for coordinates in the saved predictions/performance files that correspond to each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in tables containing the peak coordinates, padded to `input_length`\n",
    "task_coords = []\n",
    "assert len(files_spec[\"peak_beds\"]) == num_tasks\n",
    "for peak_bed_path in files_spec[\"peak_beds\"]:\n",
    "    table = pd.read_csv(\n",
    "        peak_bed_path, sep=\"\\t\", header=None,  # Infer compression\n",
    "        names=[\n",
    "            \"chrom\", \"peak_start\", \"peak_end\", \"name\", \"score\",\n",
    "            \"strand\", \"signal\", \"pval\", \"qval\", \"summit_offset\"\n",
    "        ]\n",
    "    )\n",
    "    # Add summit location column:\n",
    "    table[\"summit\"] = table[\"peak_start\"] + table[\"summit_offset\"]\n",
    "    \n",
    "    # Add start and end columns, at proper length\n",
    "    table[\"start\"] = table[\"summit\"] - (input_length // 2)\n",
    "    table[\"end\"] = table[\"start\"] + input_length\n",
    "    \n",
    "    task_coords.append(table[[\"chrom\", \"start\", \"end\"]])\n",
    "    \n",
    "all_coords = pd.concat(task_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"multitask-fold\"></a>\n",
    "### Multi-task profile model performance across all 10 folds\n",
    "A comparison of the test-set performance (averaged across tasks) between:\n",
    "1. Multi-task profile models trained across all 10 folds\n",
    "2. A fine-tuned multi-task profile model on the best-performing fold\n",
    "3. Upper and lower bounds on the best-performing fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_dict = True\n",
    "if perf_cache_dir:\n",
    "    # Import if it exists\n",
    "    perf_dict_path = os.path.join(perf_cache_dir, \"multitask_allfolds_finetuned.h5\")\n",
    "    if os.path.exists(perf_dict_path) and os.stat(perf_dict_path).st_size:\n",
    "        multitask_perf_dict = load_perf_dict(perf_dict_path)\n",
    "        compute_dict = False\n",
    "\n",
    "if compute_dict:\n",
    "    # Get bounds for each fold\n",
    "    multitask_bounds_perf_dict = extract_performance_bounds(\n",
    "        perf_bounds_path, input_length,\n",
    "        coord_sets=[\n",
    "            filter_coords_by_chroms(all_coords, chroms) for chroms in all_fold_test_chroms\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    multitask_perf_dict = {key : [] for key in multitask_bounds_perf_dict}\n",
    "\n",
    "    # 10 folds\n",
    "    for fold_index, pred_path in enumerate(multitask_preds_paths):\n",
    "        perf_dict = extract_performance_metrics(\n",
    "            pred_path,\n",
    "            coord_sets=[filter_coords_by_chroms(all_coords, all_fold_test_chroms[fold_index])]\n",
    "        )\n",
    "        for key in multitask_perf_dict.keys():\n",
    "            vals = perf_dict[key][0]\n",
    "            if not key.endswith(\"pearson\") and not key.endswith(\"spearman\"):\n",
    "                lower = multitask_bounds_perf_dict[key][fold_index][0]\n",
    "                upper = multitask_bounds_perf_dict[key][fold_index][1]\n",
    "                vals = min_max_normalize(lower, vals, upper)\n",
    "            multitask_perf_dict[key].append(vals)\n",
    "\n",
    "    # Fine-tuned\n",
    "    perf_dict = extract_performance_metrics(\n",
    "        multitask_finetuned_preds_path,\n",
    "        coord_sets=[filter_coords_by_chroms(all_coords, best_multitask_fold_test_chroms)]\n",
    "    )\n",
    "    for key in multitask_perf_dict.keys():\n",
    "        vals = perf_dict[key][0]\n",
    "        if not key.endswith(\"pearson\") and not key.endswith(\"spearman\"):\n",
    "            # Fold index is best_multitask_fold - 1\n",
    "            lower = multitask_bounds_perf_dict[key][best_multitask_fold - 1][0]\n",
    "            upper = multitask_bounds_perf_dict[key][best_multitask_fold - 1][1]\n",
    "            vals = min_max_normalize(lower, vals, upper)\n",
    "        multitask_perf_dict[key].append(vals)\n",
    "    \n",
    "    if perf_cache_dir:\n",
    "        save_perf_dict(multitask_perf_dict, perf_dict_path)\n",
    "    \n",
    "# Plot the CDFs and bar plots of the profile/counts metrics\n",
    "cond_labels = [(\"Fold %d\" % i) for i in range(1, 11)]\n",
    "cond_labels += [\"Fine-tuned\\n(fold %d)\" % best_multitask_fold]\n",
    "metric_figs = plot_performances(\n",
    "    multitask_perf_dict, \"%s multi-task models\" % tf_name, cond_labels=cond_labels\n",
    ")\n",
    "\n",
    "# Import the total counts values and predictions\n",
    "multitask_counts = []\n",
    "for pred_path in multitask_preds_paths:\n",
    "    multitask_counts.append(extract_counts(pred_path))\n",
    "multitask_counts.append(extract_counts(multitask_finetuned_preds_path))\n",
    "\n",
    "# Show counts scatterplots for each condition\n",
    "scatter_fig = plot_count_scatterplots(multitask_counts, cond_labels=cond_labels)\n",
    "\n",
    "if perf_cache_dir:\n",
    "    for key in metric_figs:\n",
    "        metric_figs[key].savefig(os.path.join(perf_cache_dir, \"multitask_allfolds_finetuned_%s.png\" % key))\n",
    "    scatter_fig.savefig(os.path.join(perf_cache_dir, \"multitask_allfolds_finetuned_scattercounts.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"singletask-fold\"></a>\n",
    "### Single-task profile model performance across all 10 folds\n",
    "**For each task**, a comparison of the test-set performance between:\n",
    "1. Single-task profile models trained across all 10 folds\n",
    "2. A fine-tuned single-task profile model on the best-performing fold\n",
    "3. Upper and lower bounds on the best-performing fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for task_index in range(len(task_coords)):\n",
    "    display(vdomh.h3(\"Task %d\" % (task_index)))\n",
    "    compute_dict = True\n",
    "    if perf_cache_dir:\n",
    "        # Import if it exists\n",
    "        perf_dict_path = os.path.join(perf_cache_dir, \"singletask_task%d_allfolds_finetuned.h5\" % task_index)\n",
    "        if os.path.exists(perf_dict_path) and os.stat(perf_dict_path).st_size:\n",
    "            singletask_perf_dict = load_perf_dict(perf_dict_path)\n",
    "            compute_dict = False\n",
    "            \n",
    "    if compute_dict:\n",
    "        # Get bounds for each fold\n",
    "        singletask_bounds_perf_dict = extract_performance_bounds(\n",
    "            perf_bounds_path, input_length,\n",
    "            coord_sets=[\n",
    "                filter_coords_by_chroms(task_coords[task_index], chroms)\n",
    "                for chroms in all_fold_test_chroms\n",
    "            ],\n",
    "            task_inds=([task_index] * len(all_fold_test_chroms))\n",
    "        )\n",
    "\n",
    "        singletask_perf_dict = {key : [] for key in singletask_bounds_perf_dict}\n",
    "\n",
    "        # 10 folds\n",
    "        for fold_index, pred_path in enumerate(singletask_preds_paths[task_index]):\n",
    "            perf_dict = extract_performance_metrics(\n",
    "                pred_path,\n",
    "                coord_sets=[filter_coords_by_chroms(task_coords[task_index], all_fold_test_chroms[fold_index])]\n",
    "            )\n",
    "            # No need to specify specific task indices, because single-task model\n",
    "            # predictions are saved only for that one task\n",
    "            for key in singletask_perf_dict.keys():\n",
    "                vals = perf_dict[key][0]\n",
    "                if not key.endswith(\"pearson\") and not key.endswith(\"spearman\"):\n",
    "                    lower = singletask_bounds_perf_dict[key][fold_index][0]\n",
    "                    upper = singletask_bounds_perf_dict[key][fold_index][1]\n",
    "                    vals = min_max_normalize(lower, vals, upper)\n",
    "                singletask_perf_dict[key].append(vals)\n",
    "\n",
    "        # Fine-tuned\n",
    "        perf_dict = extract_performance_metrics(\n",
    "            singletask_finetuned_preds_paths[task_index],\n",
    "            coord_sets=[filter_coords_by_chroms(task_coords[task_index], best_singletask_fold_test_chroms[task_index])]\n",
    "        )\n",
    "        # No need to specify specific task indices, because single-task model\n",
    "        # predictions are saved only for that one task\n",
    "        for key in singletask_perf_dict.keys():\n",
    "            vals = perf_dict[key][0]\n",
    "            if not key.endswith(\"pearson\") and not key.endswith(\"spearman\"):\n",
    "                # Fold index is best_singletask_folds[task_index] - 1\n",
    "                lower = singletask_bounds_perf_dict[key][best_singletask_folds[task_index] - 1][0]\n",
    "                upper = singletask_bounds_perf_dict[key][best_singletask_folds[task_index] - 1][1]\n",
    "                vals = min_max_normalize(lower, vals, upper)\n",
    "            singletask_perf_dict[key].append(vals)\n",
    "            \n",
    "        if perf_cache_dir:\n",
    "            save_perf_dict(singletask_perf_dict, perf_dict_path)\n",
    "            \n",
    "    # Plot the CDFs and bar plots of the profile/counts metrics\n",
    "    cond_labels = [(\"Fold %d\" % i) for i in range(1, 11)]\n",
    "    cond_labels += [\"Fine-tuned\\n(fold %d)\" % best_singletask_folds[task_index]]\n",
    "    metric_figs = plot_performances(\n",
    "        singletask_perf_dict, \"%s single-task models (task %d)\" % (tf_name, task_index),\n",
    "        cond_labels=cond_labels\n",
    "    )\n",
    "\n",
    "    # Import the total counts values and predictions\n",
    "    singletask_counts = []\n",
    "    for pred_path in singletask_preds_paths[task_index]:\n",
    "        singletask_counts.append(extract_counts(pred_path))\n",
    "    singletask_counts.append(\n",
    "        extract_counts(singletask_finetuned_preds_paths[task_index])\n",
    "    )\n",
    "    # No need to specify specific coordinates or task indices, because\n",
    "    # single-task model predictions are saved only for that one task\n",
    "\n",
    "    # Show counts scatterplots for each condition\n",
    "    scatter_fig = plot_count_scatterplots(singletask_counts, cond_labels=cond_labels)\n",
    "    \n",
    "    if perf_cache_dir:\n",
    "        for key in metric_figs:\n",
    "            metric_figs[key].savefig(os.path.join(perf_cache_dir, \"singletask_task%d_allfolds_finetuned_%s.png\" % (task_index, key)))\n",
    "        scatter_fig.savefig(os.path.join(perf_cache_dir, \"singletask_task%d_allfolds_finetuned_scattercounts.png\" % task_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"finetune-multitask-task\"></a>\n",
    "### Fine-tuned multi-task profile model task-specific performance\n",
    "A comparison of the test-set performance for between each task of a multi-task profile model fine-tuned on the best-performing fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_dict = True\n",
    "if perf_cache_dir:\n",
    "    # Import if it exists\n",
    "    perf_dict_path = os.path.join(perf_cache_dir, \"multitask_finetuned_tasks.h5\")\n",
    "    if os.path.exists(perf_dict_path) and os.stat(perf_dict_path).st_size:\n",
    "        finetune_multitask_perf_dict = load_perf_dict(perf_dict_path)\n",
    "        compute_dict = False\n",
    "\n",
    "if compute_dict:\n",
    "    # Get bounds for each task (best multi-task fold only)\n",
    "    finetune_multitask_bounds_perf_dict = extract_performance_bounds(\n",
    "        perf_bounds_path, input_length,\n",
    "        coord_sets=[\n",
    "            filter_coords_by_chroms(coords, best_multitask_fold_test_chroms)\n",
    "            for coords in task_coords\n",
    "        ],\n",
    "        task_inds=(list(range(len(task_coords))))\n",
    "    )\n",
    "    \n",
    "    finetune_multitask_perf_dict = {key : [] for key in finetune_multitask_bounds_perf_dict}\n",
    "    \n",
    "    perf_dict = extract_performance_metrics(\n",
    "        multitask_finetuned_preds_path,\n",
    "        coord_sets=[\n",
    "            filter_coords_by_chroms(coords, best_multitask_fold_test_chroms)\n",
    "            for coords in task_coords\n",
    "        ],\n",
    "        task_inds=(list(range(len(task_coords))))\n",
    "    )\n",
    "    for task_index in range(len(task_coords)):\n",
    "        for key in finetune_multitask_perf_dict.keys():\n",
    "            vals = perf_dict[key][task_index]\n",
    "            if not key.endswith(\"pearson\") and not key.endswith(\"spearman\"):\n",
    "                # Fold index is best_singletask_folds[task_index] - 1\n",
    "                lower = finetune_multitask_bounds_perf_dict[key][task_index][0]\n",
    "                upper = finetune_multitask_bounds_perf_dict[key][task_index][1]\n",
    "                vals = min_max_normalize(lower, vals, upper)\n",
    "            finetune_multitask_perf_dict[key].append(vals)\n",
    "            \n",
    "    if perf_cache_dir:\n",
    "        save_perf_dict(finetune_multitask_perf_dict, perf_dict_path)\n",
    "        \n",
    "# Plot the CDFs and bar plots of the profile/counts metrics\n",
    "cond_labels = [(\"Task %d\" % i) for i in range(len(task_coords))]\n",
    "metric_figs = plot_performances(\n",
    "    finetune_multitask_perf_dict, \"%s fine-tuned multi-task model\" % tf_name, cond_labels=cond_labels\n",
    ")\n",
    "\n",
    "if perf_cache_dir:\n",
    "    for key in metric_figs:\n",
    "        metric_figs[key].savefig(os.path.join(perf_cache_dir, \"multitask_finetuned_tasks_%s.png\" % key))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
