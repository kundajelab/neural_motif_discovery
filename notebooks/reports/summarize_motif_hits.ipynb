{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to results\n",
    "[Proportion of peaks with hits](#peaks-with-hits)\n",
    "\n",
    "[Examples of motif hits](#example-hits)\n",
    "\n",
    "[Homotypic density of motifs in peaks](#density)\n",
    "\n",
    "[Co-occurrence of motifs in peaks](#co-occurrence)\n",
    "\n",
    "[Distance between co-occurring motifs](#distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reset -f\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/users/amtseng/tfmodisco/src/\"))\n",
    "from tfmodisco.run_tfmodisco import import_shap_scores\n",
    "from motif.read_motifs import trim_motif_by_ic, pfm_to_pwm, pfm_info_content\n",
    "from motif.moods import import_moods_hits\n",
    "from motif.tfmodisco_hit_scoring import import_tfmodisco_hits\n",
    "from util import figure_to_vdom_image, import_peak_table\n",
    "import plot.viz_sequence as viz_sequence\n",
    "from modisco.util import compute_per_position_ic, cpu_sliding_window_sum\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pomegranate\n",
    "import sklearn.cluster\n",
    "import scipy.cluster.hierarchy\n",
    "import scipy.stats\n",
    "import sklearn.isotonic\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import vdom.helpers as vdomh\n",
    "from IPython.display import display\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"figure.titlesize\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"font.size\": 13,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters/fetch arguments\n",
    "tf_name = os.environ[\"TFM_TF_NAME\"]\n",
    "tfm_results_path = os.environ[\"TFM_TFM_PATH\"]\n",
    "shap_scores_path = os.environ[\"TFM_SHAP_PATH\"]\n",
    "hyp_score_key = os.environ[\"TFM_HYP_SCORE_KEY\"]\n",
    "if \"TFM_TASK_INDEX\" in os.environ:\n",
    "    task_index = int(os.environ[\"TFM_TASK_INDEX\"])\n",
    "else:\n",
    "    task_index = None\n",
    "if \"TFM_PEAKS\" in os.environ:\n",
    "    # If provided, this overrides the peaks defined by the TF name and task index\n",
    "    peak_bed_paths = os.environ[\"TFM_PEAKS\"].split(\",\")\n",
    "else:\n",
    "    peak_bed_paths = []\n",
    "motif_hits_path = os.environ[\"TFM_HITS_PATH\"]\n",
    "if \"TFM_HITS_CACHE\" in os.environ:\n",
    "    hits_cache_dir = os.environ[\"TFM_HITS_CACHE\"]\n",
    "else:\n",
    "    hits_cache_dir = None\n",
    "\n",
    "print(\"TF name: %s\" % tf_name)\n",
    "print(\"TF-MoDISco results path: %s\" % tfm_results_path)\n",
    "print(\"DeepSHAP scores path: %s\" % shap_scores_path)\n",
    "print(\"Importance score key: %s\" % hyp_score_key)\n",
    "print(\"Task index: %s\" % task_index)\n",
    "print(\"Peak BED paths: %s\" % \",\".join(peak_bed_paths))\n",
    "print(\"Motif hits path: %s\" % motif_hits_path)\n",
    "print(\"Saved motif hits cache: %s\" % hits_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "input_length = 2114 if \"TFM_INPUT_LEN\" not in os.environ else int(os.environ[\"TFM_INPUT_LEN\"])\n",
    "motif_moods_imp_perc_cutoff = 0.10  # For MOODS hits\n",
    "motif_tfm_imp_prob_cutoff = 0.5  # For TF-MoDISco hits\n",
    "motif_tfm_save_imp_prob_cutoff = 0.99  # For TF-MoDISco hits\n",
    "motif_tfm_sim_prob_cutoff = 0.8  # For TF-MoDISco hits\n",
    "\n",
    "seed = 20210412\n",
    "\n",
    "# Paths to original called peaks\n",
    "if not peak_bed_paths:\n",
    "    # Use TF name and task index\n",
    "    base_path = \"/users/amtseng/tfmodisco/\"\n",
    "    data_path = os.path.join(base_path, \"data/processed/ENCODE/\")\n",
    "    labels_path = os.path.join(data_path, \"labels/%s\" % tf_name)\n",
    "    \n",
    "    all_peak_beds = sorted([item for item in os.listdir(labels_path) if item.endswith(\".bed.gz\")])\n",
    "    if task_index is None:\n",
    "        peak_bed_paths = [os.path.join(labels_path, item) for item in all_peak_beds]\n",
    "    else:\n",
    "        peak_bed_paths = [os.path.join(labels_path, all_peak_beds[task_index])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hits_cache_dir:\n",
    "    os.makedirs(hits_cache_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "For plotting and organizing things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tfmodisco_motifs(tfm_results_path, only_pos=True):\n",
    "    \"\"\"\n",
    "    Imports the PFMs to into a dictionary, mapping `(x, y)` to the PFM,\n",
    "    where `x` is the metacluster index and `y` is the pattern index.\n",
    "    Arguments:\n",
    "        `tfm_results_path`: path to HDF5 containing TF-MoDISco results\n",
    "        `only_pos`: if True, only return motifs with positive contributions\n",
    "    Returns the dictionary of PFMs, CWMs, and hCWMs.\n",
    "    \"\"\" \n",
    "    pfms, cwms, hcwms = {}, {}, {}\n",
    "    with h5py.File(tfm_results_path, \"r\") as f:\n",
    "        metaclusters = f[\"metacluster_idx_to_submetacluster_results\"]\n",
    "        num_metaclusters = len(metaclusters.keys())\n",
    "        for metacluster_i, metacluster_key in enumerate(metaclusters.keys()):\n",
    "            metacluster = metaclusters[metacluster_key]\n",
    "            if \"patterns\" not in metacluster[\"seqlets_to_patterns_result\"]:\n",
    "                continue\n",
    "            patterns = metacluster[\"seqlets_to_patterns_result\"][\"patterns\"]\n",
    "            num_patterns = len(patterns[\"all_pattern_names\"][:])\n",
    "            for pattern_i, pattern_name in enumerate(patterns[\"all_pattern_names\"][:]):\n",
    "                pattern_name = pattern_name.decode()\n",
    "                pattern = patterns[pattern_name]\n",
    "                pfm = pattern[\"sequence\"][\"fwd\"][:]\n",
    "                cwm = pattern[\"task0_contrib_scores\"][\"fwd\"][:]\n",
    "                hcwm = pattern[\"task0_hypothetical_contribs\"][\"fwd\"][:]\n",
    "                \n",
    "                # Check that the contribution scores are overall positive\n",
    "                if only_pos and np.sum(cwm) < 0:\n",
    "                    continue\n",
    "                    \n",
    "                pfms[\"%d_%d\" % (metacluster_i,pattern_i)] = pfm\n",
    "                cwms[\"%d_%d\" % (metacluster_i,pattern_i)] = cwm\n",
    "                hcwms[\"%d_%d\" % (metacluster_i,pattern_i)] = hcwm\n",
    "    return pfms, cwms, hcwms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_motif_hits(motif_hits_path):\n",
    "    \"\"\"\n",
    "    Imports the motif hits, which may be an output of MOODS scanning\n",
    "    or TF-MoDISco hit scanning. Depending on the number of columns, the\n",
    "    hits are imported appropriately.\n",
    "    \"\"\"\n",
    "    with open(motif_hits_path, \"r\") as f:\n",
    "        cols = next(f).split(\"\\t\")\n",
    "    if len(cols) == 10:\n",
    "        print(\"MOODS hits\")\n",
    "        hit_table = import_moods_hits(motif_hits_path)\n",
    "    elif len(cols) == 16:\n",
    "        print(\"TF-MoDISco hits\")\n",
    "        hit_table = import_tfmodisco_hits(motif_hits_path)\n",
    "        \n",
    "        # Sort by aggregate similarity and drop duplicates (by strand)\n",
    "        hit_table = hit_table.sort_values(\"agg_sim\")\n",
    "        hit_table = hit_table.drop_duplicates([\"chrom\", \"start\", \"end\", \"peak_index\"], keep=\"last\")\n",
    "    else:\n",
    "        raise ValueError(\"Motif hits file of unknown format/source: %s\" % motif_hits_path)\n",
    "    return hit_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mode(x_values, bins=200, levels=1):\n",
    "    \"\"\"\n",
    "    Estimates the mode of the distribution using `levels`\n",
    "    iterations of histograms.\n",
    "    \"\"\"\n",
    "    hist, edges = np.histogram(x_values, bins=bins)\n",
    "    bin_mode = np.argmax(hist)\n",
    "    left_edge, right_edge = edges[bin_mode], edges[bin_mode + 1]\n",
    "    if levels <= 1:\n",
    "        return (left_edge + right_edge) / 2\n",
    "    else:\n",
    "        return estimate_mode(\n",
    "            x_values[(x_values >= left_edge) & (x_values < right_edge)],\n",
    "            bins=bins,\n",
    "            levels=(levels - 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tight_exponential_dist(x_values, mode=0, percentiles=np.arange(0.05, 1, 0.05)):\n",
    "    \"\"\"\n",
    "    Given an array of x-values and a set of percentiles of the distribution,\n",
    "    computes the set of lambda values for an exponential distribution if the\n",
    "    distribution were fit to each percentile of the x-values. Returns an array\n",
    "    of lambda values parallel to `percentiles`. The exponential distribution\n",
    "    is assumed to have the given mean/mode, and all data less than this mode\n",
    "    is tossed out when doing this computation.\n",
    "    \"\"\"\n",
    "    assert np.min(percentiles) >= 0 and np.max(percentiles) <= 1\n",
    "    x_values = x_values[x_values >= mode]\n",
    "    per_x_vals = np.percentile(x_values, percentiles * 100)\n",
    "    return -np.log(1 - percentiles) / (per_x_vals - mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_pdf(x_values, lamb):\n",
    "    return lamb * np.exp(-lamb * x_values)\n",
    "def exponential_cdf(x_values, lamb):\n",
    "    return 1 - np.exp(-lamb * x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_moods_peak_hits(hit_table, score_column=\"imp_frac_score\", imp_perc_cutoff=0.05):\n",
    "    \"\"\"\n",
    "    Filters the table of peak hits by the score defined by\n",
    "    `score_column` by fitting a mixture model to the score\n",
    "    distribution, taking the exponential component, and then fitting a\n",
    "    percentile-tightened exponential distribution to this component.\n",
    "    The lowest percentile specified by `imp_perc_cutoff` of this null is\n",
    "    cut out. Returns a reduced hit table of the same format, a figure for the score\n",
    "    distribution.\n",
    "    \"\"\"\n",
    "    scores = hit_table[score_column].values\n",
    "    scores_finite = scores[np.isfinite(scores)]\n",
    "    \n",
    "    mode = estimate_mode(scores_finite)\n",
    "\n",
    "    # Fit mixture of models to scores (mode-shifted)\n",
    "    over_mode_scores = scores_finite[scores_finite >= mode] - mode\n",
    "    mixed_model = pomegranate.GeneralMixtureModel.from_samples(\n",
    "        [\n",
    "            pomegranate.ExponentialDistribution,\n",
    "            pomegranate.NormalDistribution,\n",
    "            pomegranate.NormalDistribution\n",
    "        ],\n",
    "        3, over_mode_scores[:, None]\n",
    "    )\n",
    "    mixed_model = mixed_model.fit(over_mode_scores)\n",
    "    mixed_model_exp_dist = mixed_model.distributions[0]\n",
    "    \n",
    "    # Obtain a distribution of scores that belong to the exponential distribution\n",
    "    exp_scores = over_mode_scores[mixed_model.predict(over_mode_scores[:, None]) == 0]\n",
    "    \n",
    "    # Fit a tight exponential distribution based on percentiles\n",
    "    lamb = np.max(fit_tight_exponential_dist(exp_scores))\n",
    "    \n",
    "    # Plot score distribution and fit\n",
    "    \n",
    "    score_fig, ax = plt.subplots(nrows=3, figsize=(20, 20))\n",
    "\n",
    "    x = np.linspace(np.min(scores_finite), np.max(scores_finite), 200)[1:]  # Skip first bucket (it's usually very large\n",
    "    mix_dist_pdf = mixed_model.probability(x)\n",
    "    mixed_model_exp_dist_pdf = mixed_model_exp_dist.probability(x)\n",
    "\n",
    "    perc_dist_pdf = exponential_pdf(x, lamb)\n",
    "    perc_dist_cdf = exponential_cdf(x, lamb)\n",
    "    \n",
    "    thresh = scipy.stats.expon.ppf(imp_perc_cutoff, loc=mode, scale=(1 / lamb))\n",
    "\n",
    "    # Plot mixed model\n",
    "    ax[0].hist(over_mode_scores + mode, bins=500, density=True, alpha=0.3)\n",
    "    ax[0].axvline(mode)\n",
    "    ax[0].plot(x + mode, mix_dist_pdf, label=\"Mixed model\")\n",
    "    ax[0].plot(x + mode, mixed_model_exp_dist_pdf, label=\"Exponential component\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot fitted PDF\n",
    "    ax[1].hist(exp_scores, bins=500, density=True, alpha=0.3)\n",
    "    ax[1].plot(x + mode, perc_dist_pdf, label=\"Percentile-fitted\")\n",
    "    ax[1].axvline(thresh)\n",
    "\n",
    "    # Plot fitted CDF\n",
    "    ax[2].hist(exp_scores, bins=500, density=True, alpha=1, cumulative=True, histtype=\"step\")\n",
    "    ax[2].plot(x + mode, perc_dist_cdf, label=\"Percentile-fitted\")\n",
    "\n",
    "    ax[0].set_title(\"Motif hit scores\")\n",
    "    plt.show()\n",
    "    \n",
    "    return hit_table.loc[hit_table[score_column] >= thresh].reset_index(drop=True), score_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imp_score_dist(\n",
    "    act_scores, window_length, score_type=\"imp_frac_score\", center_cut_size=400, sample=10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the set of importance scores as a fraction or sum of absolute\n",
    "    or signed importance, using windows of the given length. Focuses on the central\n",
    "    bases defined by `center_cut_size`. Returns a NumPy array of values.\n",
    "    `act_scores` is an N x L x 4 array.\n",
    "    \"\"\"\n",
    "    assert score_type in (\"imp_total_signed_score\", \"imp_frac_signed_score\", \"imp_total_score\", \"imp_frac_score\")\n",
    "    start = (act_scores.shape[1] // 2) - (center_cut_size // 2)\n",
    "    end = start + center_cut_size\n",
    "    cut_scores = np.sum(act_scores, axis=2)[:, start:end]  # Shape: N x L'\n",
    "    \n",
    "    if score_type == \"imp_total_signed_score\":\n",
    "        scores = cut_scores\n",
    "    elif score_type == \"imp_total_score\":\n",
    "        scores = np.abs(cut_scores)\n",
    "    elif score_type == \"imp_frac_signed_score\":\n",
    "        totals = np.sum(cut_scores, axis=1, keepdims=True)\n",
    "        scores = np.divide(\n",
    "            cut_scores, totals,\n",
    "            out=np.zeros_like(cut_scores), where=(totals != 0)\n",
    "        )\n",
    "    else:\n",
    "        abs_cut_scores = np.abs(cut_scores)\n",
    "        totals = np.sum(abs_cut_scores, axis=1, keepdims=True)\n",
    "        scores = np.divide(\n",
    "            abs_cut_scores, totals,\n",
    "            out=np.zeros_like(abs_cut_scores), where=(totals != 0)\n",
    "        )\n",
    "    \n",
    "    window_sums = scipy.signal.correlate(scores, np.ones((1, window_length)), mode=\"valid\")\n",
    "    return np.random.choice(np.ravel(window_sums), size=min(sample, window_sums.size), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_normalize(seq, axis=None):\n",
    "    if axis is None:\n",
    "        total = np.sum(np.abs(seq))\n",
    "        return seq if not total else seq / total\n",
    "    else:\n",
    "        total = np.sum(np.abs(seq), axis=axis, keepdims=True)\n",
    "        return np.divide(seq, total, out=np.zeros_like(seq), where=(total != 0))\n",
    "\n",
    "def dot_product_vec(query_seq, target_seqs, normalize=True, revcomp=True):\n",
    "    \"\"\"\n",
    "    Takes an I x D query seq and N x I x D target seqs, and computes similarities\n",
    "    them, using a simple dot product. Returns an N-array.\n",
    "    If `revcomp` is True, takes the reverse complement independently for each and\n",
    "    returns the maximum.\n",
    "    \"\"\"\n",
    "    query_seq = np.expand_dims(query_seq, axis=0)  # 1 x I x D\n",
    "    \n",
    "    # L1-normalize\n",
    "    if normalize:\n",
    "        query_seq = l1_normalize(query_seq)\n",
    "        target_seqs = l1_normalize(target_seqs)\n",
    "    \n",
    "    sim = np.sum(query_seq * target_seqs, axis=(1, 2))\n",
    "    \n",
    "    if revcomp:\n",
    "        query_seq_rc = np.empty_like(query_seq)\n",
    "        for i in range(query_seq.shape[2] // 4):\n",
    "            query_seq_rc[:, :, (i * 4):((i + 1) * 4)] = np.flip(query_seq[:, :, (i * 4):((i + 1) * 4)])\n",
    "        rc_sim = np.sum(query_seq_rc * target_seqs, axis=(1, 2))\n",
    "        return np.maximum(sim, rc_sim)\n",
    "    else:\n",
    "        return sim\n",
    "    \n",
    "def get_motif_score_cosine_sim_dist(\n",
    "    act_scores, cwm, hyp_scores=None, hcwm=None, sample=-1, window_inds=None,\n",
    "    normalize=True, center_cut_size=400\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes a sample of cosine similarities between the CWM and the\n",
    "    actual importance scores. If `hyp_scores` and `hcwm` are also provided, then\n",
    "    computes the similarity between the actual/hypothetical scores and motifs,\n",
    "    concatenated along the bases dimension. `act_scores` and `hyp_scores`\n",
    "    are an N x L x 4 array. Samples only `sample` windows. If `window_inds` is\n",
    "    given, it must be an M x 2 array of sequence indices and window indices\n",
    "    to sample from (out of the original N and L); otherwise, random windows\n",
    "    are sampled from the central `center_cut_size` of the tracks.\n",
    "    Returns a NumPy array of values.\n",
    "    \"\"\"\n",
    "    if hyp_scores is not None:\n",
    "        assert act_scores.shape == hyp_scores.shape\n",
    "        assert hcwm is not None\n",
    "    motif_len = cwm.shape[0]\n",
    "    \n",
    "    if window_inds is None:\n",
    "        if sample > 0:\n",
    "            num_samples = min(sample, act_scores.shape[0] * (center_cut_size - motif_len + 1))\n",
    "            seq_inds = np.random.choice(act_scores.shape[0], size=num_samples)\n",
    "            window_starts = np.random.choice(center_cut_size - motif_len + 1, size=num_samples)\n",
    "            window_starts = window_starts + (act_scores.shape[1] // 2) - (center_cut_size // 2)\n",
    "        else:\n",
    "            seq_inds = np.repeat(np.arange(act_scores.shape[0]), center_cut_size - motif_len + 1)\n",
    "            window_starts = np.tile(np.arange(center_cut_size - motif_len + 1), act_scores.shape[0])\n",
    "    else:\n",
    "        if sample > 0:\n",
    "            num_samples = min(sample, len(window_inds))\n",
    "            sample_inds = np.random.choice(len(window_inds), size=num_samples, replace=False)\n",
    "            seq_inds = window_inds[sample_inds, 0]\n",
    "            window_starts = window_inds[sample_inds, 1]\n",
    "        else:\n",
    "            seq_inds = window_inds[:, 0]\n",
    "            window_starts = window_inds[:, 1]\n",
    "    \n",
    "    act_windows = act_scores[\n",
    "        np.expand_dims(seq_inds, axis=1),\n",
    "        np.linspace(window_starts, window_starts + motif_len - 1, motif_len, axis=1).astype(int)\n",
    "    ]\n",
    "    if normalize:\n",
    "        act_windows = l1_normalize(act_windows, axis=(1, 2))\n",
    "        cwm = l1_normalize(cwm)\n",
    "\n",
    "    if hyp_scores is not None:\n",
    "        hyp_windows = hyp_scores[\n",
    "            np.expand_dims(seq_inds, axis=1),\n",
    "            np.linspace(window_starts, window_starts + motif_len - 1, motif_len, axis=1).astype(int)\n",
    "        ]\n",
    "        if normalize:\n",
    "            hyp_windows = l1_normalize(hyp_windows, axis=(1, 2))\n",
    "            hcwm = l1_normalize(hcwm)\n",
    "        windows = np.concatenate([act_windows, hyp_windows], axis=2)\n",
    "        motif = np.concatenate([cwm, hcwm], axis=1)\n",
    "    else:\n",
    "        windows = act_windows\n",
    "        motif = cwm\n",
    "    \n",
    "    return dot_product_vec(motif, windows, normalize=False)\n",
    "\n",
    "def get_ic_scaled_motif_score_sim_dist(\n",
    "    act_scores, pfm, hcwm, sample=-1, window_inds=None,\n",
    "    normalize=True, center_cut_size=400\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes a sample of dot-product similarities between the hCWM and the\n",
    "    actual importance scores. The hCWM is scaled by information content,\n",
    "    based on the given PFM. `act_scores` is an N x L x 4 array. Samples\n",
    "    only `sample` windows. If `window_inds` is given, it must be an M x 2\n",
    "    array of sequence indices and window indices to sample from (out of the\n",
    "    original N and L); otherwise, random windows are sampled from the central\n",
    "    `center_cut_size` of the tracks.\n",
    "    Returns a NumPy array of values.\n",
    "    \"\"\"\n",
    "    motif_len = hcwm.shape[0]\n",
    "    \n",
    "    if window_inds is None:\n",
    "        if sample > 0:\n",
    "            num_samples = min(sample, act_scores.shape[0] * (center_cut_size - motif_len + 1))\n",
    "            seq_inds = np.random.choice(act_scores.shape[0], size=num_samples)\n",
    "            window_starts = np.random.choice(center_cut_size - motif_len + 1, size=num_samples)\n",
    "            window_starts = window_starts + (act_scores.shape[1] // 2) - (center_cut_size // 2)\n",
    "        else:\n",
    "            seq_inds = np.repeat(np.arange(act_scores.shape[0]), center_cut_size - motif_len + 1)\n",
    "            window_starts = np.tile(np.arange(center_cut_size - motif_len + 1), act_scores.shape[0])\n",
    "    else:\n",
    "        if sample > 0:\n",
    "            num_samples = min(sample, len(window_inds))\n",
    "            sample_inds = np.random.choice(len(window_inds), size=num_samples, replace=False)\n",
    "            seq_inds = window_inds[sample_inds, 0]\n",
    "            window_starts = window_inds[sample_inds, 1]\n",
    "        else:\n",
    "            seq_inds = window_inds[:, 0]\n",
    "            window_starts = window_inds[:, 1]\n",
    "    \n",
    "    act_windows = act_scores[\n",
    "        np.expand_dims(seq_inds, axis=1),\n",
    "        np.linspace(window_starts, window_starts + motif_len - 1, motif_len, axis=1).astype(int)\n",
    "    ]\n",
    "    \n",
    "    if normalize:\n",
    "        act_windows = l1_normalize(act_windows, axis=(1, 2))\n",
    "        hcwm = l1_normalize(hcwm)\n",
    "        \n",
    "    hcwm = np.expand_dims(pfm_info_content(pfm), axis=1) * hcwm\n",
    "    \n",
    "    return dot_product_vec(hcwm, act_windows, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tfm_peak_hits(\n",
    "    hit_table, shap_coords, act_scores, cwms, imp_score_column=\"imp_frac_score\",\n",
    "    imp_thresh=0.5, sim_thresh=0.99, save_imp_thresh=0.8\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters the table of peak hits. Filters based on importance and\n",
    "    similarity. `imp_score_column` defines the importance score column\n",
    "    to filter on. `shap_coords` is an N x 3 object array denoting coordinates\n",
    "    of importance scores. The `peak_index` column of `hit_table` must index\n",
    "    into these coordinates. `act_scores` is a parallel N x L x 4 array of\n",
    "    actual importance scores. `cwms` is a dictionary mapping motif keys to\n",
    "    CWMs, and must match the motifs used by the motif hit scorer exactly.\n",
    "    Returns a reduced hit table of the same format, and a dictionary of\n",
    "    figures of the distributions used for filtering, one for each motif key.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    filter_mask = np.zeros(len(hit_table), dtype=bool)  # All False\n",
    "    \n",
    "    motif_lengths = dict(zip(hit_table[\"key\"], hit_table[\"end\"] - hit_table[\"start\"]))\n",
    "    motif_keys = sorted(motif_lengths.keys())\n",
    "    \n",
    "    filter_figs = {}\n",
    "    for motif_key in motif_keys:\n",
    "        cwm = cwms[motif_key]\n",
    "        motif_hit_table = hit_table[hit_table[\"key\"] == motif_key]\n",
    "        \n",
    "        filter_fig, ax = plt.subplots(nrows=4, figsize=(20, 16))\n",
    "        \n",
    "        # Importance\n",
    "        \n",
    "        hit_imp_scores = motif_hit_table[imp_score_column].values\n",
    "        hit_imp_scores_finite = hit_imp_scores[np.isfinite(hit_imp_scores)]\n",
    "        \n",
    "        bg_imp_scores = get_imp_score_dist(act_scores, motif_lengths[motif_key], score_type=imp_score_column, sample=len(hit_imp_scores))\n",
    "        bg_imp_scores_finite = bg_imp_scores[np.isfinite(bg_imp_scores)]\n",
    "        \n",
    "        x = np.linspace(0, 1.0, 2000)  # Restrict to positive and under 1\n",
    "        reg_x = np.concatenate([bg_imp_scores_finite, hit_imp_scores_finite])\n",
    "        reg_y = np.concatenate([np.zeros(len(bg_imp_scores_finite)), np.ones(len(hit_imp_scores_finite))])\n",
    "        iso_reg_model = sklearn.isotonic.IsotonicRegression()\n",
    "        iso_reg_model.fit(reg_x, reg_y)\n",
    "        iso_preds = iso_reg_model.predict(x)\n",
    "        pass_inds = np.where(iso_preds >= 0.5)[0]\n",
    "        imp_thresh = x[np.min(pass_inds)] if pass_inds.size else np.max(hit_imp_scores_finite)\n",
    "        pass_inds = np.where(iso_preds >= 0.8)[0]\n",
    "        save_imp_thresh = x[np.min(pass_inds)] if pass_inds.size else np.max(hit_imp_scores_finite)\n",
    "        \n",
    "        ax[0].scatter(np.clip(bg_imp_scores_finite, 0, 1), np.zeros(len(bg_imp_scores_finite)), alpha=0.05, label=\"Background importance scores\")\n",
    "        ax[0].scatter(np.clip(hit_imp_scores_finite, 0, 1), np.ones(len(hit_imp_scores_finite)), alpha=0.05, label=\"Hit importance scores\")\n",
    "        ax[0].plot(x, iso_preds)\n",
    "        ax[0].axvline(imp_thresh)\n",
    "        ax[0].axvline(save_imp_thresh)\n",
    "        ymin, ymax = ax[0].get_ylim()\n",
    "        ax[0].annotate(\"%f\" % imp_thresh, xy=(imp_thresh, (ymin + ymax) * 0.25))\n",
    "        ax[0].annotate(\"%f\" % save_imp_thresh, xy=(save_imp_thresh, (ymin + ymax) * 0.75))\n",
    "        ax[0].set_title(imp_score_column)\n",
    "        ax[0].legend()\n",
    "        \n",
    "        ax[1].hist(bg_imp_scores_finite, bins=x, density=True, alpha=0.3, label=\"Background importance scores\")\n",
    "        ax[1].hist(hit_imp_scores_finite, bins=x, density=True, alpha=0.3, label=\"Hit importance scores\")\n",
    "        ax[1].axvline(imp_thresh)\n",
    "        ax[1].axvline(save_imp_thresh)\n",
    "        ymin, ymax = ax[1].get_ylim()\n",
    "        ax[1].annotate(\"%f\" % imp_thresh, xy=(imp_thresh, (ymin + ymax) * 0.25))\n",
    "        ax[1].annotate(\"%f\" % save_imp_thresh, xy=(save_imp_thresh, (ymin + ymax) * 0.75))\n",
    "        ax[1].legend()\n",
    "        \n",
    "        # Similarity\n",
    "\n",
    "        window_inds = np.empty((len(motif_hit_table), 2), dtype=int)\n",
    "        window_inds[:, 0] = motif_hit_table[\"peak_index\"].values\n",
    "        window_inds[:, 1] = motif_hit_table[\"start\"].values - shap_coords[:, 1][window_inds[:, 0]]\n",
    "        \n",
    "        hit_sim_scores = get_motif_score_cosine_sim_dist(act_scores, cwm, window_inds=window_inds)\n",
    "        hit_sim_scores_finite = hit_sim_scores[np.isfinite(hit_sim_scores)]\n",
    "\n",
    "        bg_sim_scores = get_motif_score_cosine_sim_dist(act_scores, cwm, sample=len(hit_sim_scores))\n",
    "        bg_sim_scores_finite = bg_sim_scores[np.isfinite(bg_sim_scores)]\n",
    "\n",
    "        x = np.linspace(\n",
    "            min(np.min(bg_sim_scores_finite), np.min(hit_sim_scores_finite)),\n",
    "            max(np.max(bg_sim_scores_finite), np.max(hit_sim_scores_finite)), 2000\n",
    "        )\n",
    "        reg_x = np.concatenate([bg_sim_scores_finite, hit_sim_scores_finite])\n",
    "        reg_y = np.concatenate([np.zeros(len(bg_sim_scores_finite)), np.ones(len(hit_sim_scores_finite))])\n",
    "        iso_reg_model = sklearn.isotonic.IsotonicRegression()\n",
    "        iso_reg_model.fit(reg_x, reg_y)\n",
    "        iso_preds = iso_reg_model.predict(x)\n",
    "        pass_inds = np.where(iso_preds >= 0.99)[0]\n",
    "        sim_thresh = x[np.min(pass_inds)] if pass_inds.size else 1\n",
    "\n",
    "        ax[2].scatter(bg_sim_scores_finite, np.zeros(len(bg_sim_scores_finite)), alpha=0.05, label=\"Background similarity scores\")\n",
    "        ax[2].scatter(hit_sim_scores_finite, np.ones(len(hit_sim_scores_finite)), alpha=0.05, label=\"Hit similarity scores\")\n",
    "        ax[2].plot(x, iso_preds)\n",
    "        ax[2].axvline(sim_thresh)\n",
    "        ymin, ymax = ax[3].get_ylim()\n",
    "        ax[2].annotate(\"%f\" % sim_thresh, xy=(sim_thresh, (ymin + ymax) * 0.5))\n",
    "        ax[2].set_title(\"Actual cosine similarity\")\n",
    "        ax[2].legend()\n",
    "\n",
    "        ax[3].hist(bg_sim_scores_finite, bins=500, density=True, alpha=0.3, label=\"Background similarity scores\")\n",
    "        ax[3].hist(hit_sim_scores_finite, bins=500, density=True, alpha=0.3, label=\"Hit similarity scores\")\n",
    "        ax[3].axvline(sim_thresh)\n",
    "        ymin, ymax = ax[3].get_ylim()\n",
    "        ax[3].annotate(\"%f\" % sim_thresh, xy=(sim_thresh, (ymin + ymax) * 0.5))\n",
    "        ax[3].legend()\n",
    "\n",
    "        filter_fig.suptitle(\"Filtering motif %s\" % motif_key)\n",
    "        \n",
    "        filter_figs[motif_key] = filter_fig\n",
    "        plt.show()\n",
    "        \n",
    "        motif_mask = \\\n",
    "            ((hit_sim_scores >= sim_thresh) | (hit_imp_scores >= save_imp_thresh)) & \\\n",
    "            (motif_hit_table[imp_score_column] >= imp_thresh)\n",
    "        filter_mask[hit_table[\"key\"] == motif_key] = motif_mask\n",
    "        \n",
    "        # Show statistics on how many motifs were kept\n",
    "        orig_num = len(motif_hit_table)\n",
    "        orig_peak_num = len(np.unique(motif_hit_table[\"peak_index\"]))\n",
    "        new_num = np.sum(motif_mask)\n",
    "        new_peak_num = len(np.unique(motif_hit_table.loc[motif_mask][\"peak_index\"]))\n",
    "        num_peaks = 1 + np.max(hit_table[\"peak_index\"])\n",
    "        print(\"Hit number reduction: %d -> %d (%f)\" % (orig_num, new_num, (new_num - orig_num) / orig_num))\n",
    "        print(\"Proportion of peaks reduction: %f -> %f\" % (orig_peak_num / num_peaks, new_peak_num / num_peaks))\n",
    "        \n",
    "    return hit_table.loc[filter_mask], filter_figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_hits(peak_table, hit_table):\n",
    "    \"\"\"\n",
    "    For each peak, extracts the set of motif hits that fall in that peak.\n",
    "    Returns a list mapping peak index to a subtable of `hit_table`. The index\n",
    "    of the list is the index of the peak table.\n",
    "    \"\"\"\n",
    "    peak_hits = [pd.DataFrame(columns=list(hit_table))] * len(peak_table)\n",
    "    for peak_index, matches in tqdm.notebook.tqdm(hit_table.groupby(\"peak_index\")):\n",
    "        # Check that all of the matches are indeed overlapping the peak\n",
    "        peak_row = peak_table.iloc[peak_index]\n",
    "        chrom, start, end = peak_row[\"chrom\"], peak_row[\"peak_start\"], peak_row[\"peak_end\"]\n",
    "        assert np.all(matches[\"chrom\"] == chrom)\n",
    "        assert np.all((matches[\"start\"] < end) & (start < matches[\"end\"]))\n",
    "        \n",
    "        peak_hits[peak_index] = matches\n",
    "    return peak_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_motif_counts(peak_hits, motif_keys):\n",
    "    \"\"\"\n",
    "    From the peak hits (as returned by `get_peak_hits`), computes a count\n",
    "    array of size N x M, where N is the number of peaks and M is the number of\n",
    "    motifs. Each entry represents the number of times a motif appears in a peak.\n",
    "    `motif_keys` is a list of motif keys as they appear in `peak_hits`; the\n",
    "    order of the motifs M matches this list.\n",
    "    \"\"\"\n",
    "    motif_inds = {motif_keys[i] : i for i in range(len(motif_keys))}\n",
    "    counts = np.zeros((len(peak_hits), len(motif_keys)), dtype=int)\n",
    "    for i in tqdm.notebook.trange(len(peak_hits)):\n",
    "        hits = peak_hits[i]\n",
    "        for key, num in zip(*np.unique(hits[\"key\"], return_counts=True)):\n",
    "            counts[i][motif_inds[key]] = num\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_matrix_indices(matrix, num_clusters):\n",
    "    \"\"\"\n",
    "    Clusters matrix using k-means. Always clusters on the first\n",
    "    axis. Returns the indices needed to optimally order the matrix\n",
    "    by clusters.\n",
    "    \"\"\"\n",
    "    if len(matrix) == 1:\n",
    "        # Don't cluster at all\n",
    "        return np.array([0])\n",
    "\n",
    "    num_clusters = min(num_clusters, len(matrix))\n",
    "    \n",
    "    # Perform k-means clustering\n",
    "    kmeans = sklearn.cluster.MiniBatchKMeans(n_clusters=num_clusters)\n",
    "    cluster_assignments = kmeans.fit_predict(matrix)\n",
    "\n",
    "    # Perform hierarchical clustering on the cluster centers to determine optimal ordering\n",
    "    kmeans_centers = kmeans.cluster_centers_\n",
    "    cluster_order = scipy.cluster.hierarchy.leaves_list(\n",
    "        scipy.cluster.hierarchy.optimal_leaf_ordering(\n",
    "            scipy.cluster.hierarchy.linkage(kmeans_centers, method=\"centroid\"), kmeans_centers\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Order the peaks so that the cluster assignments follow the optimal ordering\n",
    "    cluster_inds = []\n",
    "    for cluster_id in cluster_order:\n",
    "        cluster_inds.append(np.where(cluster_assignments == cluster_id)[0])\n",
    "    cluster_inds = np.concatenate(cluster_inds)\n",
    "    return cluster_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_peak_motif_indicator_heatmap(peak_hit_counts, motif_keys, subsample=None):\n",
    "    \"\"\"\n",
    "    Plots a simple indicator heatmap of the motifs in each peak.\n",
    "    Returns the figure.\n",
    "    \"\"\"\n",
    "    # Subsample peaks\n",
    "    if subsample:\n",
    "        peak_hit_counts = peak_hit_counts[np.random.choice(\n",
    "            len(peak_hit_counts), size=min(len(peak_hit_counts), subsample), replace=False\n",
    "        )]\n",
    "        \n",
    "    peak_hit_indicators = (peak_hit_counts > 0).astype(int)\n",
    "    \n",
    "    # Order columns by prevalence (by number of peaks with that motif)\n",
    "    counts = np.sum(peak_hit_indicators, axis=0)\n",
    "    inds = np.flip(np.argsort(counts))\n",
    "    matrix = peak_hit_indicators[:, inds]\n",
    "    motif_keys = np.array(motif_keys)[inds]\n",
    "    \n",
    "    # Order rows in \"binary\" order\n",
    "    places = np.power(2, np.flip(np.arange(matrix.shape[1])))\n",
    "    values = np.sum(matrix * places, axis=1)\n",
    "    inds = np.flip(np.argsort(values))\n",
    "    matrix = matrix[inds]\n",
    "    \n",
    "    # Create a figure with the right dimensions\n",
    "    fig_height = min(len(peak_hit_indicators) * 0.004, 8)\n",
    "    fig, ax = plt.subplots(figsize=(16, fig_height))\n",
    "\n",
    "    # Plot the heatmap\n",
    "    ax.imshow(matrix, interpolation=\"nearest\", aspect=\"auto\", cmap=\"Greens\")\n",
    "\n",
    "    # Set axes on heatmap\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks(np.arange(len(motif_keys)))\n",
    "    ax.set_xticklabels(motif_keys)\n",
    "    ax.set_xlabel(\"Motif\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_homotypic_densities(peak_hit_counts, motif_keys):\n",
    "    \"\"\"\n",
    "    Plots a CDF of number of motif hits per peak, for each motif.\n",
    "    Returns a dictionary mapping motif key to figure.\n",
    "    \"\"\"\n",
    "    figs = {}\n",
    "    for i in range(len(motif_keys)):\n",
    "        counts = peak_hit_counts[:, i]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        bins = np.concatenate([np.arange(np.max(counts)), [np.inf]])\n",
    "        ax.hist(counts, bins=bins, density=True, histtype=\"step\", cumulative=True)\n",
    "        ax.set_title(\"Cumulative distribution of number of %s hits per peak\" % motif_keys[i])\n",
    "        ax.set_xlabel(\"Number of motifs k in peak\")\n",
    "        ax.set_ylabel(\"Proportion of peaks with at least k motifs\")\n",
    "        plt.show()\n",
    "        figs[motif_keys[i]] = fig\n",
    "    return figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif_cooccurrence_count_matrix(peak_hit_counts):\n",
    "    \"\"\"\n",
    "    From an N x M (peaks by motifs) array of hit counts, returns\n",
    "    an M x M array of counts (i.e. how many times two motifs occur\n",
    "    together in the same peak). For the diagonal entries, we require\n",
    "    that motif occur at least twice in a peak to be counted.\n",
    "    \"\"\"\n",
    "    peak_hit_indicators = (peak_hit_counts > 0).astype(int)\n",
    "    num_motifs = peak_hit_indicators.shape[1]\n",
    "    count_matrix = np.zeros((num_motifs, num_motifs), dtype=int)\n",
    "    for i in range(num_motifs):\n",
    "        for j in range(i):\n",
    "            pair_col = np.sum(peak_hit_indicators[:, [i, j]], axis=1)\n",
    "            count = np.sum(pair_col == 2)\n",
    "            count_matrix[i, j] = count\n",
    "            count_matrix[j, i] = count\n",
    "        count_matrix[i, i] = np.sum(peak_hit_counts[:, i] >= 2)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cooccurrence_pvals(peak_hit_counts):\n",
    "    \"\"\"\n",
    "    Given the number of motif hits in each peak, computes p-value of\n",
    "    co-occurrence for each pair of motifs, including self pairs.\n",
    "    Returns an M x M array of p-values for the M motifs.\n",
    "    \"\"\"\n",
    "    peak_hit_indicators = (peak_hit_counts > 0).astype(int)\n",
    "    num_peaks, num_motifs = peak_hit_counts.shape\n",
    "    \n",
    "    pvals = np.ones((num_motifs, num_motifs))\n",
    "    \n",
    "    # Significance is based on a Fisher's exact test. If the motifs were\n",
    "    # present in peaks randomly, we'd independence of occurrence.\n",
    "    # For self-co-occurrence, the null model is not independence, but\n",
    "    # collisions\n",
    "    for i in range(num_motifs):\n",
    "        for j in range(i):\n",
    "            pair_counts = peak_hit_indicators[:, [i, j]]\n",
    "            peaks_with_1 = pair_counts[:, 0] == 1\n",
    "            peaks_with_2 = pair_counts[:, 1] == 1\n",
    "            # Contingency table (universe is set of all peaks):\n",
    "            #              no motif 1  |  has motif 1\n",
    "            # no motif 2       A       |      B\n",
    "            # -------------------------+--------------\n",
    "            # has motif 2      C       |      D\n",
    "            # The Fisher's exact test evaluates the significance of the\n",
    "            # association between the two classifications\n",
    "            cont_table = np.array([\n",
    "                [\n",
    "                    np.sum(~(peaks_with_1) & (~peaks_with_2)),\n",
    "                    np.sum(peaks_with_1 & (~peaks_with_2))\n",
    "                ],\n",
    "                [\n",
    "                    np.sum(~(peaks_with_1) & peaks_with_2),\n",
    "                    np.sum(peaks_with_1 & peaks_with_2)\n",
    "                ]\n",
    "            ])\n",
    "            pval = scipy.stats.fisher_exact(\n",
    "                cont_table, alternative=\"greater\"\n",
    "            )[1]\n",
    "            pvals[i, j] = pval\n",
    "            pvals[j, i] = pval\n",
    "\n",
    "        # Self-co-occurrence: Poissonize balls in bins\n",
    "        # Expected number of collisions (via linearity of expectations):\n",
    "        num_hits = np.sum(peak_hit_indicators[:, i])  # number of \"balls\"\n",
    "        expected_collisions = num_hits * (num_hits - 1) / (2 * num_peaks)\n",
    "        num_collisions = np.sum(peak_hit_counts[:, i] >= 2)\n",
    "        if num_collisions == 0:\n",
    "            pval = 1\n",
    "        else:\n",
    "            pval = 1 - scipy.stats.poisson.cdf(num_collisions, mu=expected_collisions)\n",
    "        pvals[i, i] = pval\n",
    "        \n",
    "    return pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_motif_cooccurrence_heatmaps(count_matrix, pval_matrix, motif_keys):\n",
    "    \"\"\"\n",
    "    Plots a heatmap showing the number of peaks that have both types of\n",
    "    each motif, as well as a heatmap showing the p-value of co-occurrence.\n",
    "    Returns the p-value figure and the count figure, as well as the indices\n",
    "    of motifs used for clustering.\n",
    "    \"\"\"\n",
    "    assert count_matrix.shape == pval_matrix.shape\n",
    "    num_motifs = pval_matrix.shape[0]\n",
    "    assert len(motif_keys) == num_motifs\n",
    "\n",
    "    # Cluster by p-value\n",
    "    inds = cluster_matrix_indices(pval_matrix, max(5, num_motifs // 4))\n",
    "    pval_matrix = pval_matrix[inds][:, inds]\n",
    "    count_matrix = count_matrix[inds][:, inds]\n",
    "    motif_keys = np.array(motif_keys)[inds]\n",
    "    \n",
    "    # Plot the p-value matrix\n",
    "\n",
    "    fig_width = max(5, num_motifs)\n",
    "    p_fig, ax = plt.subplots(figsize=(fig_width, fig_width))\n",
    "    \n",
    "    # Replace 0s with minimum value (we'll label them properly later)\n",
    "    zero_mask = pval_matrix == 0\n",
    "    non_zeros = pval_matrix[~zero_mask]\n",
    "    if not len(non_zeros):\n",
    "        logpval_matrix = np.tile(np.inf, pval_matrix.shape)\n",
    "    else:\n",
    "        min_val = np.min(pval_matrix[~zero_mask])\n",
    "        pval_matrix[zero_mask] = min_val\n",
    "        logpval_matrix = -np.log10(pval_matrix)\n",
    "    \n",
    "    hmap = ax.imshow(logpval_matrix)\n",
    "\n",
    "    ax.set_xticks(np.arange(num_motifs))\n",
    "    ax.set_yticks(np.arange(num_motifs))\n",
    "    ax.set_xticklabels(motif_keys, rotation=45)\n",
    "    ax.set_yticklabels(motif_keys)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(num_motifs):\n",
    "        for j in range(num_motifs):\n",
    "            if zero_mask[i, j]:\n",
    "                text = \"Inf\"\n",
    "            else:\n",
    "                text = \"%.2f\" % np.abs(logpval_matrix[i, j])\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\")\n",
    "    p_fig.colorbar(hmap, orientation=\"horizontal\")\n",
    "\n",
    "    ax.set_title(\"-log(p) significance of peaks with both motifs\")\n",
    "    p_fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the counts matrix\n",
    "\n",
    "    fig_width = max(5, num_motifs)\n",
    "    c_fig, ax = plt.subplots(figsize=(fig_width, fig_width))\n",
    "    \n",
    "    hmap = ax.imshow(count_matrix)\n",
    "\n",
    "    ax.set_xticks(np.arange(num_motifs))\n",
    "    ax.set_yticks(np.arange(num_motifs))\n",
    "    ax.set_xticklabels(motif_keys, rotation=45)\n",
    "    ax.set_yticklabels(motif_keys)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(num_motifs):\n",
    "        for j in range(num_motifs):\n",
    "            ax.text(j, i, count_matrix[i, j], ha=\"center\", va=\"center\")\n",
    "    c_fig.colorbar(hmap, orientation=\"horizontal\")\n",
    "\n",
    "    ax.set_title(\"Number of peaks with both motifs\")\n",
    "    c_fig.tight_layout()\n",
    "    plt.show()\n",
    "    return p_fig, c_fig, inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_violin_plot(ax, dist_list, colors):\n",
    "    \"\"\"\n",
    "    Creates a violin plot on the given instantiated axes.\n",
    "    `dist_list` is a list of vectors. `colors` is a parallel\n",
    "    list of colors for each violin.\n",
    "    \"\"\"\n",
    "    num_perfs = len(dist_list)\n",
    "\n",
    "    q1, med, q3 = np.stack([\n",
    "        np.nanpercentile(data, [25, 50, 70], axis=0) for data in dist_list\n",
    "    ], axis=1)\n",
    "    iqr = q3 - q1\n",
    "    lower_outlier = q1 - (1.5 * iqr)\n",
    "    upper_outlier = q3 + (1.5 * iqr)\n",
    "\n",
    "\n",
    "    sorted_clipped_data = [  # Remove outliers based on outlier rule\n",
    "        np.sort(vec[(vec >= lower_outlier[i]) & (vec <= upper_outlier[i])])\n",
    "        for i, vec in enumerate(dist_list)\n",
    "    ]\n",
    "\n",
    "    plot_parts = ax.violinplot(\n",
    "        sorted_clipped_data, showmeans=False, showmedians=False, showextrema=False\n",
    "    )\n",
    "    violin_parts = plot_parts[\"bodies\"]\n",
    "    for i in range(num_perfs):\n",
    "        violin_parts[i].set_facecolor(colors[i])\n",
    "        violin_parts[i].set_edgecolor(colors[i])\n",
    "        violin_parts[i].set_alpha(0.7)\n",
    "\n",
    "    inds = np.arange(1, num_perfs + 1)\n",
    "    ax.vlines(inds, q1, q3, color=\"black\", linewidth=5, zorder=1)\n",
    "    ax.scatter(inds, med, marker=\"o\", color=\"white\", s=30, zorder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intermotif_distance_violins(peak_hits, motif_keys, pair_inds, cluster_inds):\n",
    "    \"\"\"\n",
    "    For each pair of motifs, plots a violin of distances beween\n",
    "    motifs. Returns a dictionary mapping pairs of motif keys to arrays\n",
    "    of distances, and the figure.\n",
    "    \"\"\"\n",
    "    # First, compute the distribution of distances for each pair\n",
    "    distance_dict = {}\n",
    "    key_pairs = []\n",
    "    for i, j in tqdm.notebook.tqdm(pair_inds):\n",
    "        dists = []\n",
    "        for k in range(len(peak_hits)):\n",
    "            hits = peak_hits[k]\n",
    "\n",
    "            hits_1 = hits[hits[\"key\"] == motif_keys[i]]\n",
    "            hits_2 = hits[hits[\"key\"] == motif_keys[j]]\n",
    "\n",
    "            if hits_1.empty or hits_2.empty:\n",
    "                continue\n",
    "\n",
    "            pos_1 = np.array(hits_1[\"start\"])\n",
    "            pos_2 = np.array(hits_2[\"start\"])\n",
    "\n",
    "            len_1 = (hits_1[\"end\"] - hits_1[\"start\"]).values[0]\n",
    "            len_2 = (hits_2[\"end\"] - hits_2[\"start\"]).values[0]\n",
    "\n",
    "            # Differences beteween all pairs of positions\n",
    "            diffs = pos_2[None] - pos_1[:, None]\n",
    "            # Take minimum distance for each instance of motif 2, but only\n",
    "            # if the distance is an appropriate length\n",
    "            for row in diffs:\n",
    "                row = row[row != 0]\n",
    "                if not row.size:\n",
    "                    continue\n",
    "                dist = row[np.argmin(np.abs(row))]\n",
    "                if (dist < 0 and dist < -len_2) or (dist > 0 and dist > len_1):\n",
    "                    dists.append(dist)\n",
    "        dists = np.array(dists)\n",
    "        if not dists.size:\n",
    "            continue\n",
    "        key_pair = (motif_keys[i], motif_keys[j])\n",
    "        key_pairs.append(key_pair)\n",
    "        distance_dict[key_pair] = np.abs(dists)  # Take absolute value of distance\n",
    "    \n",
    "    if not distance_dict:\n",
    "        print(\"No significantly co-occurring motifs\")\n",
    "        return distance_dict, None\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(\n",
    "        nrows=len(motif_keys), ncols=len(motif_keys),\n",
    "        figsize=(len(motif_keys) * 4, len(motif_keys) * 4)\n",
    "    )\n",
    "    if type(ax) is not np.ndarray:\n",
    "        ax = np.array([[ax]])\n",
    "\n",
    "    # Map motif key to axis index\n",
    "    key_to_index = dict(zip(np.array(motif_keys)[cluster_inds], np.arange(len(motif_keys))))\n",
    "\n",
    "    def clean_subplot(ax):\n",
    "        # Do this instead of ax.axis(\"off\"), which would also remove any\n",
    "        # axis labels\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        for orient in (\"top\", \"bottom\", \"left\", \"right\"):\n",
    "            ax.spines[orient].set_visible(False)\n",
    "\n",
    "    # Create violins\n",
    "    for i in range(len(motif_keys)):\n",
    "        for j in range(i, len(motif_keys)):\n",
    "            key_1, key_2 = motif_keys[i], motif_keys[j]\n",
    "            key_pair, rev_key_pair = (key_1, key_2), (key_2, key_1)\n",
    "            axis_1, axis_2 = key_to_index[key_1], key_to_index[key_2]\n",
    "            # Always plot lower triangle\n",
    "            if axis_1 < axis_2:\n",
    "                axis_1, axis_2 = axis_2, axis_1\n",
    "\n",
    "            if key_pair in distance_dict or rev_key_pair in distance_dict:\n",
    "                if rev_key_pair in distance_dict:\n",
    "                    key_pair = rev_key_pair\n",
    "                dist = distance_dict[key_pair] \n",
    "                create_violin_plot(ax[axis_1, axis_2], [dist], [\"mediumorchid\"])\n",
    "                ax[axis_1, axis_2].set_xticks([])  # Remove x-axis labels, as they don't mean much\n",
    "                if axis_1 != axis_2:\n",
    "                    # If off diagonal, clean the axes of the symmetric cell\n",
    "                    clean_subplot(ax[axis_2, axis_1])\n",
    "            else:\n",
    "                clean_subplot(ax[axis_1, axis_2])\n",
    "                clean_subplot(ax[axis_2, axis_1])\n",
    "\n",
    "    # Make motif labels\n",
    "    for i in range(len(motif_keys)):\n",
    "        ax[i, 0].set_ylabel(motif_keys[cluster_inds[i]])\n",
    "        ax[-1, i].set_xlabel(motif_keys[cluster_inds[i]])\n",
    "\n",
    "    # Remove x-axis labels/ticks\n",
    "    ax[-1, -1].set_xticks([])\n",
    "    fig.suptitle(\"Distance distributions between co-occurring motifs\")\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "\n",
    "    return distance_dict, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import hit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PFMs, CWMs, and hCWMs\n",
    "pfms, cwms, hcwms = import_tfmodisco_motifs(tfm_results_path)\n",
    "motif_keys = list(pfms.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import peaks\n",
    "peak_table = import_peak_table(peak_bed_paths)\n",
    "\n",
    "# Expand to input length\n",
    "peak_table[\"peak_start\"] = \\\n",
    "    (peak_table[\"peak_start\"] + peak_table[\"summit_offset\"]) - (input_length // 2)\n",
    "peak_table[\"peak_end\"] = peak_table[\"peak_start\"] + input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import DeepSHAP scores\n",
    "hyp_scores, act_scores, one_hot_seqs, shap_coords = import_shap_scores(\n",
    "    shap_scores_path, hyp_score_key, center_cut_size=None, remove_non_acgt=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit SHAP coordinates/scores to only those with matching peak coordinates\n",
    "shap_coords_table = pd.DataFrame(shap_coords, columns=[\"chrom\", \"start\", \"end\"])\n",
    "peak_coords_table = peak_table[[\"chrom\", \"peak_start\", \"peak_end\"]]\n",
    "\n",
    "ind_pairs = peak_coords_table.reset_index().merge(\n",
    "    shap_coords_table.reset_index(), how=\"left\", left_on=[\"chrom\", \"peak_start\", \"peak_end\"],\n",
    "    right_on=[\"chrom\", \"start\", \"end\"]\n",
    ")[[\"index_x\", \"index_y\"]].values\n",
    "# ind_pairs contains pairs (i, j) such that peak_table[i] matches shap_coords_table[j]\n",
    "# If peak_table[i] did not match a SHAP coord, then j will be NaN\n",
    "\n",
    "ind_pairs = ind_pairs[np.isfinite(ind_pairs[:, 1])].astype(int)  # Remove peak indices with no matches\n",
    "\n",
    "order_inds = np.full(len(peak_table), -1)\n",
    "order_inds[ind_pairs[:, 0]] = ind_pairs[:, 1]\n",
    "# If order_inds[i] == j, then peak_table[i] matches shap_coords_table[j]\n",
    "# Unless a SHAP coord doesn't match a peak, in which case order_inds[i] == -1\n",
    "\n",
    "shap_coords = shap_coords[order_inds]\n",
    "hyp_scores = hyp_scores[order_inds]\n",
    "act_scores = act_scores[order_inds]\n",
    "one_hot_seqs = one_hot_seqs[order_inds]\n",
    "\n",
    "bg_freq = np.mean(one_hot_seqs, axis=(0, 1))\n",
    "\n",
    "# Whenever a SHAP coord did not exist in the peak table, set to 0\n",
    "# This ensures that when we search for matches of DeepSHAP scores that don't\n",
    "# exist, we will find nothing\n",
    "shap_coords[order_inds < 0] = 0\n",
    "hyp_scores[order_inds < 0] = 0\n",
    "act_scores[order_inds < 0] = 0\n",
    "one_hot_seqs[order_inds < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import motif hits results\n",
    "hit_table = import_motif_hits(motif_hits_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim motifs properly\n",
    "if \"agg_sim\" in list(hit_table):\n",
    "    # Perform trimming: first to 25 bp, then by IC (as when hit scoring was performed)\n",
    "    window_size = 25\n",
    "    min_ic = 0.2\n",
    "\n",
    "    for motif_key in motif_keys:\n",
    "        pfm = pfms[motif_key]\n",
    "\n",
    "        # First level trimming\n",
    "        ic = compute_per_position_ic(pfm, bg_freq, 0.001)\n",
    "        start = np.argmax(cpu_sliding_window_sum(ic, window_size))\n",
    "        end = start + window_size\n",
    "\n",
    "        pfm = pfm[start:end]\n",
    "        cwm = cwms[motif_key][start:end]\n",
    "        hcwm = hcwms[motif_key][start:end]\n",
    "\n",
    "        # Second level trimming\n",
    "        ic = compute_per_position_ic(pfm, bg_freq, 0.001)\n",
    "        pass_inds = np.where(ic >= min_ic)[0]\n",
    "        start, end = np.min(pass_inds), np.max(pass_inds) + 1\n",
    "\n",
    "        pfms[motif_key] = pfm[start:end]\n",
    "        cwms[motif_key] = cwm[start:end]\n",
    "        hcwms[motif_key] = hcwm[start:end]\n",
    "else:\n",
    "    for motif_key in motif_keys:\n",
    "        pfm = pfms[motif_key]\n",
    "        \n",
    "        pfms[motif_key] = trim_motif_by_ic(pfm, pfm)\n",
    "        cwms[motif_key] = trim_motif_by_ic(pfm, cwms[motif_key])\n",
    "        hcwms[motif_key] = trim_motif_by_ic(pfm, hcwms[motif_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter motif hit table\n",
    "if \"agg_sim\" in list(hit_table):\n",
    "    hit_table_filtered, filter_figs = filter_tfm_peak_hits(\n",
    "        hit_table, shap_coords, act_scores, cwms,\n",
    "        imp_thresh=motif_tfm_imp_prob_cutoff, sim_thresh=motif_tfm_sim_prob_cutoff,\n",
    "        save_imp_thresh=motif_tfm_save_imp_prob_cutoff\n",
    "    )\n",
    "else:\n",
    "    hit_table_filtered, score_fig = filter_moods_peak_hits(\n",
    "        hit_table, imp_perc_cutoff=motif_moods_imp_perc_cutoff\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not hit_table_filtered.empty, \"Filtered out all %d original hits\" % len(hit_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match peaks to motif hits\n",
    "peak_hits = get_peak_hits(peak_table, hit_table_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct count array of peaks and hits\n",
    "peak_hit_counts = get_peak_motif_counts(peak_hits, motif_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct count matrix of motif co-occurrence\n",
    "motif_cooccurrence_count_matrix = get_motif_cooccurrence_count_matrix(peak_hit_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the matrix of p-values for motif co-occurrence\n",
    "motif_cooccurrence_pval_matrix = compute_cooccurrence_pvals(peak_hit_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hits_cache_dir:\n",
    "    # Save the filtered hits in the cache\n",
    "    hit_table_filtered.reset_index().to_csv(\n",
    "        os.path.join(hits_cache_dir, \"filtered_hits.tsv\"), sep=\"\\t\", header=True, index=False\n",
    "    )\n",
    "    \n",
    "    # Save the peaks\n",
    "    peak_table.reset_index().to_csv(\n",
    "        os.path.join(hits_cache_dir, \"peaks.tsv\"), sep=\"\\t\", header=True, index=False\n",
    "    )\n",
    "    \n",
    "    # Save a mapping between peak index and filtered motif indices\n",
    "    with open(os.path.join(hits_cache_dir, \"peak_matched_hits.tsv\"), \"w\") as f:\n",
    "        f.write(\"peak_index\\tfiltered_hit_indices\\n\")\n",
    "        for i, table in enumerate(peak_hits):\n",
    "            f.write(\"%d\\t%s\\n\" % (i, \",\".join([str(x) for x in peak_hits[i].index])))\n",
    "    \n",
    "    # Save score figures\n",
    "    if \"agg_sim\" in list(hit_table):\n",
    "        for motif_key, fig in filter_figs.items():\n",
    "            fig.savefig(os.path.join(hits_cache_dir, \"filter_dists_%s.png\" % motif_key))\n",
    "    else:\n",
    "        score_fig.savefig(os.path.join(hits_cache_dir, \"imp_score_dist.png\"))\n",
    "    \n",
    "    # Save co-occurrence matrices\n",
    "    with h5py.File(os.path.join(hits_cache_dir, \"cooccurrences.h5\"), \"w\") as f:\n",
    "        f.create_dataset(\"counts\", data=motif_cooccurrence_count_matrix, compression=\"gzip\")\n",
    "        f.create_dataset(\"pvals\", data=motif_cooccurrence_pval_matrix, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"peaks-with-hits\"></a>\n",
    "### Proportion of peaks with hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motifs_per_peak = np.array([len(hits) for hits in peak_hits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(vdomh.p(\"Number of peaks: %d\" % len(peak_table)))\n",
    "display(vdomh.p(\"Number of motif hits before FDR filtering: %d\" % len(hit_table)))\n",
    "display(vdomh.p(\"Number of motif hits after FDR filtering: %d\" % len(hit_table_filtered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zero = np.sum(motifs_per_peak == 0)\n",
    "display(vdomh.p(\"Number of peaks with 0 motif hits: %d\" % num_zero))\n",
    "display(vdomh.p(\"Percentage of peaks with 0 motif hits: %.1f%%\" % (num_zero / len(peak_table) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quants = [0, 0.25, 0.50, 0.75, 0.99, 1]\n",
    "header = vdomh.thead(\n",
    "    vdomh.tr(\n",
    "        vdomh.th(\"Quantile\", style={\"text-align\": \"center\"}),\n",
    "        vdomh.th(\"Number of hits/peak\", style={\"text-align\": \"center\"})\n",
    "    )\n",
    ")\n",
    "body = vdomh.tbody(*([\n",
    "    vdomh.tr(\n",
    "        vdomh.td(\"%.1f%%\" % (q * 100)), vdomh.td(\"%d\" % v)\n",
    "    ) for q, v in zip(quants, np.quantile(motifs_per_peak, quants))\n",
    "]))\n",
    "vdomh.table(header, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "bins = np.concatenate([np.arange(np.max(motifs_per_peak) + 1), [np.inf]])\n",
    "ax.hist(motifs_per_peak, bins=bins, density=True, histtype=\"step\", cumulative=True)\n",
    "ax.set_title(\"Cumulative distribution of number of motif hits per peak\")\n",
    "ax.set_xlabel(\"Number of motifs k in peak\")\n",
    "ax.set_ylabel(\"Proportion of peaks with at least k motifs\")\n",
    "plt.show()\n",
    "\n",
    "if hits_cache_dir:\n",
    "    fig.savefig(os.path.join(hits_cache_dir, \"peak_hit_count_cdf.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_peaks_with_motif = np.sum(peak_hit_counts > 0, axis=0) / len(peak_hit_counts)\n",
    "labels = np.array(motif_keys)\n",
    "sorted_inds = np.flip(np.argsort(frac_peaks_with_motif))\n",
    "frac_peaks_with_motif = frac_peaks_with_motif[sorted_inds]\n",
    "labels = labels[sorted_inds]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "ax.bar(np.arange(len(labels)), frac_peaks_with_motif)\n",
    "ax.set_title(\"Proportion of peaks with each motif\")\n",
    "ax.set_xticks(np.arange(len(labels)))\n",
    "ax.set_xticklabels(labels)\n",
    "plt.show()\n",
    "\n",
    "if hits_cache_dir:\n",
    "    fig.savefig(os.path.join(hits_cache_dir, \"peaks_with_each_motif.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-hits\"></a>\n",
    "### Examples of motif hits in sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show some examples of sequences with motif hits\n",
    "num_to_draw = 3\n",
    "center_plot_size = 400\n",
    "\n",
    "unique_counts = np.sort(np.unique(motifs_per_peak))\n",
    "motif_nums = []\n",
    "if 0 in motifs_per_peak:\n",
    "    motif_nums.append(0)\n",
    "if 1 in motifs_per_peak:\n",
    "    motif_nums.append(1)\n",
    "motif_nums.extend([\n",
    "    unique_counts[0],  # Minimum\n",
    "    unique_counts[len(unique_counts) // 2],  # Median\n",
    "    unique_counts[-1],  # Maximum\n",
    "])\n",
    "\n",
    "for motif_num in np.sort(np.unique(motif_nums)):\n",
    "    display(vdomh.h4(\"Sequences with %d motif hits\" % motif_num))\n",
    "    \n",
    "    peak_inds = np.where(motifs_per_peak == motif_num)[0]\n",
    "    table_rows = []\n",
    "    rng = np.random.RandomState(seed)\n",
    "    for i in rng.choice(\n",
    "        peak_inds, size=min(num_to_draw, len(peak_inds)), replace=False\n",
    "    ):\n",
    "        peak_coord = peak_table.iloc[i][[\"chrom\", \"peak_start\", \"peak_end\"]].values\n",
    "        motif_hits = peak_hits[i]\n",
    "        \n",
    "        chrom, peak_start, peak_end = peak_coord\n",
    "        \n",
    "        # Limit peak start/end here\n",
    "        mid = (peak_start + peak_end) // 2\n",
    "        peak_start = mid - (center_plot_size // 2)\n",
    "        peak_end = peak_start + center_plot_size\n",
    "        \n",
    "        peak_len = peak_end - peak_start\n",
    "        mask = (shap_coords[:, 0] == chrom) & (shap_coords[:, 1] <= peak_start) & (shap_coords[:, 2] >= peak_end)\n",
    "        if not np.sum(mask):\n",
    "            fig = \"No matching input sequence found\"\n",
    "            table_rows.append(\n",
    "                vdomh.tr(\n",
    "                    vdomh.td(\"%s:%d-%d\" % (chrom, peak_start, peak_end)),\n",
    "                    vdomh.td(fig)\n",
    "                )\n",
    "            )\n",
    "            continue\n",
    "            \n",
    "        seq_index = np.where(mask)[0][0]  # Pick one\n",
    "        imp_scores = act_scores[seq_index]\n",
    "        _, seq_start, seq_end = shap_coords[seq_index]\n",
    "        \n",
    "        highlights = []\n",
    "        for _, row in motif_hits.iterrows():\n",
    "            start = row[\"start\"] - peak_start\n",
    "            end = start + (row[\"end\"] - row[\"start\"])\n",
    "            highlights.append((start, end))\n",
    "        \n",
    "        # Remove highlights that overrun the sequence\n",
    "        highlights = [(a, b) for a, b in highlights if a >= 0 and b < peak_len]\n",
    "        \n",
    "        start = peak_start - seq_start \n",
    "        end = start + peak_len\n",
    "        imp_scores_peak = imp_scores[start:end]\n",
    "        \n",
    "        fig = viz_sequence.plot_weights(\n",
    "            imp_scores_peak, subticks_frequency=(len(imp_scores_peak) + 1),\n",
    "            highlight={\"red\" : [pair for pair in highlights]},\n",
    "            return_fig=True\n",
    "        )\n",
    "        fig = figure_to_vdom_image(fig)\n",
    "        \n",
    "        table_rows.append(\n",
    "            vdomh.tr(\n",
    "                vdomh.td(\"%s:%d-%d\" % (chrom, peak_start, peak_end)),\n",
    "                vdomh.td(fig)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    table = vdomh.table(*table_rows)\n",
    "    display(table)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"density\"></a>\n",
    "### Homotypic motif densities\n",
    "For each motif, show how many the motif occurs in each peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "density_figs = plot_homotypic_densities(peak_hit_counts, motif_keys)\n",
    "\n",
    "if hits_cache_dir:\n",
    "    for key, fig in density_figs.items():\n",
    "        fig.savefig(os.path.join(hits_cache_dir, \"homotypic_density_%s.png\" % key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"co-occurrence\"></a>\n",
    "### Co-occurrence of motifs\n",
    "Proportion of time that motifs co-occur with each other in peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_peak_motif_indicator_heatmap(peak_hit_counts, motif_keys, subsample=10000)\n",
    "\n",
    "if hits_cache_dir:\n",
    "    fig.savefig(os.path.join(hits_cache_dir, \"peak_motif_indicator_heatmap.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_fig, c_fig, cluster_inds = plot_motif_cooccurrence_heatmaps(\n",
    "    motif_cooccurrence_count_matrix, motif_cooccurrence_pval_matrix, motif_keys\n",
    ")\n",
    "\n",
    "if hits_cache_dir:\n",
    "    p_fig.savefig(os.path.join(hits_cache_dir, \"cooccurrence_pvals.png\"))\n",
    "    c_fig.savefig(os.path.join(hits_cache_dir, \"cooccurrence_counts.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"distance\"></a>\n",
    "### Distribution of distances between motifs\n",
    "When motifs co-occur, show the distance between the instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get which pairs of motifs are significant\n",
    "sig_thresh = 1e-6\n",
    "count_thresh = 100\n",
    "\n",
    "pvals, sig_pairs = [], []\n",
    "for i in range(len(motif_keys)):\n",
    "    for j in range(i + 1):\n",
    "        if motif_cooccurrence_pval_matrix[i, j] < sig_thresh and motif_cooccurrence_count_matrix[i, j] >= count_thresh:\n",
    "            sig_pairs.append((i, j))\n",
    "            pvals.append(motif_cooccurrence_pval_matrix[i, j])\n",
    "inds = np.argsort(pvals)\n",
    "sig_pairs = [sig_pairs[i] for i in inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_dict, fig = plot_intermotif_distance_violins(peak_hits, motif_keys, sig_pairs, cluster_inds)\n",
    "\n",
    "if hits_cache_dir:\n",
    "    if fig is not None:\n",
    "        with h5py.File(os.path.join(hits_cache_dir, \"intermotif_dists.h5\"), \"w\") as f:\n",
    "            for key_pair, dists in distance_dict.items():\n",
    "                f.create_dataset(\"%s:%s\" % key_pair, data=dists, compression=\"gzip\")\n",
    "        fig.savefig(os.path.join(hits_cache_dir, \"intermotif_dists.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
