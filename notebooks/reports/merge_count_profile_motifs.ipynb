{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to results\n",
    "[Results](#results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/users/amtseng/tfmodisco/src/\"))\n",
    "from util import figure_to_vdom_image\n",
    "import plot.viz_sequence as viz_sequence\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import vdom.helpers as vdomh\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters/fetch arguments\n",
    "in_motif_file = os.environ[\"TFM_IN_MOTIF_FILE\"]\n",
    "out_motif_file = os.environ[\"TFM_OUT_MOTIF_FILE\"]\n",
    "\n",
    "print(\"Input motif file: %s\" % in_motif_file)\n",
    "print(\"Output motif file: %s\" % out_motif_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_match_sim = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(out_motif_file), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_motifs_from_hdf5_group(h5_group):\n",
    "    \"\"\"\n",
    "    Imports a set of motifs from an open HDF5 group.\n",
    "    The HDF5 group must be structured as follows:\n",
    "        count:\n",
    "            0_0:\n",
    "                cwm_trimmed\n",
    "                ...\n",
    "        profile:\n",
    "            0_0:\n",
    "                cwm_trimmed\n",
    "                ...\n",
    "    Returns a dictionary of the trimmed CWMs matching the\n",
    "    structure of the group.\n",
    "    Motifs are flipped to the purine-rich version.\n",
    "    \"\"\"\n",
    "    motifs = {}\n",
    "    for key in (\"count\", \"profile\"):\n",
    "        motifs[key] = {}\n",
    "        for motif_key in h5_group[key]:\n",
    "            cwm = h5_group[key][motif_key][\"cwm_trimmed\"][:]\n",
    "            # Flip motif to be the purine-rich version\n",
    "            if np.sum(cwm[:, [0, 2]]) < 0.5 * np.sum(cwm):\n",
    "                cwm = np.flip(cwm)\n",
    "            motifs[key][motif_key] = cwm\n",
    "        \n",
    "    return motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_merged_motifs_to_hdf5_group(in_h5_group, out_h5_group, matched_keys):\n",
    "    \"\"\"\n",
    "    Writes a set of motifs from an open HDF5 group.\n",
    "    The input HDF5 group must be structured as follows:\n",
    "        count:\n",
    "            0_0:\n",
    "                cwm_trimmed\n",
    "                ...\n",
    "        profile:\n",
    "            0_0:\n",
    "                cwm_trimmed\n",
    "                ...\n",
    "    The output HDF5 group will be stuctured as follows:\n",
    "        C0_0:P0_1:\n",
    "            cwm_trimmed\n",
    "            ...\n",
    "    `matched_keys` is a list of pairs of keys, denoting count\n",
    "    and profile keys that are matched together, respectively.\n",
    "    Motifs are flipped to the purine-rich version.\n",
    "    \"\"\"\n",
    "    count_keys = list(in_h5_group[\"count\"].keys())\n",
    "    profile_keys = list(in_h5_group[\"profile\"].keys())\n",
    "\n",
    "    matched_count, matched_profile = (list(zip(*matched_keys))) if matched_keys else ([], [])\n",
    "    \n",
    "    unmatched_count = [key for key in count_keys if key not in matched_count]\n",
    "    unmatched_profile = [key for key in profile_keys if key not in matched_profile]\n",
    "    \n",
    "    for count_key, profile_key in matched_keys:\n",
    "        group = out_h5_group.create_group(\"C%s:P%s\" % (count_key, profile_key))\n",
    "        for motif_type in [\"pfm_full\", \"cwm_full\", \"hcwm_full\", \"pfm_trimmed\", \"cwm_trimmed\", \"hcwm_trimmed\"]:\n",
    "            agg = aggregate_motifs([\n",
    "                in_h5_group[\"count\"][count_key][motif_type][:],\n",
    "                in_h5_group[\"profile\"][profile_key][motif_type][:]\n",
    "            ])\n",
    "            group.create_dataset(motif_type, data=agg, compression=\"gzip\")\n",
    "    \n",
    "    for count_key in unmatched_count:\n",
    "        group = out_h5_group.create_group(\"C%s\" % count_key)\n",
    "        for motif_type in [\"pfm_full\", \"cwm_full\", \"hcwm_full\", \"pfm_trimmed\", \"cwm_trimmed\", \"hcwm_trimmed\"]:\n",
    "            motif = in_h5_group[\"count\"][count_key][motif_type][:]\n",
    "            group.create_dataset(motif_type, data=motif, compression=\"gzip\")\n",
    "    for profile_key in unmatched_profile:\n",
    "        group = out_h5_group.create_group(\"P%s\" % profile_key)\n",
    "        for motif_type in [\"pfm_full\", \"cwm_full\", \"hcwm_full\", \"pfm_trimmed\", \"cwm_trimmed\", \"hcwm_trimmed\"]:\n",
    "            motif = in_h5_group[\"profile\"][profile_key][motif_type][:]\n",
    "            group.create_dataset(motif_type, data=motif, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_similarity_score(motif_1, motif_2, average=True, align_to_longer=True):\n",
    "    \"\"\"\n",
    "    Computes the motif similarity score between two motifs by\n",
    "    the summed cosine similarity, maximized over all possible sliding\n",
    "    windows. Also returns the index relative to the start of `motif_2`\n",
    "    where `motif_1` should be placed to maximize this score.\n",
    "    If `average` is True, then use average of similarity of overlap.\n",
    "    If `align_to_longer` is True, always use the longer motif as the basis\n",
    "    for the index computation (if tie use `motif_2`). Otherwise, always use\n",
    "    `motif_2`.\n",
    "    \"\"\"\n",
    "    # L2-normalize\n",
    "    motif_1 = motif_1 - np.mean(motif_1, axis=1, keepdims=True)\n",
    "    motif_2 = motif_2 - np.mean(motif_2, axis=1, keepdims=True)\n",
    "    motif_1 = motif_1 / np.sqrt(np.sum(motif_1 * motif_1, axis=1, keepdims=True))\n",
    "    motif_2 = motif_2 / np.sqrt(np.sum(motif_2 * motif_2, axis=1, keepdims=True))\n",
    "    \n",
    "    # Mean-normalize\n",
    "    motif_1 = motif_1 - np.mean(motif_1, axis=1, keepdims=True)\n",
    "    motif_2 = motif_2 - np.mean(motif_2, axis=1, keepdims=True)\n",
    "    \n",
    "    # Always make motif_2 longer\n",
    "    if align_to_longer and len(motif_1) > len(motif_2):\n",
    "        motif_1, motif_2 = motif_2, motif_1\n",
    "    \n",
    "    # Pad motif_2 by len(motif_1) - 1 on either side\n",
    "    orig_motif_2_len = len(motif_2)\n",
    "    pad_size = len(motif_1) - 1\n",
    "    motif_2 = np.pad(motif_2, ((pad_size, pad_size), (0, 0)))\n",
    "    \n",
    "    if average:\n",
    "        # Compute overlap sizes\n",
    "        overlap_sizes = np.empty(orig_motif_2_len + pad_size)\n",
    "        overlap_sizes[:pad_size] = np.arange(1, len(motif_1))\n",
    "        overlap_sizes[-pad_size:] = np.flip(np.arange(1, len(motif_1)))\n",
    "        overlap_sizes[pad_size:-pad_size] = len(motif_1)\n",
    "    \n",
    "    # Compute similarities across all sliding windows\n",
    "    scores = np.empty(orig_motif_2_len + pad_size)\n",
    "    for i in range(orig_motif_2_len + pad_size):\n",
    "        scores[i] = np.sum(motif_1 * motif_2[i : i + len(motif_1)])\n",
    "        \n",
    "    best_ind = np.argmax(scores)\n",
    "    if average:\n",
    "        scores = scores / overlap_sizes\n",
    "    return scores[best_ind], best_ind - pad_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_motif_pairs(motifs_a, motifs_b):\n",
    "    \"\"\"\n",
    "    For a list of motifs `motifs_a` and another list of motifs `motifs_b`,\n",
    "    computes the similarity between all pairs and pairs up the closest\n",
    "    motifs between A and B.\n",
    "    This is done greedily, where the two motifs with the biggest similarity\n",
    "    in the matrix are paired up, and then removed from the matrix.\n",
    "    Returns a list of triplets (A_i, B_i, s_i), where A_i and B_i are the\n",
    "    indices of motifs in `motifs_a` and `motifs_b` that have been matched\n",
    "    up, and `s_i` is the similarity between them. The triplets are ordered\n",
    "    by decreasing `s_i`. Note that if the lengths of the lists are not the\n",
    "    same, then some motifs will not be matched up.\n",
    "    \"\"\"\n",
    "    num_a, num_b = len(motifs_a), len(motifs_b)\n",
    "    \n",
    "    # Compute the similarity matrix\n",
    "    sim_matrix = np.empty((num_a, num_b))\n",
    "    for a_i in range(num_a):\n",
    "        for b_i in range(num_b):\n",
    "            sim, _ = motif_similarity_score(motifs_a[a_i], motifs_b[b_i])\n",
    "            sim_matrix[a_i, b_i] = sim\n",
    "    \n",
    "    a_inds, b_inds = np.arange(num_a), np.arange(num_b)\n",
    "    matches = []\n",
    "    for _ in range(min(num_a, num_b)):\n",
    "        a_i, b_i = np.unravel_index(np.argmax(sim_matrix), sim_matrix.shape)\n",
    "        sim = sim_matrix[a_i, b_i]\n",
    "        \n",
    "        matches.append((a_inds[a_i], b_inds[b_i], sim))\n",
    "        \n",
    "        a_inds = np.delete(a_inds, a_i)\n",
    "        b_inds = np.delete(b_inds, b_i)\n",
    "        sim_matrix = np.delete(np.delete(sim_matrix, a_i, axis=0), b_i, axis=1)\n",
    "        \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_motifs(motifs):\n",
    "    \"\"\"\n",
    "    Aggregates a list of L x 4 (not all the same L) motifs into a single\n",
    "    L x 4 motif.\n",
    "    \"\"\"\n",
    "    num_motifs = len(motifs)\n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = np.empty((num_motifs, num_motifs))\n",
    "    for i in range(num_motifs):\n",
    "        for j in range(i, num_motifs):\n",
    "            sim, _ = motif_similarity_score(motifs[i], motifs[j])\n",
    "            sim_matrix[i, j] = sim\n",
    "            sim_matrix[j, i] = sim\n",
    "\n",
    "    # Sort motifs by how similar it is to everyone else\n",
    "    inds = np.flip(np.argsort(np.sum(sim_matrix, axis=0)))\n",
    "    \n",
    "    # Have the consensus start with the most similar\n",
    "    consensus = np.zeros_like(motifs[inds[0]])\n",
    "    consensus = consensus + motifs[inds[0]]\n",
    "    \n",
    "    # For each successive motif, add it into the consensus\n",
    "    for i in inds[1:]:\n",
    "        motif = motifs[i]\n",
    "        _, index = motif_similarity_score(motif, consensus, align_to_longer=False)\n",
    "        if index >= 0:\n",
    "            start, end = index, index + len(motif)\n",
    "            consensus[start:end] = consensus[start:end] + motif[:len(consensus) - index]\n",
    "        else:\n",
    "            end = len(motif) + index\n",
    "            consensus[:end] = consensus[:end] + motif[-index:-index + len(consensus)]\n",
    "    return consensus / len(motifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches(count_keys, count_motifs, profile_keys, profile_motifs, matches):\n",
    "    \"\"\"\n",
    "    Show matched and leftover motifs.\n",
    "    \"\"\"\n",
    "    display(vdomh.h4(\"Matched motifs\"))\n",
    "    colgroup = vdomh.colgroup(\n",
    "        vdomh.col(style={\"width\": \"5\"}),\n",
    "        vdomh.col(style={\"width\": \"5\"}),\n",
    "        vdomh.col(style={\"width\": \"45%\"}),\n",
    "        vdomh.col(style={\"width\": \"45%\"}),\n",
    "    )\n",
    "    header = vdomh.thead(\n",
    "        vdomh.tr(\n",
    "            vdomh.th(\"Merged ID\", style={\"text-align\": \"center\"}),\n",
    "            vdomh.th(\"Similarity\", style={\"text-align\": \"center\"}),\n",
    "            vdomh.th(\"Aggregate motif\", style={\"text-align\": \"center\"}),\n",
    "            vdomh.th(\"Constituent motifs\", style={\"text-align\": \"center\"})\n",
    "        )\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for count_i, profile_i, sim in matches:\n",
    "        constituents = [count_motifs[count_i], profile_motifs[profile_i]]\n",
    "        consensus = aggregate_motifs([count_motifs[count_i], profile_motifs[profile_i]])\n",
    "\n",
    "        merged_id = \"C%s:P%s\" % (count_keys[count_i], profile_keys[profile_i])\n",
    "\n",
    "        agg_fig = viz_sequence.plot_weights(consensus, figsize=(20, 4), return_fig=True)\n",
    "        agg_fig.tight_layout()\n",
    "        const_figs = []\n",
    "        for motif in constituents:\n",
    "            fig = viz_sequence.plot_weights(motif, figsize=(20, 4), return_fig=True)\n",
    "            fig.tight_layout()\n",
    "            const_figs.append(figure_to_vdom_image(fig))\n",
    "\n",
    "        rows.append(vdomh.tr(\n",
    "            vdomh.td(merged_id),\n",
    "            vdomh.td(\"%.4f\" % sim),\n",
    "            vdomh.td(figure_to_vdom_image(agg_fig)),\n",
    "            vdomh.td(*const_figs)\n",
    "        ))\n",
    "    display(vdomh.table(colgroup, header, vdomh.tbody(*rows)))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    display(vdomh.h4(\"Unmatched motifs\"))\n",
    "    colgroup = vdomh.colgroup(\n",
    "        vdomh.col(style={\"width\": \"5%\"}),\n",
    "        vdomh.col(style={\"width\": \"45%\"}),\n",
    "        vdomh.col(style={\"width\": \"5%\"}),\n",
    "        vdomh.col(style={\"width\": \"45%\"}),\n",
    "    )\n",
    "    header = vdomh.thead(\n",
    "        vdomh.tr(\n",
    "            vdomh.th(\"Count ID\", style={\"text-align\": \"center\"}),\n",
    "            vdomh.th(\"Count motif\", style={\"text-align\": \"center\"}),\n",
    "            vdomh.th(\"Profile ID\", style={\"text-align\": \"center\"}),\n",
    "            vdomh.th(\"Profile motif\", style={\"text-align\": \"center\"})\n",
    "        )\n",
    "    )\n",
    "\n",
    "    matched_count = [trip[0] for trip in matches]\n",
    "    unmatched_count = [i for i in range(len(count_motifs)) if i not in matched_count]\n",
    "    matched_profile = [trip[1] for trip in matches]\n",
    "    unmatched_profile = [i for i in range(len(profile_motifs)) if i not in matched_profile]\n",
    "    rows = []\n",
    "    for i in range(max(len(unmatched_count), len(unmatched_profile))):\n",
    "        row = []\n",
    "        if i < len(unmatched_count):\n",
    "            fig = viz_sequence.plot_weights(\n",
    "                count_motifs[unmatched_count[i]], figsize=(20, 4), return_fig=True\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "            row.extend([\n",
    "                vdomh.td(\"C%s\" % count_keys[unmatched_count[i]]),\n",
    "                vdomh.td(figure_to_vdom_image(fig))\n",
    "            ])\n",
    "        else:\n",
    "            row.extend([vdomh.td(), vdomh.td()])\n",
    "        if i < len(unmatched_profile):\n",
    "            fig = viz_sequence.plot_weights(\n",
    "                profile_motifs[unmatched_profile[i]], figsize=(20, 4), return_fig=True\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "            row.extend([\n",
    "                vdomh.td(\"P%s\" % profile_keys[unmatched_profile[i]]),\n",
    "                vdomh.td(figure_to_vdom_image(fig))\n",
    "            ])\n",
    "        else:\n",
    "            row.extend([vdomh.td(), vdomh.td()])\n",
    "        rows.append(vdomh.tr(*row))\n",
    "    display(vdomh.table(colgroup, header, vdomh.tbody(*rows)))\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_key_sorter = lambda k: (int(k.split(\"_\")[0]), int(k.split(\"_\")[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidate count/profile head motifs for all conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(in_h5_group, out_h5_group):\n",
    "    \"\"\"\n",
    "    Performs consolidation, reading motifs from the in group and\n",
    "    writing to the out group.\n",
    "    \"\"\"\n",
    "    # Import CWMs\n",
    "    motifs = import_motifs_from_hdf5_group(in_h5_group)\n",
    "    count_keys = sorted(motifs[\"count\"].keys(), key=motif_key_sorter)\n",
    "    profile_keys = sorted(motifs[\"profile\"].keys(), key=motif_key_sorter)\n",
    "    count_motifs = [motifs[\"count\"][k] for k in count_keys]\n",
    "    profile_motifs = [motifs[\"profile\"][k] for k in profile_keys]\n",
    "    matches = compute_motif_pairs(\n",
    "        count_motifs, profile_motifs\n",
    "    )\n",
    "\n",
    "    # Plot distribution of match similarities\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.hist([trip[2] for trip in matches], bins=10)\n",
    "    ax.set_title(\"Histogram of match distances\")\n",
    "    ax.set_xlabel(\"Similarity\")\n",
    "    plt.show()\n",
    "\n",
    "    # Restrict to matches of sufficient similarity\n",
    "    matches = [match for match in matches if match[2] >= min_match_sim]\n",
    "\n",
    "    # Plot matches themselves\n",
    "    plot_matches(count_keys, count_motifs, profile_keys, profile_motifs, matches)\n",
    "\n",
    "    # Write the consolidated motifs and unconsolidated motifs\n",
    "    matched_keys = [(count_keys[trip[0]], profile_keys[trip[1]]) for trip in matches]\n",
    "    write_merged_motifs_to_hdf5_group(in_h5_group, out_h5_group, matched_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with h5py.File(in_motif_file, \"r\") as f, h5py.File(out_motif_file, \"w\") as g:\n",
    "    # Multi-task, all 10 folds\n",
    "    f_mt = f[\"multitask\"]\n",
    "    g_mt = g.create_group(\"multitask\")\n",
    "    for fold in f_mt.keys():\n",
    "        display(vdomh.h3(\"Multi-task %s\" % fold))\n",
    "        \n",
    "        f_mt_fold = f_mt[fold]\n",
    "        g_mt_fold = g_mt.create_group(fold)\n",
    "        \n",
    "        consolidate(f_mt_fold, g_mt_fold)\n",
    "        \n",
    "    f_st = f[\"singletask\"]\n",
    "    g_st = g.create_group(\"singletask\")\n",
    "    \n",
    "    # Single-task, all 10 folds for all tasks\n",
    "    for task in f_st.keys():\n",
    "        f_st_task = f_st[task]\n",
    "        g_st_task = g_st.create_group(task)\n",
    "        \n",
    "        for fold in f_st_task.keys():\n",
    "            display(vdomh.h3(\"Single-task %s %s\" % (task, fold)))\n",
    "\n",
    "            f_st_task_fold = f_st_task[fold]\n",
    "            g_st_task_fold = g_st_task.create_group(fold)\n",
    "\n",
    "            consolidate(f_st_task_fold, g_st_task_fold)\n",
    "            \n",
    "    # Multi-task fine-tuned, all tasks\n",
    "    f_mtft = f[\"multitask_finetune\"]\n",
    "    g_mtft = g.create_group(\"multitask_finetune\")\n",
    "    \n",
    "    for task in f_mtft.keys():\n",
    "        display(vdomh.h3(\"Multi-task fine-tune %s\" % task))\n",
    "        f_mtft_task = f_mtft[task]\n",
    "        g_mtft_task = g_mtft.create_group(task)\n",
    "\n",
    "        consolidate(f_mtft_task, g_mtft_task)\n",
    "        \n",
    "    # Single-task fine-tuned, all tasks\n",
    "    f_stft = f[\"singletask_finetune\"]\n",
    "    g_stft = g.create_group(\"singletask_finetune\")\n",
    "    \n",
    "    for task in f_stft.keys():\n",
    "        display(vdomh.h3(\"Single-task fine-tune %s\" % task))\n",
    "        f_stft_task = f_stft[task]\n",
    "        g_stft_task = g_stft.create_group(task)\n",
    "\n",
    "        consolidate(f_stft_task, g_stft_task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
