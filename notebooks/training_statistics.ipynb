{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_metrics_json(models_path, run_num):\n",
    "    \"\"\"\n",
    "    Looks in {models_path}/{run_num}/metrics.json and returns the contents as a\n",
    "    Python dictionary. Returns None if the path does not exist.\n",
    "    \"\"\"\n",
    "    path = os.path.join(models_path, str(run_num), \"metrics.json\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_metric(models_path, metric_extract_func, metric_compare_func):\n",
    "    \"\"\"\n",
    "    Given the path to a set of runs, determines the run with the best metric value,\n",
    "    where the metric value is fetched by `metric_extract_func`. This function must\n",
    "    take the imported metrics JSON and return the (scalar) value to use for\n",
    "    comparison. The best metric value is determiend by `metric_compare_func`, which\n",
    "    must take in two arguments, and return whether or not the _first_ one is better.\n",
    "    Returns the number of the run, the value associated with that run, and a list of\n",
    "    all the values used for comparison.\n",
    "    \"\"\"\n",
    "    # Get the metrics, ignoring empty or nonexistent metrics.json files\n",
    "    metrics = {run_num : import_metrics_json(models_path, run_num) for run_num in os.listdir(models_path)}\n",
    "    metrics = {key : val for key, val in metrics.items() if val}  # Remove empties\n",
    "    \n",
    "    # Get the best value\n",
    "    best_run, best_val, all_vals = None, None, {}\n",
    "    for run_num in metrics.keys():\n",
    "        try:\n",
    "            val = metric_extract_func(metrics[run_num])\n",
    "        except Exception:\n",
    "            print(\"Warning: Was not able to extract metric for run %s\" % run_num)\n",
    "            continue\n",
    "        all_vals[run_num] = val\n",
    "        if best_val is None or metric_compare_func(val, best_val):\n",
    "            best_val, best_run = val, run_num\n",
    "    return best_run, best_val, all_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Was not able to extract metric for run 62\n",
      "Best run: 24\n",
      "Associated value: 316.1255480083218\n",
      "1: 319\n",
      "2: 317\n",
      "3: 375\n",
      "4: 318\n",
      "5: 375\n",
      "6: 335\n",
      "7: 317\n",
      "8: 353\n",
      "9: 375\n",
      "10: 375\n",
      "11: 375\n",
      "12: 318\n",
      "13: 375\n",
      "14: 339\n",
      "15: 317\n",
      "16: 375\n",
      "17: 317\n",
      "18: 317\n",
      "19: 375\n",
      "20: 320\n",
      "21: 322\n",
      "22: 317\n",
      "23: 327\n",
      "24: 316\n",
      "25: 318\n",
      "26: 357\n",
      "27: 316\n",
      "28: 375\n",
      "29: 347\n",
      "30: 320\n",
      "31: 375\n",
      "32: 352\n",
      "33: 317\n",
      "34: 375\n",
      "35: 375\n",
      "36: 375\n",
      "37: 375\n",
      "38: 321\n",
      "39: 375\n",
      "40: 316\n",
      "41: 375\n",
      "42: 320\n",
      "43: 375\n",
      "44: 375\n",
      "45: 318\n",
      "46: 375\n",
      "47: 319\n",
      "48: 375\n",
      "49: 375\n",
      "50: 316\n",
      "51: 319\n",
      "52: 339\n",
      "53: 325\n",
      "54: 323\n",
      "55: 340\n",
      "56: 355\n",
      "57: 318\n",
      "58: 318\n",
      "59: 375\n",
      "60: 340\n",
      "61: 319\n",
      "63: 320\n",
      "64: 329\n",
      "65: 318\n",
      "66: 324\n",
      "67: 317\n",
      "68: 318\n",
      "69: 317\n",
      "70: 319\n",
      "71: 317\n",
      "72: 375\n",
      "73: 345\n",
      "74: 318\n",
      "75: 318\n",
      "76: 345\n",
      "77: 375\n",
      "78: 346\n",
      "79: 317\n",
      "80: 318\n",
      "81: 339\n",
      "82: 375\n",
      "83: 344\n",
      "84: 375\n",
      "85: 375\n",
      "86: 316\n",
      "87: 375\n",
      "88: 375\n",
      "89: 317\n",
      "90: 375\n",
      "91: 316\n",
      "92: 336\n",
      "93: 319\n",
      "94: 375\n",
      "95: 318\n",
      "96: 342\n",
      "97: 375\n",
      "98: 321\n",
      "99: 375\n",
      "100: 326\n",
      "101: 330\n",
      "102: 329\n",
      "103: 375\n",
      "104: 319\n",
      "105: 375\n",
      "106: 375\n",
      "107: 375\n",
      "108: 375\n",
      "109: 375\n",
      "110: 318\n",
      "111: 346\n",
      "112: 318\n"
     ]
    }
   ],
   "source": [
    "models_path = \"/users/amtseng/tfmodisco/models/trained_models/E2F6/\"\n",
    "best_run, best_val, all_vals = get_best_metric(\n",
    "    models_path,\n",
    "    lambda metrics: np.mean(metrics[\"summit_prof_nll\"][\"values\"][0]),\n",
    "    lambda x, y: x < y\n",
    ")\n",
    "print(\"Best run: %s\" % best_run)\n",
    "print(\"Associated value: %s\" % best_val)\n",
    "for run in sorted(all_vals.keys(), key=lambda x: int(x)):\n",
    "    print(\"%s: %d\" % (run, all_vals[run]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
