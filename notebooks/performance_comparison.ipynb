{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct links to results\n",
    "[Multi-task profile model performance across all 10 folds](#multitask-fold)\n",
    "\n",
    "[Single-task profile model performance across all 10 folds](#singletask-fold)\n",
    "\n",
    "[Fine-tuned multi-task profile model task-specific performance](#finetune-multitask-task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "tqdm.tqdm_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "font_manager.fontManager.ttflist.extend(\n",
    "    font_manager.createFontList(\n",
    "        font_manager.findSystemFonts(fontpaths=\"/users/amtseng/modules/fonts\")\n",
    "    )\n",
    ")\n",
    "plot_params = {\n",
    "    \"figure.titlesize\": 22,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"xtick.labelsize\": 16,\n",
    "    \"ytick.labelsize\": 16,\n",
    "    \"font.family\": \"Roboto\",\n",
    "    \"font.weight\": \"bold\"\n",
    "}\n",
    "plt.rcParams.update(plot_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters/fetch arguments\n",
    "tf_name = os.environ[\"TFM_RESULTS_TF_NAME\"]\n",
    "num_tasks = int(os.environ[\"TFM_RESULTS_NUM_TASKS\"])\n",
    "best_multitask_fold = int(os.environ[\"TFM_RESULTS_BEST_MULTITASK_FOLD\"])\n",
    "best_singletask_folds = [int(x) for x in os.environ[\"TFM_RESULTS_BEST_SINGLETASK_FOLDS\"].split(\",\")]\n",
    "    \n",
    "print(\"TF name: %s\" % tf_name)\n",
    "print(\"Number of tasks: %d\" % num_tasks)\n",
    "print(\"Best multi-task fold: %d\" % best_multitask_fold)\n",
    "print(\"Best single-task folds: %s\" % \" \".join([str(x) for x in best_singletask_folds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_base = \"/users/amtseng/tfmodisco/results/peak_predictions/\"\n",
    "\n",
    "# Paths for all 10 folds for multi-task profile models\n",
    "multitask_preds_paths = [\n",
    "    os.path.join(\n",
    "        preds_base,\n",
    "        \"multitask_profile\",\n",
    "        \"{0}_multitask_profile_fold{1}/{0}_multitask_profile_fold{1}_pred_perf.h5\".format(tf_name, fold)\n",
    "    ) for fold in range(1, 11)\n",
    "]\n",
    "\n",
    "# Paths for all 10 folds for single-task profile models, for each task\n",
    "singletask_preds_paths = [\n",
    "    [\n",
    "        os.path.join(\n",
    "            preds_base,\n",
    "            \"singletask_profile\",\n",
    "            \"{0}_singletask_profile_fold{1}/task_{2}/{0}_singletask_profile_task{2}_fold{1}_pred_perf.h5\".format(\n",
    "                tf_name, fold, task_index\n",
    "            )\n",
    "        ) for fold in range(1, 11)\n",
    "    ] for task_index in range(num_tasks)\n",
    "]\n",
    "\n",
    "# Path for fine-tuned multi-task profile model\n",
    "multitask_finetuned_preds_path = os.path.join(\n",
    "    preds_base,\n",
    "    \"multitask_profile_finetune\",\n",
    "    \"{0}_multitask_profile_finetune_fold{1}/{0}_multitask_profile_finetune_fold{1}_pred_perf.h5\".format(\n",
    "        tf_name, best_multitask_fold\n",
    "    )\n",
    ")\n",
    "\n",
    "# Paths for fine-tuned single-task profile models, for each task\n",
    "singletask_finetuned_preds_paths = [\n",
    "    os.path.join(\n",
    "        preds_base,\n",
    "        \"singletask_profile_finetune\",\n",
    "        \"{0}_singletask_profile_finetune_fold{1}/task_{2}/{0}_singletask_profile_finetune_task{2}_fold{1}_pred_perf.h5\".format(\n",
    "            tf_name, fold, task_index\n",
    "        )\n",
    "    ) for task_index, fold in enumerate(best_singletask_folds)\n",
    "]\n",
    "\n",
    "# Path for upper-bound and lower-bound performance metrics\n",
    "perf_bounds_path = \"/users/amtseng/tfmodisco/results/performance_bounds/{0}_performance_bounds.h5\".format(tf_name)\n",
    "\n",
    "# File specifications (including peak files)\n",
    "files_spec_json = \"/users/amtseng/tfmodisco/data/processed/ENCODE/config/{0}/{0}_training_paths.json\".format(tf_name)\n",
    "with open(files_spec_json, \"r\") as f:\n",
    "    files_spec = json.load(f)\n",
    "\n",
    "# Chromosome split definition (i.e. test set chromosomes)\n",
    "chrom_splits_json = \"/users/amtseng/tfmodisco/data/processed/ENCODE/chrom_splits.json\"\n",
    "with open(chrom_splits_json, \"r\") as f:\n",
    "    chrom_splits = json.load(f)\n",
    "all_fold_test_chroms = [\n",
    "    chrom_splits[str(fold)][\"test\"] for fold in range(1, 11)\n",
    "]\n",
    "best_multitask_fold_test_chroms = chrom_splits[str(best_multitask_fold)][\"test\"]\n",
    "best_singletask_fold_test_chroms = [chrom_splits[str(fold)][\"test\"] for fold in best_singletask_folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 2114\n",
    "profile_length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import peak coordinates for each task\n",
    "For each task, import the set of peaks belonging to that task. This allows us to get a set of indices for coordinates in the saved predictions/performance files that correspond to each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in tables containing the peak coordinates, padded to `input_length`\n",
    "task_coords = []\n",
    "assert len(files_spec[\"peak_beds\"]) == num_tasks\n",
    "for peak_bed_path in files_spec[\"peak_beds\"]:\n",
    "    table = pd.read_csv(\n",
    "        peak_bed_path, sep=\"\\t\", header=None,  # Infer compression\n",
    "        names=[\n",
    "            \"chrom\", \"peak_start\", \"peak_end\", \"name\", \"score\",\n",
    "            \"strand\", \"signal\", \"pval\", \"qval\", \"summit_offset\"\n",
    "        ]\n",
    "    )\n",
    "    # Add summit location column:\n",
    "    table[\"summit\"] = table[\"peak_start\"] + table[\"summit_offset\"]\n",
    "    \n",
    "    # Add start and end columns, at proper length\n",
    "    table[\"start\"] = table[\"summit\"] - (input_length // 2)\n",
    "    table[\"end\"] = table[\"start\"] + input_length\n",
    "    \n",
    "    task_coords.append(table[[\"chrom\", \"start\", \"end\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "For subsetting predictions/performance metrics to peak subsets, extracting performance metrics, and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_coord_inds(superset_coords, subset_coords):\n",
    "    \"\"\"\n",
    "    Both `superset_coords` and `subset_coords` are Pandas DataFrames of\n",
    "    coordinates. This will return the indices within `superset_coords`\n",
    "    (indices being the 0-indexed row numbers) that correspond to the\n",
    "    coordinates in `subset_coords`. Returns a sorted NumPy array of indices.\n",
    "    Note that if there are duplicates in either set of coordinates, they\n",
    "    will be dropped (i.e. the returned indices will be unique).\n",
    "    \"\"\"\n",
    "    inds = superset_coords.reset_index().drop_duplicates([\"chrom\", \"start\", \"end\"]).merge(\n",
    "            subset_coords.reset_index(), on=[\"chrom\", \"start\", \"end\"]\n",
    "        ).sort_values(\"index_y\")[\"index_x\"].values\n",
    "    return np.sort(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nll_log_probs(nlls, profiles):\n",
    "    \"\"\"\n",
    "    Computes the log probability portion of the NLL by adding back\n",
    "    log(N!/x1!...xk!).\n",
    "    Arguments:\n",
    "        `nlls`: An N x T array of NLLs (strands averaged)\n",
    "        `profiles`: An N x T x O x 2 corresponding array of true profile counts\n",
    "            (that were used to compute the NLLs)\n",
    "    Returns an N x T array of NLL log probabilities.\n",
    "    \"\"\"\n",
    "    counts = np.sum(profiles, axis=2)\n",
    "    log_n_fact = scipy.special.gammaln(counts + 1)\n",
    "    log_x_fact = scipy.special.gammaln(profiles + 1)\n",
    "    log_x_fact_sum = np.sum(log_x_fact, axis=2)\n",
    "    diff = np.mean(log_n_fact + log_x_fact_sum, axis=2)  # Shape: N x T\n",
    "    return nlls + diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_performance_metrics(pred_perf_path, coord_sets=None, task_inds=None):\n",
    "    \"\"\"\n",
    "    Extracts the set of performance metrics from a saved predictions/performance\n",
    "    HDF5 file. If specified, filters for coordinates that are in `coord_sets`.\n",
    "    `coord_sets` is a list of coordinate DataFrames, and a set of metrics will\n",
    "    be fetched for each table of coordinates provided. Otherwise, will simply\n",
    "    return all coordinates available (i.e. only one coordinate set with all\n",
    "    coordinates). If `task_inds` is specified, it must be a list of indices\n",
    "    parallel to `coord_sets`. For each coordinate set, the metrics extracted\n",
    "    will be for that task index only. If unspecified, the average over all tasks\n",
    "    is retained for each coordinate set.\n",
    "    Returns a dictionary of the following form:\n",
    "        `nll`: [\n",
    "            <NLL vector for coord set 1>\n",
    "            <NLL vector for coord set 2>\n",
    "            ...\n",
    "        ],\n",
    "        `count_mse`: [\n",
    "            <MSE scalar for coord set 1>\n",
    "            <MSE scalar for coord set 2>\n",
    "            ...\n",
    "        ]\n",
    "        ...\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    reader = h5py.File(pred_perf_path, \"r\")\n",
    "    coord_reader = reader[\"coords\"]\n",
    "    pred_reader = reader[\"predictions\"]\n",
    "    perf_reader = reader[\"performance\"]\n",
    "    \n",
    "    # First, get the set of indices within the HDF5 predictions/performance that\n",
    "    # correspond to the given coordinate sets\n",
    "    if coord_sets is None:\n",
    "        subset_inds = [np.arange(perf_reader[\"nll\"].shape[0])]  # The entire vector\n",
    "    else:\n",
    "        # Import the DataFrame of coordinates in this HDF5\n",
    "        pred_perf_coords = pd.DataFrame(\n",
    "            data={\n",
    "                \"chrom\": coord_reader[\"coords_chrom\"][:].astype(str),\n",
    "                \"start\": coord_reader[\"coords_start\"][:],\n",
    "                \"end\": coord_reader[\"coords_end\"][:]\n",
    "            }\n",
    "        )\n",
    "        subset_inds = [\n",
    "            subset_coord_inds(pred_perf_coords, coord_set)\n",
    "            for coord_set in coord_sets\n",
    "        ]\n",
    "    \n",
    "    # If we didn't specify a task index for each coordinate set, just use\n",
    "    # all tasks; either way, let's get each set of task indices into a\n",
    "    # NumPy array form\n",
    "    if task_inds is None:\n",
    "        task_inds = [np.arange(perf_reader[\"nll\"].shape[1])]\n",
    "    else:\n",
    "        task_inds = [np.array([i]) for i in task_inds]\n",
    "        \n",
    "    # For each performance metric, for each coordinate set/task index, extract\n",
    "    # the metrics values\n",
    "    for key in perf_reader.keys():\n",
    "        metrics_list = []\n",
    "        for i in range(len(subset_inds)):\n",
    "            subset = subset_inds[i]\n",
    "            tasks = task_inds[i]\n",
    "            \n",
    "            if len(perf_reader[key].shape) >= 2:  # Profile metric\n",
    "                metrics_list.append(\n",
    "                    np.mean(perf_reader[key][subset][:, tasks], axis=1)\n",
    "                )\n",
    "            else:  # Count metric\n",
    "                # If the coordinate set is limited, then we'll need to recompute\n",
    "                # the count metrics (i.e. MSE and correlations), since these were\n",
    "                # saved for the entire set\n",
    "                if coord_sets is None:\n",
    "                    # No need to recompute\n",
    "                    metrics_list.append(\n",
    "                        np.mean(perf_reader[key][tasks])\n",
    "                    )\n",
    "                else:\n",
    "                    log_true_counts = np.ravel(np.log(pred_reader[\"true_counts\"][subset][:, tasks] + 1))\n",
    "                    log_pred_counts = np.ravel(pred_reader[\"log_pred_counts\"][subset][:, tasks])\n",
    "                    if key == \"count_mse\":\n",
    "                        metrics_list.append(\n",
    "                            np.mean(np.square(log_true_counts - log_pred_counts))\n",
    "                        )\n",
    "                    elif key == \"count_pearson\":\n",
    "                        metrics_list.append(\n",
    "                            scipy.stats.pearsonr(log_true_counts, log_pred_counts)[0]\n",
    "                        )\n",
    "                    elif key == \"count_spearman\":\n",
    "                        metrics_list.append(\n",
    "                            scipy.stats.spearmanr(log_true_counts, log_pred_counts)[0]\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(\"Unknown count metric key: %s\" % key)\n",
    "\n",
    "        result[key] = metrics_list\n",
    "\n",
    "    # Compute the NLL log probs for the actual performance\n",
    "    nll_log_probs = []\n",
    "    for i in range(len(subset_inds)):\n",
    "        subset = subset_inds[i]\n",
    "        tasks = task_inds[i]\n",
    "        \n",
    "        log_probs = compute_nll_log_probs(\n",
    "            perf_reader[\"nll\"][subset][:, tasks],\n",
    "            pred_reader[\"true_profs\"][subset][:, tasks]\n",
    "        )\n",
    "        nll_log_probs.append(np.mean(log_probs, axis=1))\n",
    "    result[\"nll_log_probs\"] = nll_log_probs\n",
    "    \n",
    "    reader.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_performance_bounds(perf_bounds_path, input_length, coord_sets=None, task_inds=None):\n",
    "    \"\"\"\n",
    "    Extracts the set of lower and upper bound performance metrics from a saved\n",
    "    HDF5 file. `input_length` is the lenght of input sequence to use.\n",
    "    If specified, filters for coordinates that are in `coord_sets`.\n",
    "    `coord_sets` is a list of coordinate DataFrames, and a set of metrics will\n",
    "    be fetched for each table of coordinates provided. Otherwise, will simply\n",
    "    return all coordinates available (i.e. only one coordinate set with all\n",
    "    coordinates). If `task_inds` is specified, it must be a list of indices\n",
    "    parallel to `coord_sets`. For each coordinate set, the metrics extracted\n",
    "    will be for that task index only. If unspecified, the average over all tasks\n",
    "    is retained for each coordinate set.\n",
    "    Returns a dictionary of the following form:\n",
    "        `nll`: [\n",
    "            (\n",
    "                <lower-bound NLL vector for coord set 1>,\n",
    "                <upper-bound NLL vector for coord set 1>\n",
    "            ),\n",
    "            (\n",
    "                <lower-bound NLL vector for coord set 2>,\n",
    "                <upper-bound NLL vector for coord set 2>\n",
    "            ),\n",
    "            \n",
    "            ...\n",
    "        ],\n",
    "        `count_mse`: [\n",
    "            (\n",
    "                <lower-bound MSE scalar for coord set 1>,\n",
    "                <upper-bound MSE scalar for coord set 1>\n",
    "            ),\n",
    "            (\n",
    "                <lower-bound MSE scalar for coord set 2>,\n",
    "                <upper-bound MSE scalar for coord set 2>\n",
    "            ),\n",
    "            ...\n",
    "        ]\n",
    "        ...\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    reader = h5py.File(perf_bounds_path, \"r\")\n",
    "    coord_reader = reader[\"coords\"]\n",
    "    lower_perf_reader = reader[\"performance_lower\"]\n",
    "    upper_perf_reader = reader[\"performance_upper\"]\n",
    "    \n",
    "    # First, get the set of indices within the HDF5 predictions/performance that\n",
    "    # correspond to the given coordinate sets\n",
    "    if coord_sets is None:\n",
    "        subset_inds = [np.arange(lower_perf_reader[\"nll\"].shape[0])]  # The entire vector\n",
    "    else:\n",
    "        # Import the DataFrame of coordinates in this HDF5\n",
    "        perf_coords = pd.DataFrame(\n",
    "            data={\n",
    "                \"chrom\": coord_reader[\"coords_chrom\"][:].astype(str),\n",
    "                \"start\": coord_reader[\"coords_start\"][:],\n",
    "                \"end\": coord_reader[\"coords_end\"][:]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Unlike the predictions, the performance bounds are computed solely\n",
    "        # based on profiles, so their saved coordinates have a different\n",
    "        # length, although they are centered at the same summit. So we need\n",
    "        # to re-pad them.\n",
    "        perf_coords[\"midpoint\"] = (perf_coords[\"start\"] + perf_coords[\"end\"]) // 2\n",
    "        perf_coords[\"start\"] = perf_coords[\"midpoint\"] - (input_length // 2)\n",
    "        perf_coords[\"end\"] = perf_coords[\"start\"] + input_length\n",
    "        del perf_coords[\"midpoint\"]\n",
    "        \n",
    "        subset_inds = [\n",
    "            subset_coord_inds(perf_coords, coord_set)\n",
    "            for coord_set in coord_sets\n",
    "        ]\n",
    "    \n",
    "    # If we didn't specify a task index for each coordinate set, just use\n",
    "    # all tasks; either way, let's get each set of task indices into a\n",
    "    # NumPy array form\n",
    "    if task_inds is None:\n",
    "        task_inds = [np.arange(lower_perf_reader[\"nll\"].shape[1])]\n",
    "    else:\n",
    "        task_inds = [np.array([i]) for i in task_inds]\n",
    "        \n",
    "    # For each performance metric, for each coordinate set/task index, extract\n",
    "    # the metrics values for lower and upper bound\n",
    "    for key in lower_perf_reader.keys():\n",
    "        metrics_list = []\n",
    "        for i in range(len(subset_inds)):\n",
    "            subset = subset_inds[i]\n",
    "            tasks = task_inds[i]\n",
    "            \n",
    "            if len(lower_perf_reader[key].shape) >= 2:  # Profile metric\n",
    "                metrics_list.append(\n",
    "                    (\n",
    "                        np.mean(lower_perf_reader[key][subset][:, tasks], axis=1),\n",
    "                        np.mean(upper_perf_reader[key][subset][:, tasks], axis=1)\n",
    "                    )\n",
    "                )\n",
    "            else:  # Count metric\n",
    "                # For the performance bounds, we'll use counts metrics as-is without\n",
    "                # recomputing them; this is because the counts metrics are distributed\n",
    "                # very uniformly\n",
    "                metrics_list.append(\n",
    "                    (\n",
    "                        np.mean(lower_perf_reader[key][tasks]),\n",
    "                        np.mean(upper_perf_reader[key][tasks]),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        result[key] = metrics_list\n",
    "\n",
    "    # Note that the NLL log probs is already part of the saved HDF5,\n",
    "    # so we don't need to compute it separately here\n",
    "\n",
    "    reader.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performances(perf_dict, title=None, cond_labels=None, cond_colors=None):\n",
    "    \"\"\"\n",
    "    Creates plots for a performance dictionary of the following form:\n",
    "        `nll`: [\n",
    "            <NLL vector for cond 1>\n",
    "            <NLL vector for cond 2>\n",
    "            ...\n",
    "        ],\n",
    "        `count_mse`: [\n",
    "            <MSE scalar for cond 1>\n",
    "            <MSE scalar for cond 2>\n",
    "            ...\n",
    "        ]\n",
    "        ...\n",
    "    For profile metrics (i.e. where the metrics are a vector), creates violin\n",
    "    plots. For count metrics (i.e. where the metrics are a scalar), creates bar\n",
    "    plots.\n",
    "    `cond_labels` and `cond_colors` must be arrays parallel to the set of\n",
    "    vectors or scalars for each metric.\n",
    "    \"\"\"\n",
    "    def create_violins(ax, metrics_list, colors):\n",
    "        \"\"\"\n",
    "        Creates a violin plot on the given instantiated axes.\n",
    "        `metrics_list` is a list of vectors. `colors` is a parallel\n",
    "        list of colors for each violin.\n",
    "        \"\"\"\n",
    "        num_perfs = len(metrics_list)\n",
    "        \n",
    "        q1, med, q3 = np.stack([\n",
    "            np.nanpercentile(data, [25, 50, 70], axis=0) for data in metrics_list\n",
    "        ], axis=1)\n",
    "        iqr = q3 - q1\n",
    "        lower_outlier = q1 - (1.5 * iqr)\n",
    "        upper_outlier = q3 + (1.5 * iqr)\n",
    "\n",
    "        \n",
    "        sorted_clipped_data = [  # Remove outliers based on outlier rule\n",
    "            np.sort(vec[(vec >= lower_outlier[i]) & (vec <= upper_outlier[i])])\n",
    "            for i, vec in enumerate(metrics_list)\n",
    "        ]\n",
    "        \n",
    "        plot_parts = ax.violinplot(\n",
    "            sorted_clipped_data, showmeans=False, showmedians=False, showextrema=False\n",
    "        )\n",
    "        violin_parts = plot_parts[\"bodies\"]\n",
    "        for i in range(num_perfs):\n",
    "            violin_parts[i].set_facecolor(colors[i])\n",
    "            violin_parts[i].set_edgecolor(colors[i])\n",
    "            violin_parts[i].set_alpha(0.7)\n",
    "        \n",
    "        inds = np.arange(1, num_perfs + 1)\n",
    "        ax.vlines(inds, q1, q3, color=\"black\", linewidth=5, zorder=1)\n",
    "        ax.scatter(inds, med, marker=\"o\", color=\"white\", s=30, zorder=2)\n",
    "        \n",
    "    num_conds = len(perf_dict[\"nll\"])\n",
    "    if not cond_colors:\n",
    "        cond_colors = [\"mediumorchid\"] * num_conds\n",
    "    \n",
    "    # Profile metrics\n",
    "    for metric_key, metric_name in [\n",
    "        (\"nll\", \"NLL\"), (\"nll_log_probs\", \"NLL log probs\"), (\"jsd\", \"JSD\"), (\"profile_mse\", \"Profile MSE\"),\n",
    "        (\"profile_pearson\", \"Profile Pearson\"), (\"profile_spearman\", \"Profile Spearman\")\n",
    "    ]:\n",
    "        fig, ax = plt.subplots(figsize=(20, 5))\n",
    "        create_violins(ax, perf_dict[metric_key], cond_colors)\n",
    "        if title:\n",
    "            ax.set_title(\"%s: %s\" % (title, metric_name))\n",
    "        else:\n",
    "            ax.set_title(metric_name)\n",
    "        if cond_labels:\n",
    "            ax.set_xticks(np.arange(1, num_conds + 1))\n",
    "            ax.set_xticklabels(cond_labels)\n",
    "        plt.show()\n",
    "        print(\"Average values: \" + \" \".join([(\"%.3f\" % np.nanmean(arr)) for arr in perf_dict[metric_key]]))\n",
    "\n",
    "    # Count metrics\n",
    "    for metric_key, metric_name in [\n",
    "        (\"count_mse\", \"Count MSE\"), (\"count_pearson\", \"Count Pearson\"), (\"count_spearman\", \"Count Spearman\")\n",
    "    ]:\n",
    "        fig, ax = plt.subplots(figsize=(20, 5))\n",
    "        label_locs = np.arange(num_conds)  # Location of labels\n",
    "        ax.bar(\n",
    "            label_locs, perf_dict[metric_key], color=cond_colors, alpha=0.7\n",
    "        )\n",
    "        if title:\n",
    "            ax.set_title(\"%s: %s\" % (title, metric_name))\n",
    "        else:\n",
    "            ax.set_title(metric_name)\n",
    "        if cond_labels:\n",
    "            ax.set_xticks(label_locs)\n",
    "            ax.set_xticklabels(cond_labels)\n",
    "        plt.show()\n",
    "        print(\"Average values: \" + \" \".join([(\"%.3f\" % val) for val in perf_dict[metric_key]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"multitask-fold\"></a>\n",
    "### Multi-task profile model performance across all 10 folds\n",
    "A comparison of the test-set performance (averaged across tasks) between:\n",
    "1. Multi-task profile models trained across all 10 folds\n",
    "2. A fine-tuned multi-task profile model on the best-performing fold\n",
    "3. Upper and lower bounds on the best-performing fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multitask_perf_dict = {}\n",
    "\n",
    "multitask_bounds_perf_dict = extract_performance_bounds(\n",
    "    perf_bounds_path, input_length\n",
    ")\n",
    "\n",
    "# Lower bound first\n",
    "for key in multitask_bounds_perf_dict.keys():\n",
    "    multitask_perf_dict[key] = [multitask_bounds_perf_dict[key][0][0]]\n",
    "\n",
    "# 10 folds\n",
    "for pred_path in multitask_preds_paths:\n",
    "    perf_dict = extract_performance_metrics(pred_path)\n",
    "    for key in multitask_perf_dict.keys():\n",
    "        multitask_perf_dict[key].append(perf_dict[key][0])\n",
    "\n",
    "# Fine-tuned\n",
    "perf_dict = extract_performance_metrics(multitask_finetuned_preds_path)\n",
    "for key in multitask_perf_dict.keys():\n",
    "    multitask_perf_dict[key].append(perf_dict[key][0])\n",
    "    \n",
    "# Upper bound last\n",
    "for key in multitask_perf_dict.keys():\n",
    "    multitask_perf_dict[key].append(multitask_bounds_perf_dict[key][0][1])\n",
    "    \n",
    "cond_labels = [\"Randomized\"]\n",
    "cond_labels += [(\"Fold %d\" % i) for i in range(1, 11)]\n",
    "cond_labels += [\"Fine-tuned\\n(fold %d)\" % best_multitask_fold]\n",
    "cond_labels += [\"Pseudoreps\"]\n",
    "cond_colors = [\"coral\"] + ([\"mediumorchid\"] * 10) + [\"seagreen\", \"slateblue\"]\n",
    "plot_performances(\n",
    "    multitask_perf_dict,\n",
    "    title=(\"%s multi-task models\" % tf_name),\n",
    "    cond_labels=cond_labels,\n",
    "    cond_colors=cond_colors\n",
    ")\n",
    "\n",
    "del multitask_perf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"singletask-fold\"></a>\n",
    "### Single-task profile model performance across all 10 folds\n",
    "**For each task**, a comparison of the test-set performance between:\n",
    "1. Single-task profile models trained across all 10 folds\n",
    "2. A fine-tuned single-task profile model on the best-performing fold\n",
    "3. Upper and lower bounds on the best-performing fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for task_index in range(len(task_coords)):\n",
    "    singletask_perf_dict = {}\n",
    "\n",
    "    singletask_bounds_perf_dict = extract_performance_bounds(\n",
    "        perf_bounds_path, input_length,\n",
    "        coord_sets=[task_coords[task_index]], task_inds=[task_index]\n",
    "    )\n",
    "\n",
    "    # Lower bound first\n",
    "    for key in singletask_bounds_perf_dict.keys():\n",
    "        singletask_perf_dict[key] = [singletask_bounds_perf_dict[key][0][0]]\n",
    "\n",
    "    # 10 folds\n",
    "    for pred_path in singletask_preds_paths[task_index]:\n",
    "        perf_dict = extract_performance_metrics(pred_path)\n",
    "        # No need to specify specific coordinates or task indices, because\n",
    "        # single-task model predictions are saved only for that one task\n",
    "        for key in singletask_perf_dict.keys():\n",
    "            singletask_perf_dict[key].append(perf_dict[key][0])\n",
    "\n",
    "    # Fine-tuned\n",
    "    perf_dict = extract_performance_metrics(\n",
    "        singletask_finetuned_preds_paths[task_index]\n",
    "    )\n",
    "    # No need to specify specific coordinates or task indices, because\n",
    "        # single-task model predictions are saved only for that one task\n",
    "    for key in singletask_perf_dict.keys():\n",
    "        singletask_perf_dict[key].append(perf_dict[key][0])\n",
    "\n",
    "    # Upper bound last\n",
    "    for key in singletask_perf_dict.keys():\n",
    "        singletask_perf_dict[key].append(singletask_bounds_perf_dict[key][0][1])\n",
    "        \n",
    "    cond_labels = [\"Randomized\"]\n",
    "    cond_labels += [(\"Fold %d\" % i) for i in range(1, 11)]\n",
    "    cond_labels += [\"Fine-tuned\\n(fold %d)\" % best_singletask_folds[task_index]]\n",
    "    cond_labels += [\"Pseudoreps\"]\n",
    "    cond_colors = [\"coral\"] + ([\"mediumorchid\"] * 10) + [\"seagreen\", \"slateblue\"]\n",
    "    plot_performances(\n",
    "        singletask_perf_dict,\n",
    "        title=(\"%s single-task models (task %d)\" % (tf_name, task_index)),\n",
    "        cond_labels=cond_labels,\n",
    "        cond_colors=cond_colors\n",
    "    )\n",
    "    \n",
    "    del singletask_perf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"finetune-multitask-task\"></a>\n",
    "### Fine-tuned multi-task profile model task-specific performance\n",
    "A comparison of the test-set performance for between each task of a multi-task profile model fine-tuned on the best-performing fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_multitask_perf_dict = extract_performance_metrics(\n",
    "    multitask_finetuned_preds_path,\n",
    "    coord_sets=task_coords,\n",
    "    task_inds=list(range(len(task_coords)))\n",
    ")\n",
    "\n",
    "cond_labels = [(\"Task %d\" % i) for i in range(len(task_coords))]\n",
    "cond_colors = [\"mediumorchid\"] * len(task_coords)\n",
    "plot_performances(\n",
    "    finetune_multitask_perf_dict,\n",
    "    title=(\"%s fine-tuned multi-task model\" % tf_name),\n",
    "    cond_labels=cond_labels,\n",
    "    cond_colors=cond_colors\n",
    ")\n",
    "\n",
    "del finetune_multitask_perf_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
